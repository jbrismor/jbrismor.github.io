[
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Code\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import MeanShift\nfrom sklearn.cluster import Birch\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import pairwise_distances, silhouette_score\nimport warnings\nimport pandas as pd\nCode\ndf=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv')\n\nscaler = StandardScaler()\ndf_scaled = (scaler.fit_transform(df))\ndf_scaled=pd.DataFrame(df_scaled)"
  },
  {
    "objectID": "Clustering.html#explanation-of-clustering-methods",
    "href": "Clustering.html#explanation-of-clustering-methods",
    "title": "Clustering",
    "section": "Explanation of clustering methods",
    "text": "Explanation of clustering methods\nClustering focuses on finding unknown similarities or groupings in the data (unsupervised). The goal is to see if there are any groupings in a given data set in order to make generalizations about the data. The data set used for this project is the EPA Smart Location Database (EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv in the clean data folder), which contains census block data for the US. The data set contains 71 variables and 220740 census blocks, which will be used for clustering. Every explanation of each variable can be found in the raw data folder under the EPA_SLD_3.0_TechnicalDocumentationUserGuide_May2021.pdf or in the following citation (Jim Chapman 2021). There are three methods that will be used for this study: K-means, Hierarchical, and DBSCAN. The goal is to see if there are any groupings in the data in order to make generalizations about these census blocks. If there is a small enough group of groupings, we can make generalizations about the census blocks use them for public transportation planning.\n\nK-means\nK-means is a clustering method that focuses on finding the centers of the clusters and then assigning the data points to each cluster. The number of clusters is chosen by the performer, so the work around is to provide a range of possible number of clusters and get their scores in order to find the most optimal one. The algorithm randomly selects k points in the data, calculates the center of the cluster, and then assigns each point to the closest cluster. The algorithm is heavily impacted by the initial random points selected, so the algorithm is run multiple times and the best result is selected for the number of clusters specified. Furthermore, in order to get more accruacy, the data is scaled before running this algorithm as it is extreamly sensitive to outliers.\n\n\nDBSCAN\nDBSCAN is another clustering method that tries to find clusters based on the density (distances between points and a minimum number of points). It also detects and marks outliers as points that are in very low density regions. The algorithm runs until all points are placed in a cluster or marked as an outlier. Then, the best result is selected. The algorithm is also sensitive to outliers the same way as k-means, so the data set is scaled to reduce the effect of outliers.\nDBSCAN requires two parameters: epsilon and minimum samples. Epsilon is the maximum distance between two points in order for them to be considered in the same cluster. The Minimum sample is the minimum number of points in each cluster. In order to find the best parameters, the algorithm is run multiple times with different values for epsilon and minimum samples. The best result is then selected.\n\n\nHierarchical Clustering\nHierarchical clustering is a clustering method that focuses on finding clusters based on distance. The algorithm works by finding the two closest points in the data set and forming a cluster. The distance between the two points is the distance between the two clusters. The algorithm is repeated until all the points are assigned to a cluster. The algorithm is sensitive to the distance, so the algorithm is run multiple times with different values for the distance. The best result is selected. The algorithm is also sensitive to outliers, so the data set is scaled to reduce the effect of outliers.\nHierarchical clustering is a method that finds the clusters based on distance. The algorithm works by finding the two closest points in the data set and forming a cluster. The algorithm is repeated until all points are assigned to a cluster. Thus, the algorithm runs multiple times with different values for the distance and then the best result is selected. As the previous two, the algorithm is also sensitive to outliers, so the data set is scaled to reduce the effect of outliers.\n\n\nScores used:\n\nDistortion: Distortion is the sum of squared errors. It can be used empirically to find the optimal number of clusters by plotting the distortion for each number of clusters and finding the elbow point.\nDavies-Boulding Index: This index is the average of the ratios of of all clusters. Each cluster ratio is the average distance between points in a cluster and the centroid of the cluster to the distance between the centroids of the cluster and the nearest cluster. The lower the index, the better the clustering (Petrovic 2006).\nOther options that have been rejected: Due to the size of the data set, the Silhouette Score and Inertia where discard. These scores were either too computationaly expensive or not suitable for the data set."
  },
  {
    "objectID": "Clustering.html#k-means-1",
    "href": "Clustering.html#k-means-1",
    "title": "Clustering",
    "section": "k-means++",
    "text": "k-means++\nThese are the following scores obtained per number of clusters in k-means++. We will only use distortion as our parameter due to the large size of our data set and my laptop’s computational limitations. Thus, we will use the elbow method to empirically find the optimal number of clusters.\n\n\nCode\ndistortion_values = []\n\nfor i in range(1, 65):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(df_scaled)\n    cluster_assignments = kmeans.predict(df_scaled)\n    centers = kmeans.cluster_centers_\n\n    distortion = sum(np.min(pairwise_distances(df_scaled, centers, metric='euclidean'), axis=1)) / df_scaled.shape[0]\n    distortion_values.append(distortion)\n\nkmeans_df = pd.DataFrame({'Clusters': range(1, 65), 'Distortion': distortion_values})\n\npd.set_option(\"display.max_rows\", None)\nprint(kmeans_df)\n\n\n\n\nCode\nplt.figure(figsize=(10, 5))\nplt.plot(kmeans_df['Clusters'], kmeans_df['Distortion'], marker='', linestyle='-', color='darkblue')\nplt.title('Elbow Method using Distortion')\nplt.ylabel('Distortion')\nplt.xlabel('Number of clusters')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nBased on this graph, using the elbow method we can infer that the elbow could be located at 12 clusters. Even though 19, 21, or 23 clusters also could be good solutions, DBSCAN gives a similar answer (5 or 13 clusters being the two best options). Thus, we will use 12 clusters for k-means++."
  },
  {
    "objectID": "Clustering.html#dbscan-1",
    "href": "Clustering.html#dbscan-1",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN\nFor DBSCAN due to my laptops computational limitations, we used the dimensionality reduced data obtained from PCA (which explained more than 90% of the variance and held 25 out of the original 71 variables). We used Davies Boulding Index as our score due to its computational simplicity and its easy readability. We ran the algorithm with different values for epsilon and minimum samples. The algorithm took 1.5 hours to run, which is a long time, but it is a good result considering the size of the data set.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nscores_pca_df=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv')\n\n\nThese are the results:\n\n\nCode\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import davies_bouldin_score\nimport numpy as np\n\nl_eps = np.arange(0.5, 2, 0.1)\nn_sample = range(5, 11)\n\nresults = {}\n\nfor eps in l_eps:\n    best_db_index = float('inf')\n    opt_sample = None\n    opt_num_clusters = None\n\n    for n in n_sample:\n        model = DBSCAN(eps=eps, min_samples=n).fit(scores_pca_df)\n        try:\n            unique_labels = np.unique(model.labels_)\n            if len(unique_labels) &gt; 1 or (len(unique_labels) == 1 and unique_labels[0] != -1):\n                db_score = davies_bouldin_score(scores_pca_df, model.labels_)\n                num_clusters = len(unique_labels) - 1 \n                \n                if db_score &lt; best_db_index:\n                    best_db_index = db_score\n                    opt_sample = n\n                    opt_num_clusters = num_clusters\n        except:\n            continue\n    \n    results[eps] = {\n        'min_samples': opt_sample,\n        'db_index': best_db_index,\n        'num_clusters': opt_num_clusters\n    }\n\nprint(\"Results for each epsilon value:\")\nfor eps, values in results.items():\n    print(f\"For epsilon={eps}: Min_samples={values['min_samples']}, Davies-Bouldin Index={values['db_index']}, Number of Clusters={values['num_clusters']}\")\n\n\nResults for each epsilon value:\nFor epsilon=0.5: Min_samples=10, Davies-Bouldin Index=0.5465900750864565, Number of Clusters=5\nFor epsilon=0.6: Min_samples=10, Davies-Bouldin Index=1.0735197191834835, Number of Clusters=13\nFor epsilon=0.7: Min_samples=10, Davies-Bouldin Index=1.2753967110181181, Number of Clusters=24\nFor epsilon=0.7999999999999999: Min_samples=10, Davies-Bouldin Index=1.2865395597058111, Number of Clusters=24\nFor epsilon=0.8999999999999999: Min_samples=9, Davies-Bouldin Index=1.4075326833557051, Number of Clusters=32\nFor epsilon=0.9999999999999999: Min_samples=10, Davies-Bouldin Index=1.701066737957798, Number of Clusters=50\nFor epsilon=1.0999999999999999: Min_samples=5, Davies-Bouldin Index=1.7688572327797196, Number of Clusters=311\nFor epsilon=1.1999999999999997: Min_samples=5, Davies-Bouldin Index=1.683123753663869, Number of Clusters=296\nFor epsilon=1.2999999999999998: Min_samples=8, Davies-Bouldin Index=1.6238516834146322, Number of Clusters=68\nFor epsilon=1.4: Min_samples=9, Davies-Bouldin Index=1.471318653013628, Number of Clusters=28\nFor epsilon=1.4999999999999998: Min_samples=10, Davies-Bouldin Index=1.3802203666819477, Number of Clusters=20\nFor epsilon=1.5999999999999996: Min_samples=9, Davies-Bouldin Index=1.5742656310946384, Number of Clusters=32\nFor epsilon=1.6999999999999997: Min_samples=9, Davies-Bouldin Index=1.576282420115537, Number of Clusters=34\nFor epsilon=1.7999999999999998: Min_samples=8, Davies-Bouldin Index=1.474295360271144, Number of Clusters=44\nFor epsilon=1.8999999999999997: Min_samples=9, Davies-Bouldin Index=1.3577512519715513, Number of Clusters=23\n\n\n\n\nCode\nnum_clusters_values = [result['num_clusters'] for result in results.values()]\ndb_index_values = [result['db_index'] for result in results.values()]\n\nplt.figure(figsize=(10, 6))\nplt.plot(num_clusters_values, db_index_values, marker='o', linestyle='-', color='b')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Davies-Bouldin Index')\nplt.title('Number of Clusters vs Davies-Bouldin Index for DBSCAN')\nplt.grid(True)\nplt.xlim(0, 150)\nplt.show()\n\n\n\n\n\n\n\n\n\nBased on this results, we see that the first two best results are 5 and 13 clusters. As we can see that 13 clusters is still a good result based on the Davies Boulding Index, we will keep this in mind as it is more similar to the result also obtained in K-means++."
  },
  {
    "objectID": "Clustering.html#hierarchical-clustering-1",
    "href": "Clustering.html#hierarchical-clustering-1",
    "title": "Clustering",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nDue to the elevated computational complexity of this method, as k-means and DBSCAN already gave us good results, we will run Hierarchical clustering using 12 clusters as the number of clusters. We will use the same dimensionality reduced data obtained from PCA, but downsampled. The sampling was done using the stratified sampling method, which is a method that ensures that the sample is representative of the population and would better represent the data set (in order to properly assess clustering). Finally, the sample was only 20% of the original data set, which is still a large enough sample to be representative of the population.\n\n\nCode\nimport pandas as pd\nscores_pca_df=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv')\n\n\n\n\nCode\nimport pandas as pd\n\nscores_pca_df.iloc[:, 0] = pd.to_numeric(scores_pca_df.iloc[:, 0], errors='coerce')\n\nmin_col1 = scores_pca_df.iloc[:, 0].min(skipna=True)\nmax_col1 = scores_pca_df.iloc[:, 0].max(skipna=True)\nprint(f\"Min value of the first column: {min_col1}\")\nprint(f\"Max value of the first column: {max_col1}\")\n\n\n\n\nCode\nfor_samples = scores_pca_df\n\nbreakpoints = [-float('inf'), -14.93975, 11.2275, 37.39475, 63.562, 89.72925, 115.8965, 142.06375, 168.231, 194.39825, 220.5655, 246.73275, float('inf')]\n\nfor_samples['labels'] = pd.cut(for_samples.iloc[:, 0], bins=breakpoints, labels=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'])\n\nstratified_sample = for_samples.groupby('labels', group_keys=False).apply(lambda x: x.sample(frac=0.2))\n\nstratified_sample_no_lab = stratified_sample.drop(columns=['labels'])\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import davies_bouldin_score\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nlinkage_matrix = linkage(stratified_sample_no_lab, method='ward')\n\ndendrogram(linkage_matrix)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n\nnum_clusters = 12\nlabels = AgglomerativeClustering(n_clusters=num_clusters, linkage='ward').fit_predict(stratified_sample_no_lab)\n\ndb_index = davies_bouldin_score(stratified_sample_no_lab, labels)\nprint(f'Davies-Bouldin Index: {db_index}')\n\nresult_df = pd.DataFrame({\n    'Feature_1': stratified_sample_no_lab['PC1'],\n    'Feature_2': stratified_sample_no_lab['PC2'],\n    'Cluster': labels\n})\n\nprint(result_df)\n\n\n\n\n\n\n\n\n\n\nDavies-Bouldin Index: 1.4871933145528213\n         Feature_1  Feature_2  Cluster\n24683   -17.298789  22.884256       10\n25006   -18.274415  25.452802       10\n30365    -2.575073   3.024186        0\n38241     2.206078  -2.983790       11\n128967   -1.500392   1.324897        0\n...            ...        ...      ...\n150094   97.759211  26.286094        8\n209124   92.407836  32.282488        9\n152816  107.218005  44.218345        8\n42838   117.086261  64.029969        8\n16705   160.011595  -3.324794        7\n\n[44149 rows x 3 columns]\n\n\nFor the results, Feature_1 and Feature_2 are the coordinates of the data points based on the first two features. Furthermore, a Davies Boulding Index of less than 1.5 is a good result for this data set, which is a good indication for this number of clusters. The results also indicate the cluster to which each data point has been assigned to. Finally, the dendogram also allow us to understand better the distribution of the data points and the clusters in a more visual way."
  },
  {
    "objectID": "Clustering.html#conclusion",
    "href": "Clustering.html#conclusion",
    "title": "Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nBased on the results obtained from the three methods, we have a good reason to believe that clusters exist. However, due to our computational constrains encountered and the great size of our data set (in terms of variables), it will be helpful to visualize the clusters in 2D based on different variables. Firstly, we will visualize them on handpicked variables that we believe are the most explanatory from the primary data set. Then, we will use the PCA’s reduced data set to visualize the clusters in 2D from the first ten variables. Finally, we will make our inferences based on what we can see from the graphs.\n\nVisualization of the clusters of the primary data set\nThe following code adds the clusters to the data set and plots every pair of the following variables: ‘CBSA_POP’, ‘Ac_Water’, ‘Ac_Land’, ‘TotPop’, ‘P_WrkAge’, ‘E_HiWageWk’, ‘AutoOwn0’, ‘NatWalkInd’, ‘E_MedWageWk’, ‘Ac_Unpr’. These have been selected due to their importance when describing a census block and due to easy human interpretability. The objective is to see if there are any patterns in the clusters that can be used to make generalizations about the census blocks.\nNote: Only 60% of the data set was used for this visualization due to the computational limitations of my laptop. However, it has been sampled using the stratified sampling method, which ensures that the sample is representative of the population.\n\n\nCode\nwarnings.filterwarnings('ignore')\n\ndf= pd.read_csv(\"../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv\")\nscaler = StandardScaler()\ndf_scaled = (scaler.fit_transform(df))\n\nkmeans = KMeans(n_clusters=12, random_state=42)\ndf['Clusters'] = kmeans.fit_predict(df_scaled)\n\ncolumns_to_plot = ['CBSA_POP', 'Ac_Water', 'Ac_Land', 'TotPop', 'P_WrkAge', 'E_HiWageWk', 'AutoOwn0', 'NatWalkInd', 'E_MedWageWk', 'Ac_Unpr']\n\nsns.pairplot(df, vars=columns_to_plot, hue='Clusters', palette='viridis')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe previous pairplot is very interesting. While in some pairs of the variables there is a complete overlap of the clusters, in others, we can really see the separation of these. Ac_Land and AC_Unpr, TotPop and AC_Land, AC_Land and P_wrkAge, P_wrkAge and AC_Unpr, and AC_Water and E_MedWage are great visualizations for this. This is a good sign that there seems to be clusters in our data set that are human interpretable and can be used to make generalizations about the census blocks.\n\n\nVisualization of the clusters of the PCA data set\nThe following code adds the clusters to the PCA data set and plots every pair of the first ten variables of the PCA data set. A new csv file has been created (pca_clustered_data.csv) for reproducibility purposes. The objective is to see if there are any patterns in the clusters that can be used to make generalizations about the census blocks and compare the results with the previous pairplots.\nNote: Only 60% of the data set was used for this visualization due to the computational limitations of my laptop. However, it has been sampled using the stratified sampling method, which ensures that the sample is representative of the population.\n\n\nCode\ndf_pca=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv')\n\nnum_clusters = 12\n\nfeatures = df_pca.iloc[:, :]\n\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\ndf_pca['Cluster'] = kmeans.fit_predict(features)\n\ndf_pca.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/pca_clustered_data.csv', index=False)\n\n\n\n\nCode\nwarnings.filterwarnings('ignore')\n\ndf_pca= pd.read_csv(\"../../data/Clean_Data_project_Pub.Transport_5000/pca_clustered_data.csv\")\n\ncolumns_to_plot = df_pca.columns[:10]\n\nsns.pairplot(df_pca, vars=columns_to_plot, hue='Cluster', palette='viridis')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe following pairplot is very interesting. While in some pairs of the variables there is a complete overlap of the clusters, in others, we can really see the separation of these. PC8 and PC5, PC3 and PC5, and PC5 and PC4 are great visualizations for this. This is a good sign that there seems to be clusters in our data set that are human interpretable and can be used to make generalizations about the census blocks.\n\n\nFinal Conclusion\nWe can determine that there seems to be possible clustering in the data set. With more advance computational power, it could be determined with more precision. However, with the methods utilized, we can determine that there are probably 12 clusters in the data set. Not only the clustering scores support this claim, but the pairplots really display good indications that this is the case. These clusters could be used to make generalizations about the census blocks and use them for public transportation planning."
  },
  {
    "objectID": "NNets.html",
    "href": "NNets.html",
    "title": "Ice Bot: Reinforcement Learning Implementation on Custom Air Hockey Game",
    "section": "",
    "text": "In our project, we embarked on the challenging, yet intriguing task of employing reinforcement learning (RL) to master the game of air hockey. Reinforcement learning, the third paradigm of machine learning, empowers an agent to optimize actions through systematic trial-and-error, guided by rewards for successes and penalties for errors. Our primary aim was to develop an RL agent proficient in the fundamental dynamics of air hockey, capable of refining its strategies to consistently outperform human opponents. To achieve this, we meticulously crafted a faithful simulation of the game, continuously honing our models and strategies through iterative learning and performance assessments. We aimed to cultivate a reinforcement learning agent that could engage in realistic and competitive gameplay, providing a solid groundwork for future explorations and advancements."
  },
  {
    "objectID": "NNets.html#environment-structure",
    "href": "NNets.html#environment-structure",
    "title": "Ice Bot: Reinforcement Learning Implementation on Custom Air Hockey Game",
    "section": "Environment Structure",
    "text": "Environment Structure\nThe Gym environment for our air hockey game is structured to encapsulate all the dynamics and rules of the game. At each timestep, the environment accepts an action from the agent (a specific paddle movement direction or no movement) and updates the game state accordingly. The state includes the positions and velocities of the puck and paddles, which are relayed back to the agent as observations. Rewards are calculated based on the game outcomes—such as scoring a goal or colliding with the puck—providing immediate feedback to the agent about the effectiveness of its actions.\nBy wrapping our game in the Gymnasium environment, we’ve established a robust platform for conducting rigorous and reproducible reinforcement learning experiments, essential for advancing our project’s goals of developing an AI capable of mastering air hockey."
  },
  {
    "objectID": "NNets.html#customization-of-the-gym-interface",
    "href": "NNets.html#customization-of-the-gym-interface",
    "title": "Ice Bot: Reinforcement Learning Implementation on Custom Air Hockey Game",
    "section": "Customization of the Gym Interface",
    "text": "Customization of the Gym Interface\nTo integrate our game with the Gym framework, we defined several key functions:\n\nreset(): Initializes or resets the game state at the start of an episode. This function is crucial for starting new training sessions and is automatically called after each game.\nstep(action): Advances the game by one timestep. It executes an action selected by the agent, updates the game state, and returns the new state, the reward for the action taken, and a boolean value that indicates whether the episode has ended (e.g., if a goal is scored).\nrender(): Provides a visual representation of the game state, which is invaluable for debugging and visually tracking the agent’s performance during training.\nclose(): Properly shuts down the environment when it is no longer needed, ensuring that resources are cleanly released."
  },
  {
    "objectID": "NNets.html#training-specifications",
    "href": "NNets.html#training-specifications",
    "title": "Ice Bot: Reinforcement Learning Implementation on Custom Air Hockey Game",
    "section": "Training Specifications",
    "text": "Training Specifications\nWe initially used personal laptops for training, which were sufficient for early experiments and our proof of concept in training a model to play Pong. These setups allowed the model to surpass human performance within hours as the agent completed roughly 1,000,000 episodes. However, as we progressed to the more complex and demanding Air Hockey environment, the intricate game physics and strategic demands quickly demonstrated that our laptops, with their limited processing power, were insufficient. The increased complexity necessitated a shift to more powerful computing solutions to efficiently manage the advanced training requirements.\n\nGoogle Colab\nWe initially explored using Google Colab Pro, a hosted Jupyter Notebook service that offers access to more robust computing resources (n.d.) such as faster GPUs and increased system memory. After installing the necessary libraries and connecting to Google Drive for our project files, we found that while the GPU accelerated parts of our model training, it did not improve the speed of stepping through the game simulations, which was the primary bottleneck in our training process. Consequently, we decided to shift away from this paid solution to explore more effective options already available to us.\n\n\nIntel NUCs\nWe moved training to four Intel Next Unit of Computing (NUC). These are small computers measuring approximately 4’ x 4’ x 2.5’ and are used in many different use cases, from desktop replacement to edge computing nodes for major corporations like Chick-fil-A (2023). Our NUCs are equipped with i5-1135G7, 64GB RAM, and 500GB m.2 solid state drive. They are running Ubuntu Server (22.044 LTCS). While there are many solutions to ensure the code runs continuously, we used screen to maintain persistent sessions even after we disconnected from the devices. The network’s firewall runs a Virtual Private Network (VPN) service to allow us to manage the servers remotely. \nWhile not as fast as the Google Colab instances, we could leave the systems running for days without interruption, something Colab doesn’t support unless paying into a higher tier. The mini PC’s each ran models from Tuesday, April 23rd, 2023, through Sunday, April 28th, 2024. Three of the NUCs ran a single model the entire time, while the fourth switched models once on Wednesday, April 24th, 2024."
  },
  {
    "objectID": "NNets.html#training-process",
    "href": "NNets.html#training-process",
    "title": "Ice Bot: Reinforcement Learning Implementation on Custom Air Hockey Game",
    "section": "Training Process",
    "text": "Training Process\nIn any reinforcement learning project, the training process is crucial for enhancing the AI’s decision-making capabilities. Our training involves managing a combination of replay memory, dual neural networks, and an action-selection mechanism designed to optimize the agent’s learning path. Each component is vital in developing an effective and efficient learning system.\nWe utilize replay memory to store transitions observed at each timestep from which the agent samples. This process helps the agent learn from diverse experiences and avoid the pitfalls of correlated data and non-stationary distributions, enhancing the stability of the training process.\nThe core of our training architecture includes two neural networks. The policy network directly influences the agent’s decision-making by predicting the optimal actions from the current state and is continuously updated in every batch. We also employ a target network to stabilize updates—a slightly outdated copy of the policy network that provides consistent learning targets. This separation helps smooth out the training updates and reduce oscillations in the learning process.\nActions are chosen using an ε-greedy strategy, where the AI predominantly chooses the best action as suggested by the policy network but occasionally selects randomly to explore new strategies. While the proportion of random actions is high initially, it decreases as time progresses. This ensures that the AI does not become trapped in local optima and continues to discover new—potentially superior—strategies.\nWe optimize the policy network by adjusting its parameters to minimize the value of the Huber loss between the predicted Q-values and the Q-values estimated by the target network. This optimization, performed using the gradients of the loss function, is specifically designed to measure how well the network predicts the expected future rewards while being more robust to outliers than traditional mean squared error loss.\nBy integrating these elements, our training process accelerates the agent’s learning efficiency and ensures robustness and adaptability in its gameplay strategy. This approach allows the agent to continuously evolve and adapt to the competitive dynamics of air hockey."
  },
  {
    "objectID": "NNets.html#running-our-models",
    "href": "NNets.html#running-our-models",
    "title": "Ice Bot: Reinforcement Learning Implementation on Custom Air Hockey Game",
    "section": "Running our Models",
    "text": "Running our Models\nWe allocated the majority of our compute cycles to running five different models, each tailored with specific opponent mechanisms and hyperparameters. The models new_ai and old_ai, along with their variants new_ai_750 and old_ai_750, share a common base but differ in their opponent paddle control logic and the TARGET_UPDATE hyperparameter setting. Specifically, old_ai and old_ai_750 face an opponent that only moves vertically to follow the puck, while new_ai and new_ai_750 feature an opponent that moves in two dimensions. The TARGET_UPDATE is set at 1000 for new_ai and old_ai, and at 750 for their respective 750 variants, reflecting how frequently the target network updates. The fifth model, final_run, was designed with minimal logging and checkpoint saving to optimize performance. Unlike the other four, final_run operates in a different render mode and incorporates an additional linear layer in its neural network architecture. This model represents a more streamlined approach with a unique set of hyperparameters including a distinctive epsilon decay rate and increased training episodes.\nBelow are the key differences in hyperparameters among the models:\nCommon Hyperparameters:\n\nBATCH_SIZE: 64\nGAMMA (Discount factor for future rewards): 0.99\nEPS_START (Initial epsilon value for epsilon-greedy action selection): 0.95\nEPS_END (Final epsilon value): 0.05\nLR (Learning rate): 1e-4\nMEMORY_CAPACITY: 10000 (12000 for final_run)\n\nModel-Specific Hyperparameters:\n\nTARGET_UPDATE: Varies between 750 and 1000, except for final_run, which is 800.\nEPS_DECAY: 80000 for all except final_run, which is 606060.\nNUM_EPISODES: 500000 for all except final_run, which is 2500000.\nLinear Layers: 2 for the first four models, 3 for final_run.\n\nThis configuration highlights the tailored approaches we experimented with to optimize each model’s performance and learning efficacy. Our project’s GitHub repository (link at bottom of report) contains detailed source code and further model-specific settings."
  },
  {
    "objectID": "NNets.html#training-results",
    "href": "NNets.html#training-results",
    "title": "Ice Bot: Reinforcement Learning Implementation on Custom Air Hockey Game",
    "section": "Training Results",
    "text": "Training Results\nThe histogram below illustrates our models’ performance throughout their training sessions. The leftmost bin displays instances where the model received a negative reward, indicating losses by the RL agent. The central bin, showing near-zero rewards, indicates instances where neither the agent nor its opponent emerged as a clear winner. The rightmost bin, where the rewards are near 10, marks the occasions where the model successfully won the air hockey games.\n\n\n\nHistogram showing rewards per model\n\n\nThe line plot below tracks the cumulative rewards for each model over time. A downward trend indicates periods when the model consistently lost more games than it won. Conversely, an upward trend signifies phases where the model frequently outperformed the pre-programmed bot opponent.\n\n\n\nLine plot showing the running sum of rewards\n\n\nBelow, explore the progression of our reinforcement learning model as it learns to play air hockey. This video showcases training checkpoints every 25k episodes, arranged from 0 in the top left to 275k in the bottom right, descending column-wise. This video effectively demonstrates the model’s evolving capabilities throughout its training. Note: the RL model controls the right paddle for each game.\n\nContinuously visualizing model performance allowed us to learn that in some instances, the RL algorithm appears to disrupt the game’s physics. Moreover, the model initially learns to manipulate the puck by hitting it backwards against its own wall, leveraging rebounds to score. This unconventional strategy precedes mastering simpler tasks such as straightforward puck hits, presenting a fascinating aspect of the learning process."
  },
  {
    "objectID": "VoterML.html",
    "href": "VoterML.html",
    "title": "Machine Learning",
    "section": "",
    "text": "This tab contains all the information for running Machine Learning on our dataset. Our goal is to predict the political party based on the content of a Reddit post, without having to go through any cleaning, keyword identification or sentiment analysis done in the previous steps. Our hope is that this will be deployable to repeat this project’s polling process without being as resource intensive."
  },
  {
    "objectID": "VoterML.html#overview",
    "href": "VoterML.html#overview",
    "title": "Machine Learning",
    "section": "",
    "text": "This tab contains all the information for running Machine Learning on our dataset. Our goal is to predict the political party based on the content of a Reddit post, without having to go through any cleaning, keyword identification or sentiment analysis done in the previous steps. Our hope is that this will be deployable to repeat this project’s polling process without being as resource intensive."
  },
  {
    "objectID": "VoterML.html#reading-clean-labeled-data",
    "href": "VoterML.html#reading-clean-labeled-data",
    "title": "Machine Learning",
    "section": "Reading Clean & Labeled Data",
    "text": "Reading Clean & Labeled Data\n\n\nCode\n#initialize session\nspark\n\n#import libraries\nfrom azureml.core import Workspace, Dataset, Datastore\nfrom pyspark.sql import functions as F\n\n#read data\nsubscription_id = '21ff0fc0-dd2c-450d-93b7-96eeb3699b22'\nresource_group = 'project-group-11'\nworkspace_name = 'project-group-11'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndatastore = Datastore.get(workspace, \"workspaceblobstore\")\n\ndataset_submissions = Dataset.Tabular.from_parquet_files(path=(datastore, 'cleandata/submissions_labeled_Jude.parquet'))\ndataset_comments = Dataset.Tabular.from_parquet_files(path=(datastore, 'cleandata/comments_labeled_Jude.parquet'))\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 85, 8, Finished, Available, Finished)\n\n\nPerforming interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code BN33DBN5W to authenticate.\nInteractive authentication successfully completed.\n\n\nConvert to Spark Dataframe:\n\n\nCode\ndf_submissions = dataset_submissions.to_spark_dataframe()\ndf_comments = dataset_comments.to_spark_dataframe()\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 85, 9, Finished, Available, Finished)\n\n\n\n\nCode\ndf_submissions.head(1)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 85, 10, Finished, Available, Finished)\n\n\n[Row(id='uirqit', body=\" \\n\\n**Wireless Brain Sensor** **Market Overview:**\\xa0\\n\\nThe study covers the [Wireless Brain Sensor market's ](https://www.maximizemarketresearch.com/market-report/global-wireless-brain-sensor-market/84003/)most recent revenue and market trends. It stresses a market overview, definition, and structure, as well as preventative and pre-planned management. The report focuses on the factors that influence the Wireless Brain Sensor Market, such as gross margin, cost, market share, capacity utilisation, and supply. It also aids in determining the future potential of Wireless Brain Sensor Market in the next years. The report presents a market overview through common subjects that are highlighted with unique data based on the need. This overview aids in making decisions about how to approach the market and comprehending the industry's backdrop.\\n\\n**Request for free sample:**\\xa0\\n\\n[**https://www.maximizemarketresearch.com/request-sample/84003**](https://www.maximizemarketresearch.com/request-sample/84003) \\n\\n**Market Scope:**\\n\\n* **Key Market Trends &amp; Challenges:**\\n\\nThe study analyses the primary adoption trend impacting the Wireless Brain Sensor industry as well as issues that may stymie its expansion. Understanding these elements is critical for product planning and design, as well as commercial strategies. To assist you understand the Wireless Brain Sensor market, this study provides a full analysis of these trends and obstacles**.**\\n\\n* **Market Statistics:**\\n\\nThe report provides the market size and share of the Wireless Brain Sensor market. It helps in understanding the market and the report estimates upfront data and statistics that make the report a very valuable guide for individuals dealing with advertising and industry decision-making processes in the Wireless Brain Sensor market.\\n\\n**Get more Report Details**\\xa0\\n\\n[https://www.maximizemarketresearch.com/market-report/global-wireless-brain-sensor-market/84003/](https://www.maximizemarketresearch.com/market-report/global-wireless-brain-sensor-market/84003/) \\n\\n**Key Players:**\\n\\n• Muse  \\n• Natus Medical Incorporated  \\n• Cadwell Industries Inc.  \\n• NeuroWave Systems Inc.  \\n• BrainScope.  \\n• Hangzhou Zhongheng Electric  \\n• Deayea  \\n• NeuroTherapeutics  \\n• Melon\\n\\n**Regional Analysis:**\\n\\nGeographically, this report is segmented into several key countries, with market size, growth rate, import and export of Wireless Brain Sensor market in these countries, which covering North America, U.S., Canada, Mexico, Europe, UK, Germany, France, Spain, Italy, Rest of Europe, Asia Pacific, China, India, Japan, Australia, South Korea, ASEAN Countries, Rest of APAC, South America, Brazil, and Middle East and Africa.\\n\\n**COVID-19 Impact Analysis on** **Wireless Brain Sensor** **Market:**\\n\\nThe report has identified detailed impact of COVID-19 on Wireless Brain Sensor market in regions such as North America, Asia Pacific, Middle-East, Europe, and South America. The report provides Comprehensive analysis on alternatives, difficult conditions, and difficult scenarios of Wireless Brain Sensor market during this crisis. The report briefly elaborates the advantages as well as the difficulties in terms of finance and market growth attained during the COVID-19. In addition, report offers a set of concepts, which is expected to aid readers in deciding and planning a strategy for their business.\\n\\n**Key Questions answered in the** **Wireless Brain Sensor** **Market Report are:**\\xa0\\n\\n* Which product segment grabbed the largest share in the Wireless Brain Sensor market?\\n* How is the competitive scenario of the Wireless Brain Sensor market?\\n* Which are the key factors aiding the Wireless Brain Sensor market growth?\\n* Which region holds the maximum share in the Wireless Brain Sensor market?\\n* What will be the CAGR of the Wireless Brain Sensor market during the forecast period?\\n* Which application segment emerged as the leading segment in the Wireless Brain Sensor market?\\n* Which are the prominent players in the Wireless Brain Sensor market?\\n* What key trends are likely to emerge in the Wireless Brain Sensor market in the coming years?\\n* What will be the Wireless Brain Sensor market size by 2027?\\n* Which company held the largest share in the Wireless Brain Sensor market?\\n\\n**About Us:**\\n\\nMaximize Market Research provides B2B and B2C research on 12000 high growth emerging opportunities &amp; technologies as well as threats to the companies across the Healthcare, Pharmaceuticals, Electronics &amp; Communications, Internet of Things, Food and Beverages, Aerospace and Defence and other manufacturing sectors.\\n\\n**Contact Us:**\\n\\nMAXIMIZE MARKET RESEARCH PVT. LTD.\\n\\n3rd Floor, Navale IT Park Phase 2,\\n\\nPune Bangalore Highway,\\n\\nNarhe, Pune, Maharashtra 411041, India.\\n\\nEmail: sales@maximizemarketresearch.com\\n\\nPhone No.: +91 20 6630 3320\\n\\nWebsite: www.maximizemarketresearch.com\", year=2022, month=5, simple_sentiment='positive', Party='No Party')]\n\n\n\n\nCode\ndf_comments.head(1)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 14, Finished, Available, Finished)\n\n\n[Row(id='uirqit', body=\" \\n\\n**Wireless Brain Sensor** **Market Overview:**\\xa0\\n\\nThe study covers the [Wireless Brain Sensor market's ](https://www.maximizemarketresearch.com/market-report/global-wireless-brain-sensor-market/84003/)most recent revenue and market trends. It stresses a market overview, definition, and structure, as well as preventative and pre-planned management. The report focuses on the factors that influence the Wireless Brain Sensor Market, such as gross margin, cost, market share, capacity utilisation, and supply. It also aids in determining the future potential of Wireless Brain Sensor Market in the next years. The report presents a market overview through common subjects that are highlighted with unique data based on the need. This overview aids in making decisions about how to approach the market and comprehending the industry's backdrop.\\n\\n**Request for free sample:**\\xa0\\n\\n[**https://www.maximizemarketresearch.com/request-sample/84003**](https://www.maximizemarketresearch.com/request-sample/84003) \\n\\n**Market Scope:**\\n\\n* **Key Market Trends &amp; Challenges:**\\n\\nThe study analyses the primary adoption trend impacting the Wireless Brain Sensor industry as well as issues that may stymie its expansion. Understanding these elements is critical for product planning and design, as well as commercial strategies. To assist you understand the Wireless Brain Sensor market, this study provides a full analysis of these trends and obstacles**.**\\n\\n* **Market Statistics:**\\n\\nThe report provides the market size and share of the Wireless Brain Sensor market. It helps in understanding the market and the report estimates upfront data and statistics that make the report a very valuable guide for individuals dealing with advertising and industry decision-making processes in the Wireless Brain Sensor market.\\n\\n**Get more Report Details**\\xa0\\n\\n[https://www.maximizemarketresearch.com/market-report/global-wireless-brain-sensor-market/84003/](https://www.maximizemarketresearch.com/market-report/global-wireless-brain-sensor-market/84003/) \\n\\n**Key Players:**\\n\\n• Muse  \\n• Natus Medical Incorporated  \\n• Cadwell Industries Inc.  \\n• NeuroWave Systems Inc.  \\n• BrainScope.  \\n• Hangzhou Zhongheng Electric  \\n• Deayea  \\n• NeuroTherapeutics  \\n• Melon\\n\\n**Regional Analysis:**\\n\\nGeographically, this report is segmented into several key countries, with market size, growth rate, import and export of Wireless Brain Sensor market in these countries, which covering North America, U.S., Canada, Mexico, Europe, UK, Germany, France, Spain, Italy, Rest of Europe, Asia Pacific, China, India, Japan, Australia, South Korea, ASEAN Countries, Rest of APAC, South America, Brazil, and Middle East and Africa.\\n\\n**COVID-19 Impact Analysis on** **Wireless Brain Sensor** **Market:**\\n\\nThe report has identified detailed impact of COVID-19 on Wireless Brain Sensor market in regions such as North America, Asia Pacific, Middle-East, Europe, and South America. The report provides Comprehensive analysis on alternatives, difficult conditions, and difficult scenarios of Wireless Brain Sensor market during this crisis. The report briefly elaborates the advantages as well as the difficulties in terms of finance and market growth attained during the COVID-19. In addition, report offers a set of concepts, which is expected to aid readers in deciding and planning a strategy for their business.\\n\\n**Key Questions answered in the** **Wireless Brain Sensor** **Market Report are:**\\xa0\\n\\n* Which product segment grabbed the largest share in the Wireless Brain Sensor market?\\n* How is the competitive scenario of the Wireless Brain Sensor market?\\n* Which are the key factors aiding the Wireless Brain Sensor market growth?\\n* Which region holds the maximum share in the Wireless Brain Sensor market?\\n* What will be the CAGR of the Wireless Brain Sensor market during the forecast period?\\n* Which application segment emerged as the leading segment in the Wireless Brain Sensor market?\\n* Which are the prominent players in the Wireless Brain Sensor market?\\n* What key trends are likely to emerge in the Wireless Brain Sensor market in the coming years?\\n* What will be the Wireless Brain Sensor market size by 2027?\\n* Which company held the largest share in the Wireless Brain Sensor market?\\n\\n**About Us:**\\n\\nMaximize Market Research provides B2B and B2C research on 12000 high growth emerging opportunities &amp; technologies as well as threats to the companies across the Healthcare, Pharmaceuticals, Electronics &amp; Communications, Internet of Things, Food and Beverages, Aerospace and Defence and other manufacturing sectors.\\n\\n**Contact Us:**\\n\\nMAXIMIZE MARKET RESEARCH PVT. LTD.\\n\\n3rd Floor, Navale IT Park Phase 2,\\n\\nPune Bangalore Highway,\\n\\nNarhe, Pune, Maharashtra 411041, India.\\n\\nEmail: sales@maximizemarketresearch.com\\n\\nPhone No.: +91 20 6630 3320\\n\\nWebsite: www.maximizemarketresearch.com\", year=2022, month=5, simple_sentiment='positive', Party='No Party')]"
  },
  {
    "objectID": "VoterML.html#stratified-sampling",
    "href": "VoterML.html#stratified-sampling",
    "title": "Machine Learning",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\nIn order to perform ML, we used a stratified sampling method to grab 100k data points from our clean and labeled dataset. These data points will be well balanced and will be used to train our models.\nWe decided to only use the body of the post as our single input feature in order to maximize efficiency. Therefore, our ML dataframe once created only contained 2 features: the body of the text of the post, and the assigned party label.\n\n\nCode\n# Step 1: select specific colums\ndf_submissions_selected = df_submissions.select(\"body\", \"Party\")\ndf_comments_selected = df_comments.select(\"body\", \"Party\")\n\n# Step 2: combine dataframes\ndf_combined = df_submissions_selected.union(df_comments_selected)\n\n# Step 3: calculate counts and proportions to sample equal number\nclass_counts = df_combined.groupBy(\"Party\").count().collect()\ntotal_count = sum(row['count'] for row in class_counts)\ndesired_per_class = 100000 // len(class_counts)  # Target per class\nproportions = {\n    row['Party']: min(desired_per_class / row['count'], 1.0) for row in class_counts\n}\n\n# Step 4: perform stratified sampling\ndf_sampled = df_combined.sampleBy(\"Party\", fractions=proportions, seed=42)\n\n# Step 5: ensure the resulting dataset has exactly 100k rows\ndf_sampled_limited = df_sampled.limit(100000)\n\n# Step 6: double check\ndf_sampled_limited.groupBy(\"Party\").count().show()\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 16, Finished, Available, Finished)\n\n\n+----------+-----+\n|     Party|count|\n+----------+-----+\n|  No Party|33224|\n|Republican|33283|\n|  Democrat|33493|\n+----------+-----+\n\n\n\n\n\nCode\n# Show first 5 rows\ndf_sampled_limited.show(5)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 18, Finished, Available, Finished)\n\n\n+--------------------+----------+\n|                body|     Party|\n+--------------------+----------+\n| A deluded but aw...|Republican|\n|I've been thinkin...|  No Party|\n| \\n\\n# Topic: Mar...|  Democrat|\n| \\n\\nGood morning...|  Democrat|\n|\\nWelcome to /r/L...|Republican|\n+--------------------+----------+\nonly showing top 5 rows\n\n\n\nNow that we had our well balanced dataset, we could move on to ML:"
  },
  {
    "objectID": "VoterML.html#machine-learning",
    "href": "VoterML.html#machine-learning",
    "title": "Machine Learning",
    "section": "Machine Learning",
    "text": "Machine Learning\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 40, Finished, Available, Finished)\n\n\nRepartitioning the data and converting to Pandas in order to proceed with our ML algorithms:\n\n\nCode\ndf_sampled_limited = df_sampled_limited.repartition(100)  # Adjust the number of partitions\ndf_sampled_limited_pd = df_sampled_limited.toPandas()\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 33, Finished, Available, Finished)\n\n\n\nVectorization and Encoding\nBefore training our model, the text data from the body input feature needed to be vectorized, while the Party label feature needed to be encoded:\n\n\nCode\n# text vectorization\nvectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\nX = vectorizer.fit_transform(df_sampled_limited_pd['body'])  # 'body' column as features\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 35, Finished, Available, Finished)\n\n\n\n\nCode\n# encode Party label as numeric labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(df_sampled_limited_pd['Party'])  # Encode 'Democrat', 'No Party', 'Republican' to 0, 1, 2\n\n# split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 41, Finished, Available, Finished)\n\n\n\n\nModel 1: Random Forest\nAs our first classifier, we selected Random Forest (RF). As an ensemble method, the RF combines multiple decision trees to improve predictive accuracy and robustness, making it well-suited for handling noisy text data.\n\n\nCode\n# build and train the Random Forest model\nrf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# make predictions\nrf_predictions = rf_model.predict(X_test)\n\n# evaluate\nrf_accuracy = accuracy_score(y_test, rf_predictions)\nrf_confusion = confusion_matrix(y_test, rf_predictions)\nrf_classification_report = classification_report(y_test, rf_predictions)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 42, Finished, Available, Finished)\n\n\n\n\nCode\nprint(\"Random Forest - Accuracy: \", rf_accuracy)\nprint(\"Random Forest - Classification Report:\\n\", rf_classification_report)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 43, Finished, Available, Finished)\n\n\nRandom Forest - Accuracy:  0.67935\nRandom Forest - Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.74      0.56      0.64      6700\n           1       0.65      0.94      0.77      6645\n           2       0.67      0.54      0.60      6655\n\n    accuracy                           0.68     20000\n   macro avg       0.69      0.68      0.67     20000\nweighted avg       0.69      0.68      0.67     20000\n\n\n\n\n\nModel 2: XGBoost\nAs our second model, we selected the XGBoost, which is a gradient boosted technique that offers efficient learning and fine-tuning capabilities, enabling it to capture nuanced patterns in text.\n\n\nCode\n# build and train the XGBoost model\nxgb_model = XGBClassifier(n_estimators=100, max_depth=10, learning_rate=0.1, random_state=42)\nxgb_model.fit(X_train, y_train)\n\n# make predictions \nxgb_predictions = xgb_model.predict(X_test)\n\n# evaluate\nxgb_accuracy = accuracy_score(y_test, xgb_predictions)\nxgb_confusion = confusion_matrix(y_test, xgb_predictions)\nxgb_classification_report = classification_report(y_test, xgb_predictions)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 44, Finished, Available, Finished)\n\n\n\n\nCode\nprint(\"XGBoost - Accuracy: \", xgb_accuracy)\nprint(\"XGBoost - Classification Report:\\n\", xgb_classification_report)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 45, Finished, Available, Finished)\n\n\nXGBoost - Accuracy:  0.8271\nXGBoost - Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.78      0.77      0.78      6700\n           1       0.89      0.98      0.93      6645\n           2       0.80      0.73      0.76      6655\n\n    accuracy                           0.83     20000\n   macro avg       0.82      0.83      0.82     20000\nweighted avg       0.82      0.83      0.82     20000\n\n\n\nFinally, we plotted the confusion matrices for both models:\n\n\nCode\n# creating plot\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# RF Confusion Matrix\nsns.heatmap(rf_confusion, annot=True, fmt='d', cmap='Blues',\n            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, ax=axes[0])\naxes[0].set_title(\"Random Forest Confusion Matrix\")\naxes[0].set_xlabel(\"Predicted\")\naxes[0].set_ylabel(\"Actual\")\n\n# xgb Confusion Matrix\nsns.heatmap(xgb_confusion, annot=True, fmt='d', cmap='Blues',\n            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, ax=axes[1])\naxes[1].set_title(\"XGBoost Confusion Matrix\")\naxes[1].set_xlabel(\"Predicted\")\naxes[1].set_ylabel(\"Actual\")\n\nplt.tight_layout()\nplt.show()\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 46, Finished, Available, Finished)"
  },
  {
    "objectID": "VoterML.html#model-optimization",
    "href": "VoterML.html#model-optimization",
    "title": "Machine Learning",
    "section": "Model Optimization",
    "text": "Model Optimization\nThe XGB model shows a higher accuracy of 83%, compared to our Random Forest model with accuracy of 68%. However, we haven’t optimized the models yet.\nThe following section shows how we would optimize the models prior to selecting which model performs best.\nPlease note that the cells below have not been run on this notebook or as a job due to resource constraints, as we have already used the majority of our allocated funding in Azure. In theory, we would be using the Random Search function to select the best hyperparameters for each model, as shown below.\nWe feel that the XGB model, while not optimized, still provides a pretty good accuracy for predicting the political party based on the body of a post.\n\nRandom Forest Optimization\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 47, Finished, Available, Finished)\n\n\n\n\nCode\n# Random Forest hyperparameter space\nrf_param_grid = {\n    'n_estimators': [100, 200, 300, 500],\n    'max_depth': [10, 20, 30, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nrf_random = RandomizedSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    param_distributions=rf_param_grid,\n    n_iter=50,  # Number of combinations to try\n    scoring='accuracy',\n    cv=3,  # 3-fold cross-validation\n    verbose=2,\n    random_state=42,\n    n_jobs=-1  # Use all available cores\n)\n\n# Step 3: Fit the models using RandomizedSearchCV\nprint(\"Tuning Random Forest...\")\nrf_random.fit(X_train, y_train)\n\n# Step 4: Evaluate the best models\nprint(\"Best parameters for Random Forest:\", rf_random.best_params_)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 81, 48, Cancelled, Cancelling, Cancelling)\n\n\nTuning Random Forest...\nFitting 3 folds for each of 50 candidates, totalling 150 fits\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 2.3min\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 2.2min\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 2.2min\n\n\n\n\nCode\n# Get the best estimators\nbest_rf = rf_random.best_estimator_\nrf_predictions = best_rf.predict(X_test)\n\n# Evaluate Random Forest\nrf_accuracy = accuracy_score(y_test, rf_predictions)\nrf_classification_report = classification_report(y_test, rf_predictions)\nprint(f\"Optimized Random Forest Accuracy: {rf_accuracy}\")\nprint(\"Random Forest Classification Report:\\n\", rf_classification_report)\n\n\n\n\nXGBoost Optimization\n\n\nCode\n# XGBoost hyperparameter space\nxgb_param_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 10],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0]\n}\n\n# XGBoost\nxgb_random = RandomizedSearchCV(\n    estimator=XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n    param_distributions=xgb_param_grid,\n    n_iter=50,  # Number of combinations to try\n    scoring='accuracy',\n    cv=3,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\nprint(\"Tuning XGBoost...\")\nxgb_random.fit(X_train, y_train)\n\nprint(\"Best parameters for XGBoost:\", xgb_random.best_params_)\n\n\n\n\nCode\nbest_xgb = xgb_random.best_estimator_\nxgb_predictions = best_xgb.predict(X_test)\n\n# Evaluate XGBoost\nxgb_accuracy = accuracy_score(y_test, xgb_predictions)\nxgb_classification_report = classification_report(y_test, xgb_predictions)\nprint(f\"Optimized XGBoost Accuracy: {xgb_accuracy}\")\nprint(\"XGBoost Classification Report:\\n\", xgb_classification_report)"
  },
  {
    "objectID": "CIFAR2.html",
    "href": "CIFAR2.html",
    "title": "CNN Multi-class classification Pytorch",
    "section": "",
    "text": "In this lab we perform multi-class classification with a CNN using PyTorch AND Keras.\nSubmission:\n\nYou need to upload TWO documents to Canvas when you are done\n\n\nA PDF (or HTML) of the completed form of the completed MNIST.ipynb notebook (MNIST in Keras)\n\n\nA PDF (or HTML) of the completed form of the completed CIFAR.ipynb notebook (CIFAR in PyTorch)\n\n\nThe final uploaded version should NOT have any code-errors present\nAll outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\nInstructions\n\nCode Multi-class classification using LeNet type architecture CNN to train\n\n\nMNIST training with Keras, in MNIST.ipynb.\n\n\nCIFAR training with PyTorch, in CIFAR.ipynb.\n\nIf you use an online reference, please cite the source and don’t copy more than a few lines of code\nNormalize the data as needed\nVisualize the results at the end where possible\nPartition data into training, validation, and test\nMonitor training and validation throughout training by plotting\nFor the final fit, report the confusion matrix at the end\nPrint training, validation, and test errors at the very end\nYou MUST use early stopping:\nDo basic MANUAL hyper-parameter tuning to try to achieve an optimal fit model\n\ni.e. best training/validation loss without over-fitting\nExplore L1 and L2 regularization and dropout\nExplore different optimizers\nUse the loss functions specified in the textbook\nExplore different options for activation functions, network size/depth, etc\n\n\nDocument what is going on in the code, as needed, with narrative markdown text between cells.\nSubmit the version with hyper parameters that provide the optimal fit\n\ni.e. you don’t need to show the outputs of your hyper-parameter tuning process"
  },
  {
    "objectID": "CIFAR2.html#lab-3.1-cnn-multi-class-classification",
    "href": "CIFAR2.html#lab-3.1-cnn-multi-class-classification",
    "title": "CNN Multi-class classification Pytorch",
    "section": "",
    "text": "In this lab we perform multi-class classification with a CNN using PyTorch AND Keras.\nSubmission:\n\nYou need to upload TWO documents to Canvas when you are done\n\n\nA PDF (or HTML) of the completed form of the completed MNIST.ipynb notebook (MNIST in Keras)\n\n\nA PDF (or HTML) of the completed form of the completed CIFAR.ipynb notebook (CIFAR in PyTorch)\n\n\nThe final uploaded version should NOT have any code-errors present\nAll outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\nInstructions\n\nCode Multi-class classification using LeNet type architecture CNN to train\n\n\nMNIST training with Keras, in MNIST.ipynb.\n\n\nCIFAR training with PyTorch, in CIFAR.ipynb.\n\nIf you use an online reference, please cite the source and don’t copy more than a few lines of code\nNormalize the data as needed\nVisualize the results at the end where possible\nPartition data into training, validation, and test\nMonitor training and validation throughout training by plotting\nFor the final fit, report the confusion matrix at the end\nPrint training, validation, and test errors at the very end\nYou MUST use early stopping:\nDo basic MANUAL hyper-parameter tuning to try to achieve an optimal fit model\n\ni.e. best training/validation loss without over-fitting\nExplore L1 and L2 regularization and dropout\nExplore different optimizers\nUse the loss functions specified in the textbook\nExplore different options for activation functions, network size/depth, etc\n\n\nDocument what is going on in the code, as needed, with narrative markdown text between cells.\nSubmit the version with hyper parameters that provide the optimal fit\n\ni.e. you don’t need to show the outputs of your hyper-parameter tuning process"
  },
  {
    "objectID": "VoterData_Collection.html",
    "href": "VoterData_Collection.html",
    "title": "Data Collection",
    "section": "",
    "text": "This tab contains all the information related to the process of data collection, initial filtering & cleaning, and some initial exploratory data analysis (EDA)."
  },
  {
    "objectID": "VoterData_Collection.html#overview",
    "href": "VoterData_Collection.html#overview",
    "title": "Data Collection",
    "section": "",
    "text": "This tab contains all the information related to the process of data collection, initial filtering & cleaning, and some initial exploratory data analysis (EDA)."
  },
  {
    "objectID": "VoterData_Collection.html#introduction",
    "href": "VoterData_Collection.html#introduction",
    "title": "Data Collection",
    "section": "Introduction",
    "text": "Introduction\nThe data provided for this project contains all the posts of Reddit from January 2022 to March 2023. It has been hosted in Azure blobs by our DSAN 6000 professors at Georgetown University. The data is divided into 2 files, submissions and comments. While they follow a similar structure, they have some different attributes and names. The submissions file refers to the posts made by users, while the comments file refers to the comments made by users on the posts. Thus, the comments file is much larger than the submissions file. Underneath you can see the first 5 rows of each file after selecting the entries that we will be using on this project.\n\nA note on terminology\nYou may see us refer to submissions and comments directly, or more generally to “posts” - over the course of the project, we settled on using the term “post” to refer to any single row of textual data used in our pipeline, which is inclusive of both submissions & comments."
  },
  {
    "objectID": "VoterData_Collection.html#cleaning-process-summary",
    "href": "VoterData_Collection.html#cleaning-process-summary",
    "title": "Data Collection",
    "section": "Cleaning Process Summary",
    "text": "Cleaning Process Summary\nThe cleaning was submitted as a job in Azure. The actual file can be found under the folder spark_jobs/Clean_All_Data/ in the file Clean_job.py. The cleaning process was done in the following steps:\n\nLoaded the data from the Azure blob.\nRemoved subreddits that would skew the perspective of the data.\n\ndemocrats\nRepublican\nThe_Donald\nEnoughTrumpSpam\nFuckthealtright\nCommunism\nFullCommunism\nAnarchism\nAntiWork\nGenZedong\nConspiracy\nDarkEnlightenment\nNeoReaction\nIdentitarian\nFascist\nSocialism\nConservative\nPatriot\nConsumeProduct\n\nRemoved posts that were not from January 2022 until November 2022.\nRemoved all the posts that were empty, [removed], or [deleted].\nThen, we filtered to only select the posts that were politics related. To do so wey used certain keywords to filter the data. This keywords can be found in the folder data/keywords/.\nDue to our limitations, we could only process english posts, so we filtered the data to not include any other language.\n\nBelow is a breakdown of this process:"
  },
  {
    "objectID": "VoterData_Collection.html#code-process",
    "href": "VoterData_Collection.html#code-process",
    "title": "Data Collection",
    "section": "Code & Process",
    "text": "Code & Process\n\nGetting the Data\nWe used a custom environment that contained NLTK and Langdetect packages which were then used to clean and filter our Reddit data\nFirst, we imported necessary packages and initialized the spark session.\n\n\nCode\n#initialize the session\nspark\n\n# Importing necessary libraries for English language detect and text lematization\nimport langdetect\nfrom langdetect import detect\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import BooleanType\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\n\n\nIn order to collect the data, we connected to the Azure Blob where the Reddit data is housed and grabbed the Comments and Submissions datasets\n\n\nCode\nblob_account_name = \"dsan6000fall2024\"\nblob_container_name = \"reddit-project\"\nwasbs_base_url = (\n    f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/\"\n)\n\n# defining paths for comments and submissions\ncomments_path = f\"{wasbs_base_url}202101-202303/comments/\"\nsubmissions_path = f\"{wasbs_base_url}202101-202303/submissions/\"\n\n# printing to verify paths\nprint(\"Comments Path:\", comments_path)\nprint(\"Submissions Path:\", submissions_path)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 83, 7, Finished, Available, Finished)\n\n\nComments Path: wasbs://reddit-project@dsan6000fall2024.blob.core.windows.net/202101-202303/comments/\nSubmissions Path: wasbs://reddit-project@dsan6000fall2024.blob.core.windows.net/202101-202303/submissions/\n\n\n\n\nCode\n# reading the parquet files\ncomments_df = spark.read.parquet(comments_path)\nsubmissions_df = spark.read.parquet(submissions_path)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 83, 8, Finished, Available, Finished)\n\n\nOnce we had successfully read the data sets, we went ahead and performed simple EDA to get an idea of how many data points we were working with, as well as the data schemas\n\n\nCode\n# counting number of rows for each\ncomments_count = comments_df.count()\nsubmissions_count = submissions_df.count()\n\nprint(f\"Number of comments: {comments_count}\")\nprint(f\"Number of submissions: {submissions_count}\")\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 5, 11, Finished, Available, Finished)\n\n\nNumber of comments: 6114480450\nNumber of submissions: 892160821\n\n\n\n\nCode\n# printing Comments schema\nprint(\"Comments Schema:\")\ncomments_df.printSchema()\n\n\nStatementMeta(11387b7c-91c2-4dd1-a4f8-a460b59b7bdd, 14, 10, Finished, Available, Finished)\n\n\nComments Schema:\nroot\n |-- author: string (nullable = true)\n |-- author_cakeday: boolean (nullable = true)\n |-- author_flair_css_class: string (nullable = true)\n |-- author_flair_text: string (nullable = true)\n |-- body: string (nullable = true)\n |-- can_gild: boolean (nullable = true)\n |-- controversiality: long (nullable = true)\n |-- created_utc: timestamp (nullable = true)\n |-- distinguished: string (nullable = true)\n |-- edited: string (nullable = true)\n |-- gilded: long (nullable = true)\n |-- id: string (nullable = true)\n |-- is_submitter: boolean (nullable = true)\n |-- link_id: string (nullable = true)\n |-- parent_id: string (nullable = true)\n |-- permalink: string (nullable = true)\n |-- retrieved_on: timestamp (nullable = true)\n |-- score: long (nullable = true)\n |-- stickied: boolean (nullable = true)\n |-- subreddit: string (nullable = true)\n |-- subreddit_id: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n\n\n\n\n\nCode\n# printing Submissions schema\nprint(\"Submissions Schema:\")\nsubmissions_df.printSchema()\n\n\nStatementMeta(11387b7c-91c2-4dd1-a4f8-a460b59b7bdd, 14, 11, Finished, Available, Finished)\n\n\nSubmissions Schema:\nroot\n |-- adserver_click_url: string (nullable = true)\n |-- adserver_imp_pixel: string (nullable = true)\n |-- archived: boolean (nullable = true)\n |-- author: string (nullable = true)\n |-- author_cakeday: boolean (nullable = true)\n |-- author_flair_css_class: string (nullable = true)\n |-- author_flair_text: string (nullable = true)\n |-- author_id: string (nullable = true)\n |-- brand_safe: boolean (nullable = true)\n |-- contest_mode: boolean (nullable = true)\n |-- created_utc: timestamp (nullable = true)\n |-- crosspost_parent: string (nullable = true)\n |-- crosspost_parent_list: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- approved_at_utc: string (nullable = true)\n |    |    |-- approved_by: string (nullable = true)\n |    |    |-- archived: boolean (nullable = true)\n |    |    |-- author: string (nullable = true)\n |    |    |-- author_flair_css_class: string (nullable = true)\n |    |    |-- author_flair_text: string (nullable = true)\n |    |    |-- banned_at_utc: string (nullable = true)\n |    |    |-- banned_by: string (nullable = true)\n |    |    |-- brand_safe: boolean (nullable = true)\n |    |    |-- can_gild: boolean (nullable = true)\n |    |    |-- can_mod_post: boolean (nullable = true)\n |    |    |-- clicked: boolean (nullable = true)\n |    |    |-- contest_mode: boolean (nullable = true)\n |    |    |-- created: double (nullable = true)\n |    |    |-- created_utc: double (nullable = true)\n |    |    |-- distinguished: string (nullable = true)\n |    |    |-- domain: string (nullable = true)\n |    |    |-- downs: long (nullable = true)\n |    |    |-- edited: boolean (nullable = true)\n |    |    |-- gilded: long (nullable = true)\n |    |    |-- hidden: boolean (nullable = true)\n |    |    |-- hide_score: boolean (nullable = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- is_crosspostable: boolean (nullable = true)\n |    |    |-- is_reddit_media_domain: boolean (nullable = true)\n |    |    |-- is_self: boolean (nullable = true)\n |    |    |-- is_video: boolean (nullable = true)\n |    |    |-- likes: string (nullable = true)\n |    |    |-- link_flair_css_class: string (nullable = true)\n |    |    |-- link_flair_text: string (nullable = true)\n |    |    |-- locked: boolean (nullable = true)\n |    |    |-- media: string (nullable = true)\n |    |    |-- mod_reports: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- num_comments: long (nullable = true)\n |    |    |-- num_crossposts: long (nullable = true)\n |    |    |-- num_reports: string (nullable = true)\n |    |    |-- over_18: boolean (nullable = true)\n |    |    |-- parent_whitelist_status: string (nullable = true)\n |    |    |-- permalink: string (nullable = true)\n |    |    |-- pinned: boolean (nullable = true)\n |    |    |-- quarantine: boolean (nullable = true)\n |    |    |-- removal_reason: string (nullable = true)\n |    |    |-- report_reasons: string (nullable = true)\n |    |    |-- saved: boolean (nullable = true)\n |    |    |-- score: long (nullable = true)\n |    |    |-- secure_media: string (nullable = true)\n |    |    |-- selftext: string (nullable = true)\n |    |    |-- selftext_html: string (nullable = true)\n |    |    |-- spoiler: boolean (nullable = true)\n |    |    |-- stickied: boolean (nullable = true)\n |    |    |-- subreddit: string (nullable = true)\n |    |    |-- subreddit_id: string (nullable = true)\n |    |    |-- subreddit_name_prefixed: string (nullable = true)\n |    |    |-- subreddit_type: string (nullable = true)\n |    |    |-- suggested_sort: string (nullable = true)\n |    |    |-- thumbnail: string (nullable = true)\n |    |    |-- thumbnail_height: string (nullable = true)\n |    |    |-- thumbnail_width: string (nullable = true)\n |    |    |-- title: string (nullable = true)\n |    |    |-- ups: long (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user_reports: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- view_count: string (nullable = true)\n |    |    |-- visited: boolean (nullable = true)\n |    |    |-- whitelist_status: string (nullable = true)\n |-- disable_comments: boolean (nullable = true)\n |-- distinguished: string (nullable = true)\n |-- domain: string (nullable = true)\n |-- domain_override: string (nullable = true)\n |-- edited: string (nullable = true)\n |-- embed_type: string (nullable = true)\n |-- embed_url: string (nullable = true)\n |-- gilded: long (nullable = true)\n |-- hidden: boolean (nullable = true)\n |-- hide_score: boolean (nullable = true)\n |-- href_url: string (nullable = true)\n |-- id: string (nullable = true)\n |-- imp_pixel: string (nullable = true)\n |-- is_crosspostable: boolean (nullable = true)\n |-- is_reddit_media_domain: boolean (nullable = true)\n |-- is_self: boolean (nullable = true)\n |-- is_video: boolean (nullable = true)\n |-- link_flair_css_class: string (nullable = true)\n |-- link_flair_text: string (nullable = true)\n |-- locked: boolean (nullable = true)\n |-- media: struct (nullable = true)\n |    |-- event_id: string (nullable = true)\n |    |-- oembed: struct (nullable = true)\n |    |    |-- author_name: string (nullable = true)\n |    |    |-- author_url: string (nullable = true)\n |    |    |-- cache_age: long (nullable = true)\n |    |    |-- description: string (nullable = true)\n |    |    |-- height: long (nullable = true)\n |    |    |-- html: string (nullable = true)\n |    |    |-- provider_name: string (nullable = true)\n |    |    |-- provider_url: string (nullable = true)\n |    |    |-- thumbnail_height: long (nullable = true)\n |    |    |-- thumbnail_url: string (nullable = true)\n |    |    |-- thumbnail_width: long (nullable = true)\n |    |    |-- title: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- version: string (nullable = true)\n |    |    |-- width: long (nullable = true)\n |    |-- reddit_video: struct (nullable = true)\n |    |    |-- dash_url: string (nullable = true)\n |    |    |-- duration: long (nullable = true)\n |    |    |-- fallback_url: string (nullable = true)\n |    |    |-- height: long (nullable = true)\n |    |    |-- hls_url: string (nullable = true)\n |    |    |-- is_gif: boolean (nullable = true)\n |    |    |-- scrubber_media_url: string (nullable = true)\n |    |    |-- transcoding_status: string (nullable = true)\n |    |    |-- width: long (nullable = true)\n |    |-- type: string (nullable = true)\n |-- media_embed: struct (nullable = true)\n |    |-- content: string (nullable = true)\n |    |-- height: long (nullable = true)\n |    |-- scrolling: boolean (nullable = true)\n |    |-- width: long (nullable = true)\n |-- mobile_ad_url: string (nullable = true)\n |-- num_comments: long (nullable = true)\n |-- num_crossposts: long (nullable = true)\n |-- original_link: string (nullable = true)\n |-- over_18: boolean (nullable = true)\n |-- parent_whitelist_status: string (nullable = true)\n |-- permalink: string (nullable = true)\n |-- pinned: boolean (nullable = true)\n |-- post_hint: string (nullable = true)\n |-- preview: struct (nullable = true)\n |    |-- enabled: boolean (nullable = true)\n |    |-- images: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- resolutions: array (nullable = true)\n |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |-- height: long (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |-- width: long (nullable = true)\n |    |    |    |-- source: struct (nullable = true)\n |    |    |    |    |-- height: long (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- width: long (nullable = true)\n |    |    |    |-- variants: struct (nullable = true)\n |    |    |    |    |-- gif: struct (nullable = true)\n |    |    |    |    |    |-- resolutions: array (nullable = true)\n |    |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |    |-- height: long (nullable = true)\n |    |    |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |    |    |-- width: long (nullable = true)\n |    |    |    |    |    |-- source: struct (nullable = true)\n |    |    |    |    |    |    |-- height: long (nullable = true)\n |    |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |    |-- width: long (nullable = true)\n |    |    |    |    |-- mp4: struct (nullable = true)\n |    |    |    |    |    |-- resolutions: array (nullable = true)\n |    |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |    |-- height: long (nullable = true)\n |    |    |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |    |    |-- width: long (nullable = true)\n |    |    |    |    |    |-- source: struct (nullable = true)\n |    |    |    |    |    |    |-- height: long (nullable = true)\n |    |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |    |-- width: long (nullable = true)\n |    |    |    |    |-- nsfw: struct (nullable = true)\n |    |    |    |    |    |-- resolutions: array (nullable = true)\n |    |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |    |-- height: long (nullable = true)\n |    |    |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |    |    |-- width: long (nullable = true)\n |    |    |    |    |    |-- source: struct (nullable = true)\n |    |    |    |    |    |    |-- height: long (nullable = true)\n |    |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |    |-- width: long (nullable = true)\n |    |    |    |    |-- obfuscated: struct (nullable = true)\n |    |    |    |    |    |-- resolutions: array (nullable = true)\n |    |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |    |-- height: long (nullable = true)\n |    |    |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |    |    |-- width: long (nullable = true)\n |    |    |    |    |    |-- source: struct (nullable = true)\n |    |    |    |    |    |    |-- height: long (nullable = true)\n |    |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |    |-- width: long (nullable = true)\n |-- promoted: boolean (nullable = true)\n |-- promoted_by: string (nullable = true)\n |-- promoted_display_name: string (nullable = true)\n |-- promoted_url: string (nullable = true)\n |-- retrieved_on: timestamp (nullable = true)\n |-- score: long (nullable = true)\n |-- secure_media: struct (nullable = true)\n |    |-- event_id: string (nullable = true)\n |    |-- oembed: struct (nullable = true)\n |    |    |-- author_name: string (nullable = true)\n |    |    |-- author_url: string (nullable = true)\n |    |    |-- cache_age: long (nullable = true)\n |    |    |-- description: string (nullable = true)\n |    |    |-- height: long (nullable = true)\n |    |    |-- html: string (nullable = true)\n |    |    |-- provider_name: string (nullable = true)\n |    |    |-- provider_url: string (nullable = true)\n |    |    |-- thumbnail_height: long (nullable = true)\n |    |    |-- thumbnail_url: string (nullable = true)\n |    |    |-- thumbnail_width: long (nullable = true)\n |    |    |-- title: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- version: string (nullable = true)\n |    |    |-- width: long (nullable = true)\n |    |-- type: string (nullable = true)\n |-- secure_media_embed: struct (nullable = true)\n |    |-- content: string (nullable = true)\n |    |-- height: long (nullable = true)\n |    |-- media_domain_url: string (nullable = true)\n |    |-- scrolling: boolean (nullable = true)\n |    |-- width: long (nullable = true)\n |-- selftext: string (nullable = true)\n |-- spoiler: boolean (nullable = true)\n |-- stickied: boolean (nullable = true)\n |-- subreddit: string (nullable = true)\n |-- subreddit_id: string (nullable = true)\n |-- suggested_sort: string (nullable = true)\n |-- third_party_trackers: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- third_party_tracking: string (nullable = true)\n |-- third_party_tracking_2: string (nullable = true)\n |-- thumbnail: string (nullable = true)\n |-- thumbnail_height: long (nullable = true)\n |-- thumbnail_width: long (nullable = true)\n |-- title: string (nullable = true)\n |-- url: string (nullable = true)\n |-- whitelist_status: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n\n\n\n\nA Disclaimer\nPlease note that because there are too many data points to work with on this notebook, we will be sampling the datasets to 100 observations each.\n\n\nCode\n# Sample the datasets to include only 100 rows each\ncomments_sampled_df = comments_df.limit(100)\nsubmissions_sampled_df = submissions_df.limit(100)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 83, 11, Finished, Available, Finished)\n\n\n\n\nCode\n# cache'ing the sampled DFs\ncomments_sampled_df.cache()\nsubmissions_sampled_df.cache()\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 83, 12, Finished, Available, Finished)\n\n\nDataFrame[adserver_click_url: string, adserver_imp_pixel: string, archived: boolean, author: string, author_cakeday: boolean, author_flair_css_class: string, author_flair_text: string, author_id: string, brand_safe: boolean, contest_mode: boolean, created_utc: timestamp, crosspost_parent: string, crosspost_parent_list: array&lt;struct&lt;approved_at_utc:string,approved_by:string,archived:boolean,author:string,author_flair_css_class:string,author_flair_text:string,banned_at_utc:string,banned_by:string,brand_safe:boolean,can_gild:boolean,can_mod_post:boolean,clicked:boolean,contest_mode:boolean,created:double,created_utc:double,distinguished:string,domain:string,downs:bigint,edited:boolean,gilded:bigint,hidden:boolean,hide_score:boolean,id:string,is_crosspostable:boolean,is_reddit_media_domain:boolean,is_self:boolean,is_video:boolean,likes:string,link_flair_css_class:string,link_flair_text:string,locked:boolean,media:string,mod_reports:array&lt;string&gt;,name:string,num_comments:bigint,num_crossposts:bigint,num_reports:string,over_18:boolean,parent_whitelist_status:string,permalink:string,pinned:boolean,quarantine:boolean,removal_reason:string,report_reasons:string,saved:boolean,score:bigint,secure_media:string,selftext:string,selftext_html:string,spoiler:boolean,stickied:boolean,subreddit:string,subreddit_id:string,subreddit_name_prefixed:string,subreddit_type:string,suggested_sort:string,thumbnail:string,thumbnail_height:string,thumbnail_width:string,title:string,ups:bigint,url:string,user_reports:array&lt;string&gt;,view_count:string,visited:boolean,whitelist_status:string&gt;&gt;, disable_comments: boolean, distinguished: string, domain: string, domain_override: string, edited: string, embed_type: string, embed_url: string, gilded: bigint, hidden: boolean, hide_score: boolean, href_url: string, id: string, imp_pixel: string, is_crosspostable: boolean, is_reddit_media_domain: boolean, is_self: boolean, is_video: boolean, link_flair_css_class: string, link_flair_text: string, locked: boolean, media: struct&lt;event_id:string,oembed:struct&lt;author_name:string,author_url:string,cache_age:bigint,description:string,height:bigint,html:string,provider_name:string,provider_url:string,thumbnail_height:bigint,thumbnail_url:string,thumbnail_width:bigint,title:string,type:string,url:string,version:string,width:bigint&gt;,reddit_video:struct&lt;dash_url:string,duration:bigint,fallback_url:string,height:bigint,hls_url:string,is_gif:boolean,scrubber_media_url:string,transcoding_status:string,width:bigint&gt;,type:string&gt;, media_embed: struct&lt;content:string,height:bigint,scrolling:boolean,width:bigint&gt;, mobile_ad_url: string, num_comments: bigint, num_crossposts: bigint, original_link: string, over_18: boolean, parent_whitelist_status: string, permalink: string, pinned: boolean, post_hint: string, preview: struct&lt;enabled:boolean,images:array&lt;struct&lt;id:string,resolutions:array&lt;struct&lt;height:bigint,url:string,width:bigint&gt;&gt;,source:struct&lt;height:bigint,url:string,width:bigint&gt;,variants:struct&lt;gif:struct&lt;resolutions:array&lt;struct&lt;height:bigint,url:string,width:bigint&gt;&gt;,source:struct&lt;height:bigint,url:string,width:bigint&gt;&gt;,mp4:struct&lt;resolutions:array&lt;struct&lt;height:bigint,url:string,width:bigint&gt;&gt;,source:struct&lt;height:bigint,url:string,width:bigint&gt;&gt;,nsfw:struct&lt;resolutions:array&lt;struct&lt;height:bigint,url:string,width:bigint&gt;&gt;,source:struct&lt;height:bigint,url:string,width:bigint&gt;&gt;,obfuscated:struct&lt;resolutions:array&lt;struct&lt;height:bigint,url:string,width:bigint&gt;&gt;,source:struct&lt;height:bigint,url:string,width:bigint&gt;&gt;&gt;&gt;&gt;&gt;, promoted: boolean, promoted_by: string, promoted_display_name: string, promoted_url: string, retrieved_on: timestamp, score: bigint, secure_media: struct&lt;event_id:string,oembed:struct&lt;author_name:string,author_url:string,cache_age:bigint,description:string,height:bigint,html:string,provider_name:string,provider_url:string,thumbnail_height:bigint,thumbnail_url:string,thumbnail_width:bigint,title:string,type:string,url:string,version:string,width:bigint&gt;,type:string&gt;, secure_media_embed: struct&lt;content:string,height:bigint,media_domain_url:string,scrolling:boolean,width:bigint&gt;, selftext: string, spoiler: boolean, stickied: boolean, subreddit: string, subreddit_id: string, suggested_sort: string, third_party_trackers: array&lt;string&gt;, third_party_tracking: string, third_party_tracking_2: string, thumbnail: string, thumbnail_height: bigint, thumbnail_width: bigint, title: string, url: string, whitelist_status: string, year: int, month: int]\n\n\nNow we took a peek at what the datasets look like:\n\n\nCode\n# displaying each\nprint(\"Sampled Comments DataFrame:\")\ncomments_sampled_df.show(1, truncate=False)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 83, 13, Finished, Available, Finished)\n\n\nSampled Comments DataFrame:\n+----------+--------------+----------------------+-----------------+------------------+--------+----------------+-------------------+-------------+------+------+-------+------------+---------+----------+--------------------------------------------------------+------------+-----+--------+---------------------+------------+----+-----+\n|author    |author_cakeday|author_flair_css_class|author_flair_text|body              |can_gild|controversiality|created_utc        |distinguished|edited|gilded|id     |is_submitter|link_id  |parent_id |permalink                                               |retrieved_on|score|stickied|subreddit            |subreddit_id|year|month|\n+----------+--------------+----------------------+-----------------+------------------+--------+----------------+-------------------+-------------+------+------+-------+------------+---------+----------+--------------------------------------------------------+------------+-----+--------+---------------------+------------+----+-----+\n|FieldOfFox|null          |null                  |null             |Music taste is bad|true    |0               |2021-06-23 08:32:32|null         |false |0     |h2qxlzs|false       |t3_o62mtb|t1_h2qvhl6|/r/terriblefacebookmemes/comments/o62mtb/genius/h2qxlzs/|null        |1    |false   |terriblefacebookmemes|t5_2tja6    |2021|6    |\n+----------+--------------+----------------------+-----------------+------------------+--------+----------------+-------------------+-------------+------+------+-------+------------+---------+----------+--------------------------------------------------------+------------+-----+--------+---------------------+------------+----+-----+\nonly showing top 1 row\n\n\n\n\n\nCode\nprint(\"Sampled Submissions DataFrame:\")\nsubmissions_sampled_df.show(1, truncate=False)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 83, 14, Finished, Available, Finished)\n\n\nSampled Submissions DataFrame:\n+------------------+------------------+--------+------------+--------------+----------------------+-----------------+---------+----------+------------+-------------------+----------------+---------------------+----------------+-------------+------+---------------+------+----------+---------+------+------+----------+--------+------+---------+----------------+----------------------+-------+--------+--------------------+---------------+------+-----+------------------------+-------------+------------+--------------+-------------+-------+-----------------------+-----------------------------------------------------------------------------------+------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+-----------+---------------------+------------+------------+-----+------------+------------------------------+--------+-------+--------+--------------+------------+--------------+--------------------+--------------------+----------------------+--------------------------------------------------------------------------------+----------------+---------------+--------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------+----+-----+\n|adserver_click_url|adserver_imp_pixel|archived|author      |author_cakeday|author_flair_css_class|author_flair_text|author_id|brand_safe|contest_mode|created_utc        |crosspost_parent|crosspost_parent_list|disable_comments|distinguished|domain|domain_override|edited|embed_type|embed_url|gilded|hidden|hide_score|href_url|id    |imp_pixel|is_crosspostable|is_reddit_media_domain|is_self|is_video|link_flair_css_class|link_flair_text|locked|media|media_embed             |mobile_ad_url|num_comments|num_crossposts|original_link|over_18|parent_whitelist_status|permalink                                                                          |pinned|post_hint|preview                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |promoted|promoted_by|promoted_display_name|promoted_url|retrieved_on|score|secure_media|secure_media_embed            |selftext|spoiler|stickied|subreddit     |subreddit_id|suggested_sort|third_party_trackers|third_party_tracking|third_party_tracking_2|thumbnail                                                                       |thumbnail_height|thumbnail_width|title                                                               |url                                                                                                 |whitelist_status|year|month|\n+------------------+------------------+--------+------------+--------------+----------------------+-----------------+---------+----------+------------+-------------------+----------------+---------------------+----------------+-------------+------+---------------+------+----------+---------+------+------+----------+--------+------+---------+----------------+----------------------+-------+--------+--------------------+---------------+------+-----+------------------------+-------------+------------+--------------+-------------+-------+-----------------------+-----------------------------------------------------------------------------------+------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+-----------+---------------------+------------+------------+-----+------------+------------------------------+--------+-------+--------+--------------+------------+--------------+--------------------+--------------------+----------------------+--------------------------------------------------------------------------------+----------------+---------------+--------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------+----+-----+\n|null              |null              |false   |toronto_news|null          |null                  |null             |null     |null      |false       |2021-02-23 02:54:20|null            |null                 |null            |null         |cbc.ca|null           |false |null      |null     |0     |false |false     |null    |lq7cf0|null     |true            |false                 |false  |false   |null                |null           |false |null |{null, null, null, null}|null         |0           |0             |null         |false  |null                   |/r/u_toronto_news/comments/lq7cf0/ontario_reports_cumulative_total_of_401_cases_of/|false |link     |{false, [{wyHLj_-2bO0KBYh7N1q_Lwj_IbdQctgV_NAGfGn1dP4, [{60, https://external-preview.redd.it/LAwIRuef0fAcidKCRvPSXKms-SV7G_hniA-QIH7oFng.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=753e33aa435dc4813e41a537e2fb31ef98d3f4fc, 108}, {121, https://external-preview.redd.it/LAwIRuef0fAcidKCRvPSXKms-SV7G_hniA-QIH7oFng.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0cd35dc9079d9a1d91ed5b2ece867b9fff173ba, 216}, {180, https://external-preview.redd.it/LAwIRuef0fAcidKCRvPSXKms-SV7G_hniA-QIH7oFng.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f78bbc461120a5119178d2015d88f5f89c963528, 320}], {349, https://external-preview.redd.it/LAwIRuef0fAcidKCRvPSXKms-SV7G_hniA-QIH7oFng.jpg?auto=webp&amp;s=f94d19a38d5484d3ec791e5074d579dd92968911, 620}, {null, null, null, null}}]}|null    |null       |null                 |null        |null        |1    |null        |{null, null, null, null, null}|        |false  |false   |u_toronto_news|t5_22gcg0   |qa            |null                |null                |null                  |https://a.thumbs.redditmedia.com/kiyj7mHda0TpNJ-vHtAH3byYZV5r-jm_3XDiq6x1-64.jpg|78              |140            |Ontario reports cumulative total of 401 cases of variants of concern|https://www.cbc.ca/news/canada/toronto/ontario-health-ministry-variants-of-concern-1.5922341?cmp=rss|null            |2021|2    |\n+------------------+------------------+--------+------------+--------------+----------------------+-----------------+---------+----------+------------+-------------------+----------------+---------------------+----------------+-------------+------+---------------+------+----------+---------+------+------+----------+--------+------+---------+----------------+----------------------+-------+--------+--------------------+---------------+------+-----+------------------------+-------------+------------+--------------+-------------+-------+-----------------------+-----------------------------------------------------------------------------------+------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+-----------+---------------------+------------+------------+-----+------------+------------------------------+--------+-------+--------+--------------+------------+--------------+--------------------+--------------------+----------------------+--------------------------------------------------------------------------------+----------------+---------------+--------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------+----+-----+\nonly showing top 1 row\n\n\n\n\n\n\nSub-Setting the Data\nThe following cells created new functions that would 1) detect English language only and 2) load and lemmatized keywords\n\n\nCode\n# First, we fix the pathing so that we can successfully locate our repo in AzureML, as well as the full paths for our code\n# we will be using Jude's path.\nimport os\n\nrepo_path = os.path.join(os.getcwd(), \"Users\", \"jjm385\", \"fall-2024-project-team-11\")\nos.chdir(repo_path)\n\n# lets verify the updated working directory\nprint(\"Updated Working Directory:\", os.getcwd())\n\n\nStatementMeta(11387b7c-91c2-4dd1-a4f8-a460b59b7bdd, 14, 14, Finished, Available, Finished)\n\n\nUpdated Working Directory: /synfs/notebook/14/aml_notebook_mount/Users/jjm385/fall-2024-project-team-11\n\n\nThe following function was created using the LangDetect package, which we used to only select posts written in English.\n\n\nCode\n# defining a UDF to detect English text. This will be used for both the Comments and Submissions Datasets\ndef is_english(text):\n    try:\n        return detect(text) == \"en\"\n    except:\n        return False\n\nis_english_udf = udf(is_english, BooleanType())\n\n\nStatementMeta(11387b7c-91c2-4dd1-a4f8-a460b59b7bdd, 14, 15, Finished, Available, Finished)\n\n\nThe following cell created a function that used NLTK and our list of keywords to check for our keywords (both original and lemmatized) within each post.\n\n\nCode\n# Download NLTK data if needed\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n# Initialize WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Function to load and lemmatize keywords\ndef load_and_lemmatize_keywords(file_path):\n    with open(file_path, 'r') as f:\n        keywords = [line.strip() for line in f.readlines()]\n    # Lemmatize each keyword\n    lemmatized_keywords = [lemmatizer.lemmatize(word) for word in keywords]\n    return lemmatized_keywords\n\n# Load and lemmatize the `_Lemmatized.txt` files\nkeywords_left_lemmatized = load_and_lemmatize_keywords(\"website-source/keywords/Keywords_Left_Lemmatized.txt\")\nkeywords_right_lemmatized = load_and_lemmatize_keywords(\"website-source/keywords/Keywords_Right_Lemmatized.txt\")\nkeywords_non_partisan_lemmatized = load_and_lemmatize_keywords(\"website-source/keywords/Keywords_Non_Partisan_Lemmatized.txt\")\n\n# Load the unchanged keywords (as in the previous code)\ndef load_keywords(file_path):\n    with open(file_path, 'r') as f:\n        return [line.strip() for line in f.readlines()]\n\nkeywords_left_unchanged = load_keywords(\"website-source/keywords/Keywords_Left_Unchanged.txt\")\nkeywords_right_unchanged = load_keywords(\"website-source/keywords/Keywords_Right_Unchanged.txt\")\nkeywords_non_partisan_unchanged = load_keywords(\"website-source/keywords/Keywords_Non_Partisan_Unchanged.txt\")\n\n# Combine all keywords into a single list\nall_keywords = set(\n    keywords_left_unchanged + keywords_right_unchanged + keywords_non_partisan_unchanged +\n    keywords_left_lemmatized + keywords_right_lemmatized + keywords_non_partisan_lemmatized\n)\n\n# Creating function to check for keywords in each post and comment\ndef contains_keyword(body):\n    if body is None:\n        return False\n    for keyword in all_keywords:\n        if keyword in body:\n            return True\n    return False\n\nis_keyword_present = udf(contains_keyword, BooleanType())\n\n\nStatementMeta(11387b7c-91c2-4dd1-a4f8-a460b59b7bdd, 14, 16, Finished, Available, Finished)\n\n\n[nltk_data] Downloading package wordnet to /home/trusted-service-\n[nltk_data]     user/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /home/trusted-service-\n[nltk_data]     user/nltk_data...\n\n\nNow that we had both of our functions ready to go, we then moved on to sub-setting our data for both Comments and Submissions datasets\n\n\nComments Subsetting\n\n\nCode\n# selecting only necessary columns \ncomments_filtered = comments_sampled_df.select(\n    \"id\",          # unique identifier\n    \"author\",      # author \n    \"body\",        # text content\n    \"created_utc\", # timestamp\n    \"year\",        \n    \"month\",       \n    \"subreddit\"    \n)\n\n# filter out the subreddits that are affiliated with the extreme political viewpoints\ncomments_df = comments_filtered.filter(\n    (~comments_filtered[\"subreddit\"].isin(\n        \"democrats\", \"Republican\", \"The_Donald\", \"EnoughTrumpSpam\", \"Fuckthealtright\",\n        \"Communism\", \"FullCommunism\", \"Anarchism\", \"AntiWork\", \"GenZedong\", \"Conspiracy\",\n        \"DarkEnlightenment\", \"NeoReaction\", \"Identitarian\", \"Fascist\", \"Socialism\", \n        \"Conservative\", \"Patriot\", \"ConsumeProduct\")) &\n    (comments_filtered[\"year\"] == 2022) &\n    (comments_filtered[\"month\"].between(1, 11)) &\n    (comments_filtered[\"body\"] != \"[removed]\") &  # Exclude removed comments\n    (comments_filtered[\"body\"] != \"[deleted]\") &  # Exclude deleted comments\n    (comments_filtered[\"body\"] != \"\") &           # Exclude empty comments\n    (is_english_udf(comments_filtered[\"body\"])) & # Ensure comments are in English\n    (is_keyword_present(comments_filtered[\"body\"]))  # Ensure keywords are present\n)\ncomments_df.show(5)\n\n\nStatementMeta(11387b7c-91c2-4dd1-a4f8-a460b59b7bdd, 14, 17, Finished, Available, Finished)\n\n\n+---+------+----+-----------+----+-----+---------+\n| id|author|body|created_utc|year|month|subreddit|\n+---+------+----+-----------+----+-----+---------+\n+---+------+----+-----------+----+-----+---------+\n\n\n\n\n\nSubmissions Subsetting\n\n\nCode\n#selecting only necessary columns\n\nsubmissions_filtered = submissions_sampled_df.select(\n    \"id\",          # unique identifier\n    \"author\",      # author \n    \"title\",       # post title\n    \"created_utc\", # timestamp\n    \"num_comments\",# could be useful for engagement\n    \"selftext\",    # text of the post\n    \"year\",        \n    \"month\",       \n    \"subreddit\"    \n)\n\n# filter out the subreddits that are affiliated with the extreme political viewpoints\nsubmissions_df = submissions_filtered.filter(\n    (~submissions_filtered[\"subreddit\"].isin(\"democrats\", \"Republican\", \"The_Donald\", \n    \"EnoughTrumpSpam\", \"Fuckthealtright\",\"Communism\", \"FullCommunism\", \"Anarchism\", \"AntiWork\", \"GenZedong\",\n    \"Conspiracy\", \"DarkEnlightenment\", \"NeoReaction\", \"Identitarian\", \"Fascist\", \"Socialism\", \"Conservative\", \"Patriot\", \"ConsumeProduct\"))  & \n    (submissions_filtered[\"year\"] == 2022) & \n    (submissions_filtered[\"month\"].between(1, 11)) &\n    (submissions_filtered[\"selftext\"].isNotNull()) & #grabs the non-link posts \n    (submissions_filtered[\"selftext\"] != \"[removed]\") & #strips posts that were removed and have no text\n    (submissions_filtered[\"selftext\"] != \"[deleted]\") & #similar but with self deletion\n    (submissions_filtered[\"selftext\"] != \"\") & #gets rid of posts with no content\n    is_english_udf(submissions_filtered[\"selftext\"]) & # filters for English text\n    is_keyword_present(submissions_filtered[\"selftext\"])  # filters for keywords\n)\nsubmissions_df.show(5)\n\n\nStatementMeta(11387b7c-91c2-4dd1-a4f8-a460b59b7bdd, 14, 19, Finished, Available, Finished)\n\n\n+---+------+-----+-----------+------------+--------+----+-----+---------+\n| id|author|title|created_utc|num_comments|selftext|year|month|subreddit|\n+---+------+-----+-----------+------------+--------+----+-----+---------+\n+---+------+-----+-----------+------------+--------+----+-----+---------+\n\n\n\nPlease note that the sub-setting of the data successfully works, however no data is being displayed above since we are only using 100 data points on this notebook, and in this limited sample there are none of our goal posts present.\nThe code was ran successfully as a job against the entire Comments and Submissions datasets. The resulting data sets were then used to transform our data in order to make our labeling process easier."
  },
  {
    "objectID": "Naive_Bayes.html",
    "href": "Naive_Bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a supervised machine learning algorithm based on Bayes’ Theorem. It is used for classification tasks where every pair of features being classified is independent of each other. Naive Bayes leverages the probability of an event occurring given the probability of another event that has already occurred using Baye’s theorem to asses the probability of a feature belonging to a class.\nWhile Naive Bayes might seem a simple process, its results are very insightful and have been proven to work very efficiently. One of the benefits is that it needs very little data to be trained and, as more data comes in, it can always be trained incrementally. However, it assumes independence among the predictors. Thus, while being a very good model for predicting spam emails, sentiment analysis, or document categorization, it is not as robust for other more complex tasks. The three most common Naive Bayes algorithms are: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\nOn this tab, Naive Bayes will be used to predict the method of transportation (“Measure” in the data set) based on the value and the State of entrance from the Clean_Border_Crossing_Entry_Data_20231103.csv file (found in the clean data folder). While it will not be a great addition to our project, for the sake of learning and assignment fullfillment, this is a good example of how Naive Bayes works. On the other hand, more insightfull, Naive Bayes will be used in our text data to predict the sentiment of reddit posts.\n\n\nThe formula for Bayes’ Theorem is: \\(P(\\text{A}|\\text{B}) = \\frac{P(\\text{B}|\\text{A}) \\times P(\\text{A})} {P(\\text{B})}\\)\n\n\n\nIt is used when the variables are continuous and have a normal distribution\nFormula: \\(P(\\text{class}|\\text{data}) = \\frac{P(\\text{data}|\\text{class}) \\times P(\\text{class})} {P(\\text{data})}\\)\n\n\n\nIt is used when the variables are discrete and have a multinomial distribution.\n\n\n\nIt is used when the variables are binary and have a Bernoulli distribution.\n\n\n\n\nMissing values are ignored while training the model and ignored when a probability is calculated.\nPerforms well even when the independence assumption is not satisfied.\nIt is easily to interpretate and has fast prediction time.\n\n\n\n\nThe data for producing the model will be divided into training and testing data. The training data will account for 80% of the data and the testing data will account for 20% of the data. Then the model will be trained using the train ing data and then tested its accruacy with the testing data since it would be new data that our model has never seen before. That way, we will be able to asses the accruacy of our model or, for our text data, assess the number of features we want to take into account in order to not overfit our model and have a more accurate prediction."
  },
  {
    "objectID": "Naive_Bayes.html#introduction",
    "href": "Naive_Bayes.html#introduction",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a supervised machine learning algorithm based on Bayes’ Theorem. It is used for classification tasks where every pair of features being classified is independent of each other. Naive Bayes leverages the probability of an event occurring given the probability of another event that has already occurred using Baye’s theorem to asses the probability of a feature belonging to a class.\nWhile Naive Bayes might seem a simple process, its results are very insightful and have been proven to work very efficiently. One of the benefits is that it needs very little data to be trained and, as more data comes in, it can always be trained incrementally. However, it assumes independence among the predictors. Thus, while being a very good model for predicting spam emails, sentiment analysis, or document categorization, it is not as robust for other more complex tasks. The three most common Naive Bayes algorithms are: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\nOn this tab, Naive Bayes will be used to predict the method of transportation (“Measure” in the data set) based on the value and the State of entrance from the Clean_Border_Crossing_Entry_Data_20231103.csv file (found in the clean data folder). While it will not be a great addition to our project, for the sake of learning and assignment fullfillment, this is a good example of how Naive Bayes works. On the other hand, more insightfull, Naive Bayes will be used in our text data to predict the sentiment of reddit posts.\n\n\nThe formula for Bayes’ Theorem is: \\(P(\\text{A}|\\text{B}) = \\frac{P(\\text{B}|\\text{A}) \\times P(\\text{A})} {P(\\text{B})}\\)\n\n\n\nIt is used when the variables are continuous and have a normal distribution\nFormula: \\(P(\\text{class}|\\text{data}) = \\frac{P(\\text{data}|\\text{class}) \\times P(\\text{class})} {P(\\text{data})}\\)\n\n\n\nIt is used when the variables are discrete and have a multinomial distribution.\n\n\n\nIt is used when the variables are binary and have a Bernoulli distribution.\n\n\n\n\nMissing values are ignored while training the model and ignored when a probability is calculated.\nPerforms well even when the independence assumption is not satisfied.\nIt is easily to interpretate and has fast prediction time.\n\n\n\n\nThe data for producing the model will be divided into training and testing data. The training data will account for 80% of the data and the testing data will account for 20% of the data. Then the model will be trained using the train ing data and then tested its accruacy with the testing data since it would be new data that our model has never seen before. That way, we will be able to asses the accruacy of our model or, for our text data, assess the number of features we want to take into account in order to not overfit our model and have a more accurate prediction."
  },
  {
    "objectID": "Naive_Bayes.html#naive-bayes-for-labeled-record-data",
    "href": "Naive_Bayes.html#naive-bayes-for-labeled-record-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes for labeled record data",
    "text": "Naive Bayes for labeled record data\nNaive Bayes has been used in our record data from our data set: Clean_Border_Crossing_Entry_Data_20231103.csv. The model tries to predict the method of transportation (“Measure” in the data set) based on the value and the State of entrance. The goal is to predict the method of transportation based on the value and the State of entrance. All categorical variables are converted into factor types in order to run the model (that is the variable: State).\nThe scores obtained from our model are printed bellow. Due to the great number of labels, printing the confusion matrix or the overall statistics for each label will be confusing, as well as plotting the confusion matrix. Therefore, we will only print the statistics.\n\n\nCode\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(e1071)\n\nchicago &lt;- read.csv('../../data/Clean_Data_project_Pub.Transport_5000/Clean_Border_Crossing_Entry_Data_20231103.csv')\n\nnumeric_cols &lt;- chicago[sapply(chicago, is.numeric)]\n\nscaled_numeric_cols &lt;- as.data.frame(scale(numeric_cols))\n\nchicago[sapply(df, is.numeric)] &lt;- scaled_numeric_cols\n\nvars &lt;- c(\"Value\", \"State\")\n\nsub &lt;- createDataPartition(y = chicago$Measure, \n                           p = 0.80, \n                           list = FALSE)\n\nchicago[] &lt;- lapply(chicago, as.factor)\n\ncen_bcs &lt;- preProcess(x = chicago[ ,vars],\n                      method = c(\"BoxCox\", \"center\", \"scale\"))\n\nDM_bcs &lt;- predict(object = cen_bcs,\n                      newdata = chicago)\n\ntrain &lt;- DM_bcs[sub, ] \ntest &lt;- DM_bcs[-sub, ]\n\nnb_model &lt;- naiveBayes(Measure ~ ., data = train)\n\npredictions &lt;- predict(nb_model, newdata = test)\n\npredictions &lt;- factor(predictions, levels = levels(test$Measure))\n\nconfusion_matrix &lt;- confusionMatrix(predictions, test$Measure)\n\noverall_stats &lt;- confusion_matrix$overall\nprint(overall_stats[c(\"Accuracy\", \"AccuracyLower\", \"AccuracyUpper\", \"Kappa\", \"McnemarPValue\")])\n\n\nWarning message in pre_process_options(method, column_types):\n\"The following pre-processing methods were eliminated: 'BoxCox', 'center', 'scale'\"\n\n\n     Accuracy AccuracyLower AccuracyUpper         Kappa McnemarPValue \n   0.16986819    0.16722706    0.17253412    0.09602056    0.00000000 \n\n\nThe following code could be utilized to view the confusion matrix and the overall statistics for each label. However, it has not been ran due to the reasons mentioned above.\n\n\nCode\nconfusion_matrix_df &lt;- as.data.frame(as.table(confusion_matrix$table))\n\nlibrary(ggplot2)\ngg &lt;- ggplot(data = confusion_matrix_df, aes(x = Reference, y = Prediction)) +\n  geom_tile(aes(fill = Freq)) +\n  geom_text(aes(label = Freq), vjust = 1) +\n  scale_fill_gradient(low = \"#7cf09b\", high = \"#3ee882\") +\n  labs(\n    x = \"Reference\",\n    y = \"Prediction\",\n    fill = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12)) +\n  ggtitle(\"Confusion Matrix\")\n\nprint(gg)\n\n\n\nConclusion\nThe model has a very low accruacy score, only 17%. This can be because this type of model is not the best for our type of data or our labels cannot be properly predicted based on the features we have since they are not deterministic on the label. Further study can be done for to see which of these two reasons is the cause of the low accuracy score. However, it seems that predicting the method of transportation based on the value and the State of entrance is not a good idea."
  },
  {
    "objectID": "Naive_Bayes.html#naive-bayes-for-labeled-text-data",
    "href": "Naive_Bayes.html#naive-bayes-for-labeled-text-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes for labeled text data",
    "text": "Naive Bayes for labeled text data\nNaive Bayes has been used in our text data from redit in order to predict the sentiment of the comments. The data has been labeled by the sentiment of the comments as shown in the data gathering tab. The data has been split into a training set and a test set. The training set has been used to train the model and the test set has been used to test the model.\nNote: The following code includes a bit more parsing of the data than the one in the data cleaning tab due to some inconsistencies found in the data. Furthermore, the sentiment scores have been changed to positive if the score is greater or equal to 0 and negative if the score is less than 0.\nCredits: Most of the code for this section has been changed from the code privided by Professor James Hickman from the DSAN program at Georgetown University.\n\n\nCode\nimport re\nimport spacy.lang.en.stop_words as stopwords\nimport spacy\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\ndf=pd.read_json(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/sentiment_results.json\")\nprint(df.shape)\nprint(df.columns)\n\ndf = df[df.text != '']\n#print(df.shape)\n#print(df.columns)\n\ndf['sentiment'] = df['sentiment_score'].apply(lambda score: 'positive' if score &gt;= 0 else 'negative')\n\n\n\n\nCode\ntexts = []\ny = []\n\nparser = spacy.load('en_core_web_sm')\nstop_words = stopwords.STOP_WORDS\n\n\nfor i in range(df.shape[0]):\n    keep = \"abcdefghijklmnopqrstuvwxyz \"\n    replace = \".,!;\"\n    tmp = \"\"\n    text_value = df[\"text\"].iloc[i]\n    sentiment_value = df[\"sentiment\"].iloc[i] \n\n\n    text_value = re.sub('[^a-zA-Z ]+', '', text_value.replace(\"&lt;br /&gt;\", \"\").lower())\n    text_value = parser(text_value)\n    tokens = [token.lower_ for token in text_value]\n\n    tokens = [token.lemma_ for token in text_value if token not in stop_words]\n\n\n    tmp = \" \".join(tokens)\n    texts.append(tmp)\n\n    if sentiment_value == \"positive\":\n        y.append(1)\n    elif sentiment_value == \"negative\":\n        y.append(0)\n\n    if i &lt; 3:\n        print(i)\n        print(tmp.replace(\"&lt;br /&gt;\", \"\"), '\\n')\n        print(tmp)\n        print(sentiment_value, y[i])\n\ny=np.array(y)\n\n\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(texts,MAX_FEATURES=10000)\n\n\n\n\nCode\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\n\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\n\nprint()\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\n\ndf2.columns = range(df2.columns.size)\nprint(df2.head())\nprint(df2.sum(axis=0))\nx=df2.to_numpy()\n\ndef train_MNB_model(X, Y, i_print=False):\n    if i_print:\n        print(X.shape, Y.shape)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    model = MultinomialNB()\n\n    start = time.process_time()\n    model.fit(X_train, y_train)\n    time_train = time.process_time() - start\n\n    start = time.process_time()\n    yp_train = model.predict(X_train)\n    yp_test = model.predict(X_test)\n    time_eval = time.process_time() - start\n\n    acc_train = accuracy_score(y_train, yp_train) * 100\n    acc_test = accuracy_score(y_test, yp_test) * 100\n\n    if i_print:\n        print(acc_train, acc_test, time_train, time_eval)\n\n    return acc_train, acc_test, time_train, time_eval\n\nprint(type(x), type(y))\nprint(x.shape, y.shape)\n(acc_train, acc_test, time_train, time_eval) = train_MNB_model(x, y, i_print=True)\n\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\ninitialize_arrays()\n\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\nsave_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/results_naive_bayes\")\n\n\n\n\nCode\ndef plot_results(path_root):\n\n    plt.plot(num_features,train_accuracies,'-ob')\n    plt.plot(num_features,test_accuracies,'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\nplot_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/results_naive_bayes\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsave_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/partial_grid_search\")\nplot_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nFrom these graphs, we can see that Naive Bayes does a good job in predicting whether the posts are positive or negative based on the text if the number of features is kept under 2000 or between 4000 and 6000. However, if the number of features is greater than 6000, the accruacy score drops significantly. This is because the model would be overfitting the data based on the training data."
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression Models",
    "section": "",
    "text": "Assignment by: Dr. Purna Gamage\nInstructions\nSubmission:\nOptional:"
  },
  {
    "objectID": "regression.html#import",
    "href": "regression.html#import",
    "title": "Regression Models",
    "section": "Import",
    "text": "Import\n\n\nCode\n#knitr::opts_chunk$set(include = FALSE) # for making prompts\nknitr::opts_chunk$set(echo = TRUE) # for making solutions\nknitr::opts_chunk$set(fig.width = 3)\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(modeldata)\nlibrary(leaps)\nlibrary(caret)\nlibrary(corrplot)\nlibrary(MASS)\nlibrary(ISLR)\nlibrary(glmnet)\nlibrary(gam)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "regression.html#hw-3.1-stock-returns",
    "href": "regression.html#hw-3.1-stock-returns",
    "title": "Regression Models",
    "section": "HW-3.1: Stock returns",
    "text": "HW-3.1: Stock returns\nUse polynomials and ridge regression to predict stock returns\nThis problem uses the built in EuStockMarkets dataset. The dataset contains time series of closing prices of major European stock indices from 1991 to 1998. We use only the FTSE column in this problem. The dataset is a time series object, but you will need to extract the FTSE column and make it into a data frame.\n\nHW-3.1a:\nFit polynomial models of degrees 4, 8, 12 to the FTSE data and plot all three fitted curves together with a scatterplot of the data. Comment on the plots. Which features in the data are resolved by the polynomial models? Which features are not resolved? Do the polynomial curves show any artifacts such as oscillations?\n\n\nCode\n# GET DATA\ndata(\"EuStockMarkets\")\nprint(head(EuStockMarkets))\n\n\n         DAX    SMI    CAC   FTSE\n[1,] 1628.75 1678.1 1772.8 2443.6\n[2,] 1613.63 1688.5 1750.5 2460.2\n[3,] 1606.51 1678.6 1718.0 2448.2\n[4,] 1621.04 1684.1 1708.1 2470.4\n[5,] 1618.16 1686.6 1723.1 2484.7\n[6,] 1610.61 1671.6 1714.3 2466.8\n\n\nCode\nftse &lt;- EuStockMarkets[,4]\n\n\n\n\nCode\nprint(attributes(ftse))\n\n\n$tsp\n[1] 1991.496 1998.646  260.000\n\n$class\n[1] \"ts\"\n\n\nCode\ntypeof(ftse)\n\n\n[1] \"double\"\n\n\n\n\nCode\n# INSERT CODE HERE \n\n# Load my theme\nmy_theme &lt;- readRDS('~/.Rthemes/my_theme.rds')\n\n# make ftse a df\nftse_df &lt;- data.frame(date = seq(as.Date(\"1991-01-01\"), by = \"day\", length.out = length(ftse)), FTSE = as.vector(ftse))\n\n# Models\nfit4 &lt;- lm(FTSE ~ poly(date, 4, raw=FALSE), data = ftse_df)\nfit8 &lt;- lm(FTSE ~ poly(date, 8, raw=FALSE), data = ftse_df)\nfit12 &lt;- lm(FTSE ~ poly(date, 12, raw=FALSE), data = ftse_df)\n\n# Predictions\npred4 &lt;- predict(fit4, newdata = ftse_df)\npred8 &lt;- predict(fit8, newdata = ftse_df)\npred12 &lt;- predict(fit12, newdata = ftse_df)\n\n\n\n\nCode\n# Plot\nggplot(ftse_df, aes(x = date)) +\n  geom_point(aes(y = FTSE)) +\n  geom_line(aes(y = pred4, color = \"Degree 4\"), size = 1.5) +\n  geom_line(aes(y = pred8, color = \"Degree 8\"), size = 1.5) +\n  geom_line(aes(y = pred12, color = \"Degree 12\"), size = 1.5) +\n  scale_color_manual(\n    name = \"Degree of Poly\",\n    values = c(\"Degree 4\" = \"pink\", \"Degree 8\" = \"royalblue1\", \"Degree 12\" = \"darkolivegreen1\")) +\n  labs(title = \"FTSE over time & Fitted Polynomial Curves\",\n       x = \"Date\", y = \"FTSE\") +\n  my_theme() +\n  theme(panel.grid = element_line(color = \"grey\"))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThe best one seems to be the one of degree 12, followed by the one of degree 8 and then 4. That is because the higher degree the more it can follow the patterns over time. However, while all of them seem to follow the trend, they all lack the ability to capture the ups and downs over time (probably due to seasons or months in the year). They show some osculations (the higher degree the more) but still cannot capture the complexity that times adds into the data. However, some oscillation are not in the data but they are shown, especially for degree 8 and 12, where they drop down at the beggining and at the end of the dataset. These are signs of some over fitting.\n\n\nHW-3.1.b:\nUse ridge regression to regularize the polynomial model of degree 12. Use \\(\\lambda_{1}SE\\). Plot the resulting polynomial model onto the the data and comment on it.\n\n\nCode\n# Make model matrix (degree 12)\nfeatures &lt;- model.matrix(~ poly(as.numeric(ftse_df$date), 12, raw = FALSE), data = ftse_df)[, -1] \n\nset.seed(444)\n\n# Cross Validation\ncv &lt;- cv.glmnet(features, ftse_df$FTSE, alpha = 0)\n# lambda 1se (optimal lambda with 1 standard error rule)\nlambda_1SE &lt;- cv$lambda.1se \n\n# model & predictions\nridge_model &lt;- glmnet(features, ftse_df$FTSE, alpha = 0)\nridge_predictions &lt;- predict(ridge_model, s = lambda_1SE, newx = features)\n\n# Plot\nplot(ftse_df$date, ftse_df$FTSE, type = \"l\", xlab = \"Date\", ylab = \"FTSE\", main = \"Ridge Regression on Degree 12 Polynomial\")\nlines(ftse_df$date, ridge_predictions, col = \"red\")\nlegend(\"bottomright\", legend = c(\"Actual\", \"Ridge Prediction\"), col = c(\"black\", \"red\"), lwd = 2, bty = \"n\")\n\n\n\n\n\n\n\n\n\nSome over fitting has been cut down through Ridge regression, but still have the endings of the model drop dramatically, which might be a problem for predicting future data. Ridge regression, while penalizing the magnitude of the coefficients, doesn’t seem to fully work for what was mentioned."
  },
  {
    "objectID": "regression.html#hw-3.2-advertising-budgets",
    "href": "regression.html#hw-3.2-advertising-budgets",
    "title": "Regression Models",
    "section": "HW-3.2: Advertising budgets",
    "text": "HW-3.2: Advertising budgets\nImprove advertising budgets using GAMs\nUse the Advertising dataset, which can either be found here. Split the data into a training and test set (70% / 30%).\n\n\nCode\n# GET DATA\nset.seed(441)\nads &lt;- read_csv('https://www.statlearning.com/s/Advertising.csv')\nads &lt;- ads[,-1]\n\n# Splitting data\ntrainIndex &lt;- sample(seq_len(nrow(ads)), size = floor(0.7 * nrow(ads)))\n\ntrain_data &lt;- ads[trainIndex, ]\ntest_data &lt;- ads[-trainIndex, ]\n\n# For our checking\npaste(\"rows of training data:\", nrow(train_data), \"rows of testing data:\", nrow(test_data))\n\n\n[1] \"rows of training data: 140 rows of testing data: 60\"\n\n\n\nHW-3.2a:\nFit generalized additive models to predict sales, using smoothing splines of degrees 2, 3, 4, 5, 6 for the three predictors. How do the rms prediction errors compare to the rms prediction error of a multiple regression model on the training set? On the test set?\n\n\nCode\n# INSERT CODE HERE \n\n# Linear Model\nlm_model &lt;- lm(sales ~ TV + radio + newspaper, data = train_data)\n\n# Predictions on test and training\nlm_predictions_train &lt;- predict(lm_model, newdata = train_data)\nlm_predictions_test &lt;- predict(lm_model, newdata = test_data)\n\n# RMS\nlm_rms_train &lt;- sqrt(mean((lm_predictions_train - train_data$sales)^2))\nlm_rms_test &lt;- sqrt(mean((lm_predictions_test - test_data$sales)^2))\n\nprint(paste(\"Linear Model RMS on Training Set:\", lm_rms_train))\n\n\n[1] \"Linear Model RMS on Training Set: 1.58165007636303\"\n\n\nCode\nprint(paste(\"Linear Model RMS on Test Set:\", lm_rms_test))\n\n\n[1] \"Linear Model RMS on Test Set: 1.93558979182608\"\n\n\n\n\nCode\n# INSERT CODE HERE \n# Set up the degrees, predictors, and storing for models and names\ndegrees &lt;- 2:6\npredictors &lt;- c(\"TV\", \"radio\", \"newspaper\")\ngam_mods &lt;- list()\nmod_names &lt;- vector()\n\n# Loop through all degrees\nfor (degree in degrees) {\n  # Formula\n  gam_form &lt;- as.formula(paste(\"sales ~\", paste(\"ns(\", predictors, \", df =\", degree, \")\", collapse = \" + \")))\n  \n  # Fit model and store\n  gam_mod_name &lt;- paste(\"GAM_Degree\", degree, sep = \"_\")\n  gam_mods[[gam_mod_name]] &lt;- gam(gam_form, data = train_data)\n  mod_names &lt;- c(mod_names, gam_mod_name)}\n\n# RMSE function\ncalculateRMSE &lt;- function(actual, predicted) {\n  sqrt(mean((actual - predicted)^2))}\n\n# to store RMSE\nrmse_train &lt;- setNames(numeric(length(mod_names)), mod_names)\nrmse_test &lt;- setNames(numeric(length(mod_names)), mod_names)\n\n# RMSE for each GAM model (training)\nfor (mod_name in names(gam_mods)) {\n  predictions1 &lt;- predict(gam_mods[[mod_name]], newdata = train_data, type = \"response\")\n  rmse_train[mod_name] &lt;- calculateRMSE(train_data$sales, predictions1)}\n\n# RMSE for each GAM model (testing)\nfor (mod_name in names(gam_mods)) {\n  predictions2 &lt;- predict(gam_mods[[mod_name]], newdata = test_data, type = \"response\")\n  rmse_test[mod_name] &lt;- calculateRMSE(test_data$sales, predictions2)}\n\n# RMSE values\nprint(\"RMSE for each model on the train set:\")\n\n\n[1] \"RMSE for each model on the train set:\"\n\n\nCode\nprint(rmse_train)\n\n\nGAM_Degree_2 GAM_Degree_3 GAM_Degree_4 GAM_Degree_5 GAM_Degree_6 \n    1.359051     1.319147     1.265214     1.249750     1.201283 \n\n\nCode\nprint(\"RMSE for each model on the test set:\")\n\n\n[1] \"RMSE for each model on the test set:\"\n\n\nCode\nprint(rmse_test)\n\n\nGAM_Degree_2 GAM_Degree_3 GAM_Degree_4 GAM_Degree_5 GAM_Degree_6 \n    1.966195     1.809398     1.836296     1.847180     1.930020 \n\n\nFor the training set, all the errors are lower than the one for multiple regression, and they lower even more the higher the degree, which is expected. However, in the test set, the model with degree 2 seems to have about or a tiny bit over the error of the multiple regression model while the other ones have lower. The one with degree 3 seems to do the best while then, the error slowly increases for the following degrees.\n\n\nHW-3.2.b:\nIs there evidence of overfitting?\nINSERT EXPLANATION HERE\nWhile there is not a clear sign of over fitting, as the GAM models have lower testing errors than the multiple regression one, degree 3 seems to do the best, indicating that higher degrees might do some over fitting in comparison to that one, but the difference is small.\n\n\nHW-3.2.c:\nYou now have six models (five GAM and one LM). Which model should be used? Explain your answer.\nINSERT EXPLANATION HERE\nThe model GAM with degree 3 has the lowest RMSE for the testing error, also having the lowest degree (after 2). Both indicate that it probably extrapolates better to the real world and it is the simplest model (after the one of degree 2), which is also good. Additionally, it doesn’t show signs of overfitting."
  },
  {
    "objectID": "regression.html#hw-3.3-boston-housing",
    "href": "regression.html#hw-3.3-boston-housing",
    "title": "Regression Models",
    "section": "HW-3.3: Boston housing",
    "text": "HW-3.3: Boston housing\nUse LASSO to predict housing prices in Boston\nConsider the Boston data from the MASS package. We want to use LASSO to predict the median home value medv using all the other predictors.\n\nHW-3.3.a:\nSet up the LASSO and plot the trajectories of all coefficients. What are the last five variables to remain in the model?\nNote: This was lab 3.1 question 2.\n\n\nCode\n# INSERT CODE HERE \n\ndata(\"Boston\")\n\n# data as matrix\nx &lt;- as.matrix(Boston[, -which(names(Boston) == \"medv\")])\ny &lt;- Boston$medv\n\nset.seed(444)\n\n# split data and set as matrix\ntrain_indices &lt;- sample(1:nrow(Boston), size = 0.8 * nrow(Boston))\n\ntrain_data &lt;- Boston[train_indices, ]\ntest_data &lt;- Boston[-train_indices, ]\n\nx_train &lt;- as.matrix(train_data[, -which(names(train_data) == \"medv\")])\ny_train &lt;- train_data$medv\n\nx_test &lt;- as.matrix(test_data[, -which(names(test_data) == \"medv\")])\ny_test &lt;- test_data$medv\n\n\n\n\nCode\n# INSERT CODE HERE \n# model\nlasso_model &lt;- glmnet(x_train, y_train, alpha = 1)\n\n# plot\nplot(lasso_model, xvar = \"lambda\", label = TRUE)\ntitle(\"Trajectories of coefficients\", line = 3)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# get the lambdas\nlambda_seq &lt;- exp(seq(log(max(lasso_model$lambda)), log(min(lasso_model$lambda)), length.out = 100))\n\n# to save the variables\nlast_five_vars &lt;- character()\n\n# for all lambdas\nfor (lambda in lambda_seq) {\n    # get the coreficients for each\n    lasso_coef &lt;- predict(lasso_model, type = \"coefficients\", s = lambda)\n    # make it as numeric\n    coef_vector &lt;- as.numeric(lasso_coef[-1, , drop = FALSE])\n    # cannot be zero\n    non_zero_coefs &lt;- which(coef_vector != 0)\n    if (length(non_zero_coefs) == 5) {\n      # get last five without the intercept\n        last_five_vars &lt;- rownames(lasso_coef)[-1][non_zero_coefs]\n        break}}\n\ncat(\"Last five variables to remain:\", paste(last_five_vars, collapse = \", \"))\n\n\nLast five variables to remain: chas, rm, ptratio, black, lstat\n\n\n\n\nHW-3.3.b:\nFind the 1SE value of \\(\\lambda\\), using 10-fold cross-validation. What is the cross validation estimate for the residual standard error?\n\n\nCode\n# INSERT CODE HERE \n\nset.seed(444)\n# model\ncv_lasso &lt;- cv.glmnet(x_train, y_train, alpha = 1, nfolds = 10)\n\n# best lambda\nlambda_min &lt;- cv_lasso$lambda.min\n\n# 1SE\nlambda_1se &lt;- cv_lasso$lambda.1se\n\n# get the index\nindex_1se &lt;- which.min(abs(cv_lasso$lambda - lambda_1se))\n\n# select for that lambda\ncvm_1se &lt;- sqrt(cv_lasso$cvm[index_1se])\n\ncat(\"Lambda 1-SE:\", lambda_1se, \"\\n\")\n\n\nLambda 1-SE: 0.3625409 \n\n\nCode\ncat(\"Cross-validation estimate for Residual Standard Error:\", cvm_1se)\n\n\nCross-validation estimate for Residual Standard Error: 4.924468\n\n\n\n\nHW-3.3.c:\nRescale all predictors so that their mean is zero and their standard deviation is 1. Then set up the LASSO and plot the trajectories of all coefficients. What are the last five variables to remain in the model? Compare your answer to part a.\n\n\nCode\n# INSERT CODE HERE \n\n# scale\nx_train_scaled &lt;- scale(x_train) \n\nset.seed(444) \n# model\nlasso_model &lt;- glmnet(x_train_scaled, y_train, alpha = 1)\n\n# plot\nplot(lasso_model, xvar = \"lambda\", label = TRUE)\ntitle(\"Trajectories of coefficients\", line = 3)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# INSERT CODE HERE \n\nlambda_seq &lt;- exp(seq(log(max(lasso_model$lambda)), log(min(lasso_model$lambda)), length.out = 100))\n\nlast_five_vars &lt;- character()\n\nfor (lambda in lambda_seq) {\n \n    lasso_coef &lt;- predict(lasso_model, type = \"coefficients\", s = lambda)\n    \n    coef_vector &lt;- as.numeric(lasso_coef[-1, , drop = FALSE])\n    \n    non_zero_coefs &lt;- which(coef_vector != 0)\n    \n    if (length(non_zero_coefs) == 5) {\n        last_five_vars &lt;- rownames(lasso_coef)[-1][non_zero_coefs]\n        break}}\n\ncat(\"Last five variables to remain:\", paste(last_five_vars, collapse = \", \"))\n\n\nLast five variables to remain: chas, rm, ptratio, black, lstat\n\n\nThe last 5 variables are still the same. However, rescaling them has changed their trajectory as seen in the plot and also makes them easier to visualize\n\n\nHW-3.3.d:\nFind the 1SE value of \\(\\lambda\\) using 10-fold cross-validation. What is the cross validation estimate for the residual standard error now? Does rescaling lead to a better performing model?\n\n\nCode\n# INSERT CODE HERE \nset.seed(444)\nlasso.out &lt;- cv.glmnet(x_train_scaled, y_train, alpha = 1, nfolds = 10)\n\n# extract the lambda value that is one standard error away from the minimum lambda\nlambda_1se &lt;- lasso.out$lambda.1se\n\n# calculate the cross-validated RSE\nrse &lt;- sqrt(lasso.out$cvm[lasso.out$lambda == lambda_1se])\n\n# print lambda value and RSE\ncat(\"1SE Lambda:\", lambda_1se, \"\\nRSE:\", rse)\n\n\n1SE Lambda: 0.3625409 \nRSE: 4.924468\n\n\nThe error is the same and the lambda used too."
  },
  {
    "objectID": "regression.html#hw-3.4-bike-share-usage",
    "href": "regression.html#hw-3.4-bike-share-usage",
    "title": "Regression Models",
    "section": "HW-3.4: Bike share usage",
    "text": "HW-3.4: Bike share usage\nPredict bike share usage in Seoul using ridge and LASSO regressions\nAccess the dataset here. Filter the data to only include rows with “functioning days” == ‘Yes’. Next drop the columns Date, Hour, Seasons, and Holiday, and Functioning Day. Then drop any rows that have any missing values in any columns. Hint: You will need to rename some of the variable names because they include non-ASCII characters. This will help you later on.\n\n\nCode\n# GET DATA\n\n# Obtaining data and column names\nbike &lt;- read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv', \n                 locale = locale(encoding = \"UTF-8\"), show_col_types = FALSE)\n\ncolnames(bike)\n\n\n [1] \"Date\"                         \"Rented Bike Count\"           \n [3] \"Hour\"                         \"Temperature(\\xb0C)\"          \n [5] \"Humidity(%)\"                  \"Wind speed (m/s)\"            \n [7] \"Visibility (10m)\"             \"Dew point temperature(\\xb0C)\"\n [9] \"Solar Radiation (MJ/m2)\"      \"Rainfall(mm)\"                \n[11] \"Snowfall (cm)\"                \"Seasons\"                     \n[13] \"Holiday\"                      \"Functioning Day\"             \n\n\n\n\nCode\n# Filtering only Functioning dat = yes\nbike &lt;- bike %&gt;% filter(`Functioning Day` == \"Yes\")\n\n# Manually changing the names (other forms of doing it were not supported due to format)\nnames(bike) &lt;- c(\"Date\", \"Rented_Bike_Count\", \"Hour\", \"Temperature_C\", \n                          \"Humidity\", \"Wind_speed_ms\", \"Visibility_10m\", \n                          \"Dew_point_temperature_C\", \"Solar_Radiation_MJ_m2\", \n                          \"Rainfall_mm\", \"Snowfall_cm\", \"Seasons\", \n                          \"Holiday\", \"Functioning_Day\")\n\n# Drop unwanted columns\ncols_to_drop &lt;- c(\"Date\", \"Hour\", \"Seasons\", \"Holiday\", \"Functioning_Day\")\n\nbike &lt;- bike %&gt;% dplyr::select(-one_of(cols_to_drop))\n\n# Drp NAs\nbike &lt;- na.omit(bike)\n\nhead(bike)\n\n\n# A tibble: 6 × 9\n  Rented_Bike_Count Temperature_C Humidity Wind_speed_ms Visibility_10m\n              &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1               254          -5.2       37           2.2           2000\n2               204          -5.5       38           0.8           2000\n3               173          -6         39           1             2000\n4               107          -6.2       40           0.9           2000\n5                78          -6         36           2.3           2000\n6               100          -6.4       37           1.5           2000\n# ℹ 4 more variables: Dew_point_temperature_C &lt;dbl&gt;,\n#   Solar_Radiation_MJ_m2 &lt;dbl&gt;, Rainfall_mm &lt;dbl&gt;, Snowfall_cm &lt;dbl&gt;\n\n\n\nHW-3.4.a:\nRun a linear regression to predict rented bike count using the remaining 8 variables in the dataset. Report the MSE and the most influential variables.\n\n\nCode\n# linear regression model\nlm_model &lt;- lm(Rented_Bike_Count ~ ., data = bike)\n\n# extract MSE\nmse &lt;- mean(lm_model$residuals^2)\n\n# get variable coefficients\ncoefs &lt;- coef(lm_model)[-1]  # Exclude intercept\n\n# identify influential variables (5)\nmost_influential &lt;- names(sort(abs(coefs), decreasing = TRUE)[1:5])\n\n# print MSE and vars \nprint(paste(\"MSE:\", mse))\n\n\n[1] \"MSE: 234111.046434225\"\n\n\nCode\nprint(cat(\"The 5 most influential variables:\", most_influential))\n\n\nThe 5 most influential variables: Solar_Radiation_MJ_m2 Wind_speed_ms Rainfall_mm Snowfall_cm Temperature_CNULL\n\n\n\n\nHW-3.4.b:\nFit a ridge regression model with the optimal \\(\\lambda\\) chosen by cross validation. Report the CV MSE.\n\n\nCode\n# Data as matrix\nx &lt;- as.matrix(subset(bike, select = -c(Rented_Bike_Count)))\ny &lt;- bike$Rented_Bike_Count\n\n# fit ridge with CV\nridge_mod &lt;- cv.glmnet(x, y, alpha = 0)\n\n# Optimal lambda\nopt_lambda &lt;- ridge_mod$lambda.min\n\n# fit ridge model with optimal lambda\nridge_fit &lt;- glmnet(x, y, alpha = 0, lambda = opt_lambda)\n\nridge_pred &lt;- predict(ridge_fit, s = opt_lambda, newx = x)\n\n# MSE\ncv_mse &lt;- min(ridge_mod$cvm)\n\n# Print\ncat(\"CV MSE:\", cv_mse)\n\n\nCV MSE: 236017\n\n\n\n\nHW-3.4.c:\nPerform the same fit using a LASSO regression this time. Choose the optimal \\(\\lambda\\) using cross validation. Report on the remaining variables in the model and the CV MSE. How does this performance compare to ridge and a plain linear model?\n\n\nCode\n# INSERT CODE HERE \n\n# Fit Lasso\ncv_lasso &lt;- cv.glmnet(x, y, alpha = 1, type.measure = \"mse\")\n\n# Optimal lambda\noptimal_lambda_lasso &lt;- cv_lasso$lambda.min\ncat(\"Optimal lambda for LASSO:\", optimal_lambda_lasso, \"\\n\")\n\n\nOptimal lambda for LASSO: 0.9379857 \n\n\nCode\n# MSE\ncv_mse_lasso &lt;- min(cv_lasso$cvm)\npaste(\"CV MSE for LASSO with optimal lambda:\", cv_mse_lasso)\n\n\n[1] \"CV MSE for LASSO with optimal lambda: 234598.796445585\"\n\n\nCode\n# Model with optimal lambda\nlasso_model &lt;- glmnet(x, y, alpha = 1, lambda = optimal_lambda_lasso)\n\n# Coefficients\ncoef_lasso &lt;- coef(lasso_model)\n# Remaining vars\nremaining_vars &lt;- rownames(coef_lasso)[coef_lasso[,1] != 0]\n\n# Print remaining Vars\ncat(\"Remaining variables in the LASSO model:\\n\", remaining_vars)\n\n\nRemaining variables in the LASSO model:\n (Intercept) Temperature_C Humidity Wind_speed_ms Visibility_10m Solar_Radiation_MJ_m2 Rainfall_mm Snowfall_cm\n\n\nCode\nprint(\"Coefficients:\")\n\n\n[1] \"Coefficients:\"\n\n\nCode\nprint(lasso_coefs&lt;-coef(cv_lasso, s = optimal_lambda_lasso, exact = TRUE))\n\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                                   s1\n(Intercept)              8.523407e+02\nTemperature_C            3.634304e+01\nHumidity                -1.052416e+01\nWind_speed_ms            5.231777e+01\nVisibility_10m           3.064328e-03\nDew_point_temperature_C  .           \nSolar_Radiation_MJ_m2   -1.141101e+02\nRainfall_mm             -5.273742e+01\nSnowfall_cm              3.350817e+01\n\n\nThis model has a lower MSE than Ridge but slightly higher than the plain linear model\n\n\nHW-3.4.e:\nInterpretation and communication. Write a short paragraph about your analysis and recommendations, explaining the most important factors for high bike share usage, why you came to that conclusion, and what actions can be taken by a bike rental company based on this information.\nINSERT EXPLANATION HERE\nBy analyzing the data and all the models ran, the MSE is the lowest for the plain linear model, followed by the LASSO model and then the Ridge. This suggests that we would want to use the plain linear one probably, even though they all have very similar values. The most significant values are Temperature, Humidity, Wind Speed, Visibility, Solar Radiation, Rainfall, and Snowfall for the LASSO one, giving us some insight of the most important variables to predict rented bike count. We can see that Humidity, Solar radiation, and rainfall negatively impact the bike rentals while the rest mentioned affect it positively. This is useful information for the bike rental company to take into account when making decisions such as location where opening new rentals, amount of people to hire in different seasons, prepare differently depending on the weather forecast, or strategically place marketing campains."
  },
  {
    "objectID": "regression.html#hw-3.5-splines",
    "href": "regression.html#hw-3.5-splines",
    "title": "Regression Models",
    "section": "HW-3.5: Splines",
    "text": "HW-3.5: Splines\nCompare the characteristics of two different smoothing splines\nConsider two curves called \\(\\hat{g}_1\\) and \\(\\hat{g}_2\\) are as follows:\n\\[\n\\hat{g}_1 = argmin_g \\left(\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int [g^{(3)}(x)]^2 \\right)\n\\]\n\\[\n\\hat{g}_2 = argmin_g \\left(\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int [g^{(4)}(x)]^2 \\right)\n\\] where \\(g^{(m)}\\) represents the \\(m^{th}\\) derivative of \\(g\\).\n\nHW-3.5.a:\nAs \\(\\lambda \\to \\infty\\), which function (\\(\\hat{g_1}\\) or \\(\\hat{g_2}\\)) will have the smaller training RSS?\nINSERT EXPLANATION HERE\nAs lambda goes to infinity, the penalty becomes dominant, pushing the derivatives (\\(g^{(3)}\\) for \\(\\hat{g}_1\\) and \\(g^{(4)}\\) for \\(\\hat{g}_2\\)) towards zero. However, for \\(\\hat{g}_1\\) it forces the third derivative to 0 while for \\(\\hat{g}_2\\) forces the forth derivative to 0. This makes a difference as it tends \\(\\hat{g}_1\\) to a polynomial of degree 2 (quadratic) and \\(\\hat{g}_2\\) to a polynomial of degree 3 (cubic). Thus, \\(\\hat{g}_2\\) will be able to capture more noise probably and thus have a lower RSS.However, this always depends on the shape of the data and since they will be extremely smooth, they will probably have close values.\n\n\nHW-3.5.b:\nAs \\(\\lambda \\to \\infty\\), which function (\\(\\hat{g_1}\\) or \\(\\hat{g_2}\\)) will have the smaller test RSS?\nINSERT EXPLANATION HERE\nFrom before, we know what \\(\\hat{g}_1\\) and \\(\\hat{g}_2\\) tend to. Now, it is worth noting that as lambda goes to infinity, it makes them extremely smooth, probably causing under fitting. The simpler model may or may not extrapolate better to the real world and this, indeed, would depend on the nature of the data. However, the expected difference should be extremely small due to their extreme smoothness.\n\n\nHW-3.5.c:\nFor \\(\\lambda = 0\\), which function (\\(\\hat{g_1}\\) or \\(\\hat{g_2}\\)) will have the smaller training and test RSS?\nINSERT EXPLANATION HERE\nWhen lambda equals 0, the whole integral being multiplied by lambda sets to 0 as it is multiplied by 0 (lambda). This makes the penalty term 0 permitting the splines to fit the data as closely as posible. The term on the left of the equation of both (\\(\\hat{g_1}\\) and \\(\\hat{g_2}\\)) is exactly the same, meaning \\(\\hat{g_1}\\) and \\(\\hat{g_2}\\) will output the same result and thus, have the same RSS."
  },
  {
    "objectID": "regression.html#hw-3.6",
    "href": "regression.html#hw-3.6",
    "title": "Regression Models",
    "section": "HW-3.6:",
    "text": "HW-3.6:\nExplain the behavior of the curve for a variety of \\(\\lambda\\) and \\(m\\) values.\nSuppose a curve \\(\\hat{g}\\) is fit smoothly to a set of \\(n\\) points as follows:\n\\[\n\\hat{g} = argmin_g \\left(\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int [g^{(m)}(x)]^2 \\right)\n\\]\nwhere \\(g^{(m)}\\) is the \\(m\\)th derivative of \\(\\hat{g}\\) and \\(g^{(0)}=g\\). Provide plots of \\(\\hat{g}\\) in each of the following scenarios along with the original points provided.\nUse the following starter code to make your set of points and plot your various model predictions.\n\n\nCode\nset.seed(325626)\n\nX &lt;- runif(100)\neps &lt;- rnorm(100)\nY &lt;- sin(12*(X + 0.2)) / (X + 0.2) + eps\ngenerating_fn &lt;- function(X) {sin(12*(X + 0.2)) / (X + 0.2)}\ndf &lt;- data.frame(X, Y)\n\nggplot(df, aes(x = X, y = Y)) + \n  geom_point(alpha = 0.5) + \n  stat_function(fun = generating_fn, aes(col = \"Generating Function\")) + \n  scale_color_manual(values = \"deepskyblue3\") + \n  theme(legend.position = \"right\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\nHW-3.6.a:\n\\(\\lambda = \\infty, m = 0\\).\n\n\nCode\n# INSERT SOLUTION HERE \n# Small approximating infinity (biggest I could run)\nlambda &lt;- 1e8\n\n# m\nm &lt;- 0\n\n# fit smoothing spline to data\nfit &lt;- smooth.spline(df$X, df$Y, lambda = lambda)\n\n# Predict using fitted smoothing spline with derivative order m\npreds &lt;- predict(fit, df$X, deriv = m)\n\n# predictions as df\npreds_df &lt;- as.data.frame(preds)\n\n# plot\nggplot(df, aes(x = X, y = Y)) + \n  geom_point(alpha = 0.5) +\n  stat_function(fun = generating_fn, aes(col = \"Generating Function\")) + \n  geom_line(data = preds_df, aes(x = x, y = y, col = \"Fitted Spline\")) +\n  theme(legend.position = \"right\", legend.title = element_blank()) \n\n\n\n\n\n\n\n\n\n\n\nHW-3.6.b:\n\\(\\lambda = \\infty, m = 1\\).\n\n\nCode\n# INSERT SOLUTION HERE \n\n# Small approximating infinity (biggest I could run)\nlambda &lt;- 1e8\n\n# m\nm &lt;- 1\n\n# fit smoothing spline to data\nfit &lt;- smooth.spline(df$X, df$Y, df = 100, lambda = lambda)\n\n# Predict using fitted smoothing spline with derivative order m\npreds &lt;- predict(fit, df$X, deriv = m)\n\n# predictions as df\npreds_df &lt;- as.data.frame(preds)\n\n# Plot\nggplot(df, aes(x = X, y = Y)) + \n  geom_point(alpha = 0.5) +  # plot original points\n  stat_function(fun = generating_fn, aes(col = \"Generating Function\")) + \n  geom_line(data = preds_df, aes(x = x, y = y, col = \"Fitted Spline\")) +\n  theme(legend.position = \"right\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nHW-3.6.c:\n\\(\\lambda = \\infty, m = 2\\).\n\n\nCode\n# INSERT SOLUTION HERE \n\n# Small approximating infinity (biggest I could run)\nlambda &lt;- 1e8\n\n# m\nm &lt;- 2\n\n# fit smoothing spline to data\nfit &lt;- smooth.spline(df$X, df$Y, df = 100, lambda = lambda)\n\n# Predict using fitted smoothing spline with derivative order m\npreds &lt;- predict(fit, df$X, deriv = m)\n\n# predictions as df\npreds_df &lt;- as.data.frame(preds)\n\n# Plot\nggplot(df, aes(x = X, y = Y)) + \n  geom_point(alpha = 0.5) +\n  stat_function(fun = generating_fn, aes(col = \"Generating Function\")) + \n  geom_line(data = preds_df, aes(x = x, y = y, col = \"Fitted Spline\")) +\n  theme(legend.position = \"right\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nHW-3.6.d:\n\\(\\lambda = \\infty, m = 3\\).\n\n\nCode\n# INSERT SOLUTION HERE \n\n# Small approximating infinity (biggest I could run)\nlambda &lt;- 1e8\n\n# m\nm &lt;- 3\n\n# fit smoothing spline to data\nfit &lt;- smooth.spline(df$X, df$Y, df = 100, lambda = lambda)\n\n# Predict using fitted smoothing spline with derivative order m\npreds &lt;- predict(fit, df$X, deriv = m)\n\n# predictions as df\npreds_df &lt;- as.data.frame(preds)\n\n# Plot\nggplot(df, aes(x = X, y = Y)) + \n  geom_point(alpha = 0.5) +\n  stat_function(fun = generating_fn, aes(col = \"Generating Function\")) +\n  geom_line(data = preds_df, aes(x = x, y = y, col = \"Fitted Spline\")) + \n  theme(legend.position = \"right\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nHW-3.6.e:\n\\(\\lambda = 0, m = 3\\).\n\n\nCode\n# INSERT SOLUTION HERE \n\n# lamda = 0\nlambda &lt;- 0\n\n# m\nm &lt;- 3\n\n# fit smoothing spline to data\nfit &lt;- smooth.spline(df$X, df$Y, lambda = lambda)\n\n# Predict using fitted smoothing spline\npreds &lt;- predict(fit, df$X)\n\n# predictions as df\npreds_df &lt;- as.data.frame(preds)\n\n# Plot\nggplot(df, aes(x = X, y = Y)) + \n  geom_point(alpha = 0.5) +  \n  stat_function(fun = generating_fn, aes(col = \"Generating Function\")) +  \n  geom_line(data = preds_df, aes(x = x, y = y, col = \"Fitted Spline\")) + \n  theme(legend.position = \"right\", legend.title = element_blank()) \n\n\n\n\n\n\n\n\n\n\n\nHW-3.6.f:\nFit a smoothing spline on the dataset and report the optimal lambda\n\n\nCode\n# INSERT SOLUTION HERE \n\n# Optimal fit spline\noptimal_fit &lt;- smooth.spline(df$X, df$Y)\n\n# optimal lambda defined by our fit\nprint(paste(\"Optimal lambda:\", optimal_fit$lambda))\n\n\n[1] \"Optimal lambda: 0.000111660415400747\"\n\n\nCode\n# Predict using fitted smoothing spline\npreds &lt;- predict(optimal_fit, df$X)\n\n# predictions as df\npreds_df &lt;- as.data.frame(preds)\n\n# Plot\nggplot(df, aes(x = X, y = Y)) + \n  geom_point(alpha = 0.5) + \n  stat_function(fun = generating_fn, aes(col = \"Generating Function\")) + \n  geom_line(data = preds_df, aes(x = x, y = y, col = \"Fitted Spline\")) + \n  theme(legend.position = \"right\", legend.title = element_blank())"
  },
  {
    "objectID": "Data_Deep_Dive.html",
    "href": "Data_Deep_Dive.html",
    "title": "Data Deep Dive",
    "section": "",
    "text": "Description\nIn this tab I will be submitting certain work produced during my time at Georgetown University that I believe is relevant to showcase. This will include statistical analyses, supervised and unsupervised machine learning models, and data visualization."
  },
  {
    "objectID": "Conclusion.html",
    "href": "Conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "This section of the conclusion will be a summary of the insights from the different tabs. Before we dive into making overall conclusions and suggestions for the improvement of public transportation, it is important to have a summary of the insights obtained from our different analysis and models.\n\n\nFrom a general perspective, it seems that the performance of public transportation, its usage, and its contamination levels have not improved much in the last 10 years. Furthermore, it seems like it could be improved in many areas to be more efficient and sustainable.\n\n\n\nOur Naive Bayes model shows that predicting the type of transportation based on the State crossed and its value is not really accurate. While other type of analysis could be done to see if they are related or not with more precision, our model is not able to predict the type of transportation. This seems to be a clear fact that the type of transportation is not related to the State crossed and its value. However, I suggest studying this topic further with other analysis.\nOn the other hand, Naive Bayes does a great job at predicting the sentiment of reddit posts/comments. While this model could be used to identify the negative reddit posts (or posts in other platforms if checked its accruacy), it could be more effective if utilized with the purpose of identifying the negative posts in order to exteract them and identify the main issues users encounter. That way, the issues could be addressed and solved.\n\n\n\nOur clustering tab indicates that clusters seem to be present in the census blocks of the US. This is a great finding since we can infer that there are different “types” of census blocks and we could address them in a group manner. This could allow the US to invest in their development as the reduced number of clusters allow the US to invest in the development of the clusters in a more efficient and economic manner. If dealt with them individually, the US would have to invest more money and time in the development of the census blocks, which probably is not possible.\n\n\n\nOur data set for the census blocks had too many redundant variables, as PCA identified that 25 would explain more than 90% of the variance. This is a great insight as, if inferences made for different clusters are to be made, it would be much more computational efficient to use the PCA variables. FDurthermore, if the inferences are to be extrapolated to other or new census blocks, it would be much more efficient to use the PCA variables to determine their cluster and then make the inferences.\n\n\n\nDecision trees seem to be a great model to predict the average miles per gallon based on city, agency, organization type, Primary UZA population, mode of transportation, and type of service, electric atteries used. Even more effectively, just using Agency, type of service, and mode of transportation seems to be the right choice. Thus, this model could be used to find the optimal combination of these three components in order to choos the most efficient one in every case. That way, we could reduce our carbon footprint while reducing our fossil fuel’s cost.\n\n\n\nPeople seem to care about time and stops the most when writing on reddit about public transportation. This gives us an insight that time is one of the most important things for people and having stops available for when they use public transportation seems to be logical. Thus, we could invest in the development of more stops and more efficient routes to reduce the time people spend on public transportation.\nAdditionally, another key insight worth highlighting is that transportation has been performing worse over time. However, the energy consumed seems to also have decreased (slowly) overtime. Furthermore, month seems to have an apparent effect on performance, which could be overcome by investing in more resources during the months where performance is lower. Finally, understanding what countries play the biggest role in the manufacturing of public transportation vehicles could be a great insight to understand the impact of public transportation in the world. More advances in R&D could be made to improve the manufacturing of these vehicles and reduce their carbon footprint in these countries."
  },
  {
    "objectID": "Conclusion.html#insights-and-general-comments-from-different-tabs",
    "href": "Conclusion.html#insights-and-general-comments-from-different-tabs",
    "title": "Conclusion",
    "section": "",
    "text": "This section of the conclusion will be a summary of the insights from the different tabs. Before we dive into making overall conclusions and suggestions for the improvement of public transportation, it is important to have a summary of the insights obtained from our different analysis and models.\n\n\nFrom a general perspective, it seems that the performance of public transportation, its usage, and its contamination levels have not improved much in the last 10 years. Furthermore, it seems like it could be improved in many areas to be more efficient and sustainable.\n\n\n\nOur Naive Bayes model shows that predicting the type of transportation based on the State crossed and its value is not really accurate. While other type of analysis could be done to see if they are related or not with more precision, our model is not able to predict the type of transportation. This seems to be a clear fact that the type of transportation is not related to the State crossed and its value. However, I suggest studying this topic further with other analysis.\nOn the other hand, Naive Bayes does a great job at predicting the sentiment of reddit posts/comments. While this model could be used to identify the negative reddit posts (or posts in other platforms if checked its accruacy), it could be more effective if utilized with the purpose of identifying the negative posts in order to exteract them and identify the main issues users encounter. That way, the issues could be addressed and solved.\n\n\n\nOur clustering tab indicates that clusters seem to be present in the census blocks of the US. This is a great finding since we can infer that there are different “types” of census blocks and we could address them in a group manner. This could allow the US to invest in their development as the reduced number of clusters allow the US to invest in the development of the clusters in a more efficient and economic manner. If dealt with them individually, the US would have to invest more money and time in the development of the census blocks, which probably is not possible.\n\n\n\nOur data set for the census blocks had too many redundant variables, as PCA identified that 25 would explain more than 90% of the variance. This is a great insight as, if inferences made for different clusters are to be made, it would be much more computational efficient to use the PCA variables. FDurthermore, if the inferences are to be extrapolated to other or new census blocks, it would be much more efficient to use the PCA variables to determine their cluster and then make the inferences.\n\n\n\nDecision trees seem to be a great model to predict the average miles per gallon based on city, agency, organization type, Primary UZA population, mode of transportation, and type of service, electric atteries used. Even more effectively, just using Agency, type of service, and mode of transportation seems to be the right choice. Thus, this model could be used to find the optimal combination of these three components in order to choos the most efficient one in every case. That way, we could reduce our carbon footprint while reducing our fossil fuel’s cost.\n\n\n\nPeople seem to care about time and stops the most when writing on reddit about public transportation. This gives us an insight that time is one of the most important things for people and having stops available for when they use public transportation seems to be logical. Thus, we could invest in the development of more stops and more efficient routes to reduce the time people spend on public transportation.\nAdditionally, another key insight worth highlighting is that transportation has been performing worse over time. However, the energy consumed seems to also have decreased (slowly) overtime. Furthermore, month seems to have an apparent effect on performance, which could be overcome by investing in more resources during the months where performance is lower. Finally, understanding what countries play the biggest role in the manufacturing of public transportation vehicles could be a great insight to understand the impact of public transportation in the world. More advances in R&D could be made to improve the manufacturing of these vehicles and reduce their carbon footprint in these countries."
  },
  {
    "objectID": "Conclusion.html#general-conclusion",
    "href": "Conclusion.html#general-conclusion",
    "title": "Conclusion",
    "section": "General Conclusion",
    "text": "General Conclusion\nAs shown in our study, transportation is a key component of society. It not only allows people to move from one place to another but is also needed for the economy to function. Furthermorem, it plays a big role in city and economic development as well as in the environment (accounting for almost 30% of the polution in the US). Thus, it is important to understand how it works and how it can be improved.\nOne of the biggest takeaways from this study is that census blocks can be grouped since they seem to have similar characteristics. This insight is very revealing as implementing changes or building public transportation models for each place in the US is too time consuming and costly for the US. Thus, the first suggestion to improve public transportation would be to generate twelve (one for each cluster) base transportation models that could be slightly tweaked to fit the more specific needs of each census block. This would allow the US to generate twelve very robust models that would work much better than creating over 220,000 simple models. Furthermore, improvements on each could be implemented in more places just with slight changes, making the process much more efficient.\nAnother key insight is that the Agency, type of service, and mode of transportation seem to be deterministic in the fuel consumption efficiency. This could be used to find the most optimal combination of these three components in order to choose the most efficient one in every case. That way, we could reduce our carbon footprint while reducing our fossil fuel’s cost.\nIncentivizing the use of public transpotation is also very important. That way, we would be able to reduce the amount of cars on the road, reducing the amount of polution and traffic. This could be done by improving the areas that are most important to people. Our Naive Bayes model can do a good job at detecting negative comments from people which then can be analyzed in order to identify the main issues people have with public transportation. With this information, issues can be addressed and solved. Additionally, what seems to be the most important to people is time, and the public transportation performance has actually been decreasing over the past years. We have information that could be used to revert this tendency such as the months that make the reliability of public transportation worse. Thus, we could invest in more resources during the months where performance is lower.\nMoreover, understanding what countries play the biggest role in the manufacturing of public transportation vehicles could be a great insight to understand the impact of public transportation in the world. More advances in R&D could be made to improve the manufacturing of these vehicles and reduce their carbon footprint in these countries. Associations with these countries could be done to reach a common goal of reducing the carbon footprint of public transportation while improving its performance.\nFinally, it is worth noting that this is a very complex topic and that this analysis only provides an overview of some of the most important aspects of public transportation. There are many other aspects that could be analyzed and that could provide more insights. Further research in fuels, economic factors, and other areas are definitely needed to be able to effectively implement the reccomendations made in this study. However, this analysis provides a good starting point to understand the current situation of public transportation and how it might be improved."
  },
  {
    "objectID": "Conclusion.html#lets-preserve-the-beautiful-planet-we-live-in",
    "href": "Conclusion.html#lets-preserve-the-beautiful-planet-we-live-in",
    "title": "Conclusion",
    "section": "Let’s preserve the beautiful planet we live in!",
    "text": "Let’s preserve the beautiful planet we live in!"
  },
  {
    "objectID": "Classification_mult_models.html",
    "href": "Classification_mult_models.html",
    "title": "Homework-4: Classification",
    "section": "",
    "text": "Submission:\n\nSubmit this assignment to Canvas as THREE HTML (or PDF) files, one for each section below.\nMake sure that the files are self contained and render correctly without external dependencies.\n\n\n\n\n\n\nThis assignment builds off of Lab-4.1, in that lab you made a python class which acted as a “wrapper” for SkLearn. Use your solution to that lab as a starting point, but add to it in the following ways\nGeneralize the class so has functionality to switch between four classifier options (Logistic regression, LDA, QDA, SVM).\nModify the methods of the class appropriately to work with all 4 methods\nSimilar to lab-4.1, run the methods of the class for all four models, on the MNIST dataset and display the results.\n\nNote: I have also added an explanation of SVMs in the class, as it was not covered in the lab (the function explanation explains all as it could be informative to which model to pick).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\nfrom sklearn.preprocessing import label_binarize\nimport pandas as pd\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# INSERT SOLUTION HERE\n\nclass Classifier:\n    def __init__(self, params, model_type):\n\n        self.model_type = model_type\n        self.params = params if params is not None else {}\n\n        # Choosing model\n        if model_type == 'logistic':\n            self.model = LogisticRegression(**self.params)\n        elif model_type == 'lda':\n            self.model = LDA(**self.params)\n        elif model_type == 'qda':\n            self.model = QDA(**self.params)\n        elif model_type == 'svm':\n            self.model = SVC(probability=True, **self.params)\n        else:\n            raise ValueError(\"That is not a valid model type! Please use logistic, lda, qda, or svm instead.\")\n\n    # fit model\n    def fit(self, x_train, y_train):\n      self.model.fit(x_train, y_train)\n      self.classes_ = np.unique(y_train)\n\n    # hyperparameter tuning\n    def tune(self, x_train, y_train, params_try):\n\n        # Gridsearch with cross-validation (K=4)\n        self.tuner = GridSearchCV(self.model, params_try, scoring = 'accuracy', cv = 4)\n        self.tuner.fit(x_train, y_train)\n\n        # Choose best model\n        self.model = self.tuner.best_estimator_ \n\n    # plot\n    def plot(self, x_test, y_test):\n\n        # plot confusion matrix\n        plt.figure(figsize=(12, 8))\n        preds = self.model.predict(x_test)\n        cm = confusion_matrix(y_test, preds)\n\n        sns.heatmap(cm, annot = True, fmt = 'd', cmap = plt.cm.Blues)\n        plt.title('Confusion Matrix')\n        plt.xlabel('Predicted Label')\n        plt.ylabel('Actual Label')\n        plt.show()\n\n        # Number of classes\n        num_class = len(self.classes_)\n        \n        # Prob. of class\n        y_probability = self.model.predict_proba(x_test)\n        \n        # binary class.\n        if num_class == 2:\n            # ROC curve and ROC area for each class\n            result_roc = roc_curve(y_test, y_probability[:, 1])\n            false_positive_rate, true_positive_rate, _ = result_roc\n            auc_roc = auc(false_positive_rate, true_positive_rate)\n            \n            # plot ROC curve\n            plt.figure(figsize=(12, 8))\n\n            # Adding random classifier line\n            plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label='Random classifier')\n\n            plt.plot(false_positive_rate, true_positive_rate, color='darkred', lw=2, label=f'ROC curve (area = {auc_roc:.2f})') \n            plt.title('ROC Curve')\n            plt.xlabel('False Positive Rate')\n            plt.ylabel('True Positive Rate')\n            plt.legend(loc=\"lower right\")\n            plt.show()\n        \n        # multi-class classification\n        else:\n            # binary class\n            y_test_binary = label_binarize(y_test, classes=self.classes_)\n\n            true_positive_rate = dict()\n            false_positive_rate = dict()\n            auc_roc = dict()\n            \n            # ROC curve & area for class\n            for i in range(num_class):\n                true_binary_outcomes_i = y_test_binary[:, i]\n                predicted_probabilities_i = y_probability[:, i]\n\n                fpr_i, tpr_i, _ = roc_curve(true_binary_outcomes_i, predicted_probabilities_i)\n                false_positive_rate[i] = fpr_i\n                true_positive_rate[i] = tpr_i\n\n                auc_roc[i] = auc(false_positive_rate[i], true_positive_rate[i])\n            \n            # plot ROC \n            plt.figure(figsize=(12, 8))\n\n            # Adding random classifier line\n            plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random classifier')\n            \n            # plot ROC curve\n            for i in range(num_class):\n                plt.plot(false_positive_rate[i], true_positive_rate[i], lw=2,\n                         label='ROC curve of number/class {0} (area ≈ {1:0.2f})'\n                               ''.format(i, auc_roc[i]))\n            \n            plt.title('ROC Curves')\n            plt.xlabel('False Positive Rate')\n            plt.ylabel('True Positive Rate')\n            plt.xlim([0.0, 1.02])\n            plt.ylim([0.0, 1.02])\n            plt.legend()\n            plt.show()\n\n    # report\n    def report(self, x_test, y_test):\n\n        # report\n        predictions = self.model.predict(x_test)\n        print(classification_report(y_test, predictions, target_names=[str(cls) for cls in self.classes_]))\n\n    # explanation of models\n    def explain(self):\n        explanation = \"\"\"\n        Logistic Regression is a common model for classification tasks. While it stands out for binary classification problems, it can also handle multi-class\n        Classification problems (with methods like One vs Rest or One vs One). The model is based on the logistic function, which is a sigmoid function that maps the input\n        to a value between 0 and 1. And, in multi-class classification, the model performs this task for each class. Logistic regression is also very robust\n        and can handle noise very well without overfitting. The model is also very interpretable, as it provides the probability score of the class. However, \n        it assumes linearity betweeen the variables and the log-odds produced. It also assumes that the data is independent and identically distributed.\n\n        Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are also common models for classification tasks. LDA assumes that the data is normally distributed\n        and that the classes have the same covariance. QDA, on the other hand, assumes that the data is normally distributed but that the classes have different covariance. Both models\n        are very robust and can handle noise very well without overfitting. They are also very interpretable, as they provide the probability score of the class. However, they assume\n        that the data is normally distributed and that the classes have the same covariance (LDA) or different covariance (QDA). They also assume that the data is independent and \n        identically distributed. The choice on which to use depends on the data you have. While LDA is a simpler model and maybe can produce less overfitting, it\n        can be too simple in some cases. QDA, on the other hand, can be more flexible and can handle more complex data. However, it can also be more prone to overfitting.\n\n        SVMs are a very powerful model for classification tasks. They work by finding the hyperplane that separates the classes with the largest margin. They do this by maximizing the margin between the classes. The points that are closest to the hyperplane are called support vectors. The margin is the distance between the hyperplane and the support vectors. The larger the margin, the better the model. SVMs can also handle non-linear data by using kernel functions. These functions transform the data into a higher-dimensional space where the data is linearly separable. There are many types of kernel functions like linear, polynomial, and radial basis function (RBF) kernels. The choice of kernel function depends on the data you have. While the linear kernel is the simplest, it can be too simple in some cases. The polynomial kernel can handle more complex data, but it can also be more prone to overfitting. The RBF kernel is the most complex and can handle the most complex data. However, it can also be the most prone to overfitting. SVMs are particularly effective when the data is non-linear and when the classes are well separated. They are also very robust and can handle noise very well without overfitting. However, it some cases, they can be computationally expensive and can be difficult to interpret. They also require careful parameter tuning to get the best performance and extrapolation to new data. \n\n        For Classification tasks, accruacy is a good metric to evaluate the model. It is the ratio of the correctly predicted instances to the total instances in the dataset.\n        However, one flaw of accuracy is that it does not take into account the class imbalance. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n        This provides us with the percentage of correctly predicted positive observations. Recall is the ratio of correctly predicted positive observations to the all observations in actual class.\n        F1-score is the harmonic mean of Precision and Recall. It takes into account false positives and false negatives, which could be very beneficial in some cases.\n        \"\"\"\n        print(explanation)\n\n\nLoading & splitting data\n\n\nCode\n# load data\ndigits = datasets.load_digits()\n\n# Splitting data into training and test sets\nx_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=0)\n\nprint(\"X train shape:\", x_train.shape, \"X test shape:\", x_test.shape)\nprint(\"y train shape:\", y_train.shape, \"y test shape:\", y_test.shape)\n\n\nX train shape: (1437, 64) X test shape: (360, 64)\ny train shape: (1437,) y test shape: (360,)\n\n\nExplanation of models\n\n\nCode\n# explaination\n# (setting a model randomly to call the explanation)\nparams = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\nclassifier = Classifier(params, 'logistic')\nclassifier.explain()\n\n\n\n        Logistic Regression is a common model for classification tasks. While it stands out for binary classification problems, it can also handle multi-class\n        Classification problems (with methods like One vs Rest or One vs One). The model is based on the logistic function, which is a sigmoid function that maps the input\n        to a value between 0 and 1. And, in multi-class classification, the model performs this task for each class. Logistic regression is also very robust\n        and can handle noise very well without overfitting. The model is also very interpretable, as it provides the probability score of the class. However, \n        it assumes linearity betweeen the variables and the log-odds produced. It also assumes that the data is independent and identically distributed.\n\n        Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are also common models for classification tasks. LDA assumes that the data is normally distributed\n        and that the classes have the same covariance. QDA, on the other hand, assumes that the data is normally distributed but that the classes have different covariance. Both models\n        are very robust and can handle noise very well without overfitting. They are also very interpretable, as they provide the probability score of the class. However, they assume\n        that the data is normally distributed and that the classes have the same covariance (LDA) or different covariance (QDA). They also assume that the data is independent and \n        identically distributed. The choice on which to use depends on the data you have. While LDA is a simpler model and maybe can produce less overfitting, it\n        can be too simple in some cases. QDA, on the other hand, can be more flexible and can handle more complex data. However, it can also be more prone to overfitting.\n\n        SVMs are a very powerful model for classification tasks. They work by finding the hyperplane that separates the classes with the largest margin. They do this by maximizing the margin between the classes. The points that are closest to the hyperplane are called support vectors. The margin is the distance between the hyperplane and the support vectors. The larger the margin, the better the model. SVMs can also handle non-linear data by using kernel functions. These functions transform the data into a higher-dimensional space where the data is linearly separable. There are many types of kernel functions like linear, polynomial, and radial basis function (RBF) kernels. The choice of kernel function depends on the data you have. While the linear kernel is the simplest, it can be too simple in some cases. The polynomial kernel can handle more complex data, but it can also be more prone to overfitting. The RBF kernel is the most complex and can handle the most complex data. However, it can also be the most prone to overfitting. SVMs are particularly effective when the data is non-linear and when the classes are well separated. They are also very robust and can handle noise very well without overfitting. However, it some cases, they can be computationally expensive and can be difficult to interpret. They also require careful parameter tuning to get the best performance and extrapolation to new data. \n\n        For Classification tasks, accruacy is a good metric to evaluate the model. It is the ratio of the correctly predicted instances to the total instances in the dataset.\n        However, one flaw of accuracy is that it does not take into account the class imbalance. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n        This provides us with the percentage of correctly predicted positive observations. Recall is the ratio of correctly predicted positive observations to the all observations in actual class.\n        F1-score is the harmonic mean of Precision and Recall. It takes into account false positives and false negatives, which could be very beneficial in some cases.\n        \n\n\nLogistic Regression\n\n\nCode\n# Logistic Regression\nparams = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\nclassifier = Classifier(params, 'logistic')\nclassifier.fit(x_train, y_train)\nclassifier.plot(x_test, y_test)\nclassifier.report(x_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        27\n           1       0.89      0.89      0.89        35\n           2       0.97      0.94      0.96        36\n           3       0.91      1.00      0.95        29\n           4       0.97      1.00      0.98        30\n           5       0.97      0.97      0.97        40\n           6       0.98      0.98      0.98        44\n           7       1.00      0.95      0.97        39\n           8       0.88      0.90      0.89        39\n           9       0.95      0.90      0.93        41\n\n    accuracy                           0.95       360\n   macro avg       0.95      0.95      0.95       360\nweighted avg       0.95      0.95      0.95       360\n\n\n\nLDA\n\n\nCode\n# LDA\nparams = {}\nclassifier = Classifier(params, 'lda')\nclassifier.fit(x_train, y_train)\nclassifier.plot(x_test, y_test)\nclassifier.report(x_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98        27\n           1       0.94      0.91      0.93        35\n           2       1.00      0.92      0.96        36\n           3       0.90      0.97      0.93        29\n           4       0.97      0.97      0.97        30\n           5       1.00      0.93      0.96        40\n           6       1.00      0.98      0.99        44\n           7       0.93      1.00      0.96        39\n           8       0.90      0.97      0.94        39\n           9       0.90      0.88      0.89        41\n\n    accuracy                           0.95       360\n   macro avg       0.95      0.95      0.95       360\nweighted avg       0.95      0.95      0.95       360\n\n\n\nQDA\n\n\nCode\n# QDA\nparams = {}\nclassifier = Classifier(params, 'qda')\nclassifier.fit(x_train, y_train)\nclassifier.plot(x_test, y_test)\nclassifier.report(x_test, y_test)\n\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 2 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 3 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 4 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 5 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 6 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 7 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 8 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 9 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        27\n           1       0.77      0.97      0.86        35\n           2       0.93      0.78      0.85        36\n           3       0.93      0.86      0.89        29\n           4       1.00      0.57      0.72        30\n           5       0.80      1.00      0.89        40\n           6       1.00      0.77      0.87        44\n           7       0.72      0.97      0.83        39\n           8       0.71      0.95      0.81        39\n           9       1.00      0.63      0.78        41\n\n    accuracy                           0.85       360\n   macro avg       0.89      0.85      0.85       360\nweighted avg       0.88      0.85      0.85       360\n\n\n\nSVM\n\n\nCode\n# SVM\nparams = {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'}\nclassifier = Classifier(params, 'svm')\nclassifier.fit(x_train, y_train)\nclassifier.plot(x_test, y_test)\nclassifier.report(x_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        27\n           1       0.97      1.00      0.99        35\n           2       1.00      1.00      1.00        36\n           3       1.00      1.00      1.00        29\n           4       1.00      1.00      1.00        30\n           5       0.97      0.97      0.97        40\n           6       1.00      1.00      1.00        44\n           7       1.00      1.00      1.00        39\n           8       1.00      0.97      0.99        39\n           9       0.98      0.98      0.98        41\n\n    accuracy                           0.99       360\n   macro avg       0.99      0.99      0.99       360\nweighted avg       0.99      0.99      0.99       360"
  },
  {
    "objectID": "Classification_mult_models.html#section-3-python-portion",
    "href": "Classification_mult_models.html#section-3-python-portion",
    "title": "Homework-4: Classification",
    "section": "",
    "text": "This assignment builds off of Lab-4.1, in that lab you made a python class which acted as a “wrapper” for SkLearn. Use your solution to that lab as a starting point, but add to it in the following ways\nGeneralize the class so has functionality to switch between four classifier options (Logistic regression, LDA, QDA, SVM).\nModify the methods of the class appropriately to work with all 4 methods\nSimilar to lab-4.1, run the methods of the class for all four models, on the MNIST dataset and display the results.\n\nNote: I have also added an explanation of SVMs in the class, as it was not covered in the lab (the function explanation explains all as it could be informative to which model to pick).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\nfrom sklearn.preprocessing import label_binarize\nimport pandas as pd\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# INSERT SOLUTION HERE\n\nclass Classifier:\n    def __init__(self, params, model_type):\n\n        self.model_type = model_type\n        self.params = params if params is not None else {}\n\n        # Choosing model\n        if model_type == 'logistic':\n            self.model = LogisticRegression(**self.params)\n        elif model_type == 'lda':\n            self.model = LDA(**self.params)\n        elif model_type == 'qda':\n            self.model = QDA(**self.params)\n        elif model_type == 'svm':\n            self.model = SVC(probability=True, **self.params)\n        else:\n            raise ValueError(\"That is not a valid model type! Please use logistic, lda, qda, or svm instead.\")\n\n    # fit model\n    def fit(self, x_train, y_train):\n      self.model.fit(x_train, y_train)\n      self.classes_ = np.unique(y_train)\n\n    # hyperparameter tuning\n    def tune(self, x_train, y_train, params_try):\n\n        # Gridsearch with cross-validation (K=4)\n        self.tuner = GridSearchCV(self.model, params_try, scoring = 'accuracy', cv = 4)\n        self.tuner.fit(x_train, y_train)\n\n        # Choose best model\n        self.model = self.tuner.best_estimator_ \n\n    # plot\n    def plot(self, x_test, y_test):\n\n        # plot confusion matrix\n        plt.figure(figsize=(12, 8))\n        preds = self.model.predict(x_test)\n        cm = confusion_matrix(y_test, preds)\n\n        sns.heatmap(cm, annot = True, fmt = 'd', cmap = plt.cm.Blues)\n        plt.title('Confusion Matrix')\n        plt.xlabel('Predicted Label')\n        plt.ylabel('Actual Label')\n        plt.show()\n\n        # Number of classes\n        num_class = len(self.classes_)\n        \n        # Prob. of class\n        y_probability = self.model.predict_proba(x_test)\n        \n        # binary class.\n        if num_class == 2:\n            # ROC curve and ROC area for each class\n            result_roc = roc_curve(y_test, y_probability[:, 1])\n            false_positive_rate, true_positive_rate, _ = result_roc\n            auc_roc = auc(false_positive_rate, true_positive_rate)\n            \n            # plot ROC curve\n            plt.figure(figsize=(12, 8))\n\n            # Adding random classifier line\n            plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label='Random classifier')\n\n            plt.plot(false_positive_rate, true_positive_rate, color='darkred', lw=2, label=f'ROC curve (area = {auc_roc:.2f})') \n            plt.title('ROC Curve')\n            plt.xlabel('False Positive Rate')\n            plt.ylabel('True Positive Rate')\n            plt.legend(loc=\"lower right\")\n            plt.show()\n        \n        # multi-class classification\n        else:\n            # binary class\n            y_test_binary = label_binarize(y_test, classes=self.classes_)\n\n            true_positive_rate = dict()\n            false_positive_rate = dict()\n            auc_roc = dict()\n            \n            # ROC curve & area for class\n            for i in range(num_class):\n                true_binary_outcomes_i = y_test_binary[:, i]\n                predicted_probabilities_i = y_probability[:, i]\n\n                fpr_i, tpr_i, _ = roc_curve(true_binary_outcomes_i, predicted_probabilities_i)\n                false_positive_rate[i] = fpr_i\n                true_positive_rate[i] = tpr_i\n\n                auc_roc[i] = auc(false_positive_rate[i], true_positive_rate[i])\n            \n            # plot ROC \n            plt.figure(figsize=(12, 8))\n\n            # Adding random classifier line\n            plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random classifier')\n            \n            # plot ROC curve\n            for i in range(num_class):\n                plt.plot(false_positive_rate[i], true_positive_rate[i], lw=2,\n                         label='ROC curve of number/class {0} (area ≈ {1:0.2f})'\n                               ''.format(i, auc_roc[i]))\n            \n            plt.title('ROC Curves')\n            plt.xlabel('False Positive Rate')\n            plt.ylabel('True Positive Rate')\n            plt.xlim([0.0, 1.02])\n            plt.ylim([0.0, 1.02])\n            plt.legend()\n            plt.show()\n\n    # report\n    def report(self, x_test, y_test):\n\n        # report\n        predictions = self.model.predict(x_test)\n        print(classification_report(y_test, predictions, target_names=[str(cls) for cls in self.classes_]))\n\n    # explanation of models\n    def explain(self):\n        explanation = \"\"\"\n        Logistic Regression is a common model for classification tasks. While it stands out for binary classification problems, it can also handle multi-class\n        Classification problems (with methods like One vs Rest or One vs One). The model is based on the logistic function, which is a sigmoid function that maps the input\n        to a value between 0 and 1. And, in multi-class classification, the model performs this task for each class. Logistic regression is also very robust\n        and can handle noise very well without overfitting. The model is also very interpretable, as it provides the probability score of the class. However, \n        it assumes linearity betweeen the variables and the log-odds produced. It also assumes that the data is independent and identically distributed.\n\n        Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are also common models for classification tasks. LDA assumes that the data is normally distributed\n        and that the classes have the same covariance. QDA, on the other hand, assumes that the data is normally distributed but that the classes have different covariance. Both models\n        are very robust and can handle noise very well without overfitting. They are also very interpretable, as they provide the probability score of the class. However, they assume\n        that the data is normally distributed and that the classes have the same covariance (LDA) or different covariance (QDA). They also assume that the data is independent and \n        identically distributed. The choice on which to use depends on the data you have. While LDA is a simpler model and maybe can produce less overfitting, it\n        can be too simple in some cases. QDA, on the other hand, can be more flexible and can handle more complex data. However, it can also be more prone to overfitting.\n\n        SVMs are a very powerful model for classification tasks. They work by finding the hyperplane that separates the classes with the largest margin. They do this by maximizing the margin between the classes. The points that are closest to the hyperplane are called support vectors. The margin is the distance between the hyperplane and the support vectors. The larger the margin, the better the model. SVMs can also handle non-linear data by using kernel functions. These functions transform the data into a higher-dimensional space where the data is linearly separable. There are many types of kernel functions like linear, polynomial, and radial basis function (RBF) kernels. The choice of kernel function depends on the data you have. While the linear kernel is the simplest, it can be too simple in some cases. The polynomial kernel can handle more complex data, but it can also be more prone to overfitting. The RBF kernel is the most complex and can handle the most complex data. However, it can also be the most prone to overfitting. SVMs are particularly effective when the data is non-linear and when the classes are well separated. They are also very robust and can handle noise very well without overfitting. However, it some cases, they can be computationally expensive and can be difficult to interpret. They also require careful parameter tuning to get the best performance and extrapolation to new data. \n\n        For Classification tasks, accruacy is a good metric to evaluate the model. It is the ratio of the correctly predicted instances to the total instances in the dataset.\n        However, one flaw of accuracy is that it does not take into account the class imbalance. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n        This provides us with the percentage of correctly predicted positive observations. Recall is the ratio of correctly predicted positive observations to the all observations in actual class.\n        F1-score is the harmonic mean of Precision and Recall. It takes into account false positives and false negatives, which could be very beneficial in some cases.\n        \"\"\"\n        print(explanation)\n\n\nLoading & splitting data\n\n\nCode\n# load data\ndigits = datasets.load_digits()\n\n# Splitting data into training and test sets\nx_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=0)\n\nprint(\"X train shape:\", x_train.shape, \"X test shape:\", x_test.shape)\nprint(\"y train shape:\", y_train.shape, \"y test shape:\", y_test.shape)\n\n\nX train shape: (1437, 64) X test shape: (360, 64)\ny train shape: (1437,) y test shape: (360,)\n\n\nExplanation of models\n\n\nCode\n# explaination\n# (setting a model randomly to call the explanation)\nparams = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\nclassifier = Classifier(params, 'logistic')\nclassifier.explain()\n\n\n\n        Logistic Regression is a common model for classification tasks. While it stands out for binary classification problems, it can also handle multi-class\n        Classification problems (with methods like One vs Rest or One vs One). The model is based on the logistic function, which is a sigmoid function that maps the input\n        to a value between 0 and 1. And, in multi-class classification, the model performs this task for each class. Logistic regression is also very robust\n        and can handle noise very well without overfitting. The model is also very interpretable, as it provides the probability score of the class. However, \n        it assumes linearity betweeen the variables and the log-odds produced. It also assumes that the data is independent and identically distributed.\n\n        Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are also common models for classification tasks. LDA assumes that the data is normally distributed\n        and that the classes have the same covariance. QDA, on the other hand, assumes that the data is normally distributed but that the classes have different covariance. Both models\n        are very robust and can handle noise very well without overfitting. They are also very interpretable, as they provide the probability score of the class. However, they assume\n        that the data is normally distributed and that the classes have the same covariance (LDA) or different covariance (QDA). They also assume that the data is independent and \n        identically distributed. The choice on which to use depends on the data you have. While LDA is a simpler model and maybe can produce less overfitting, it\n        can be too simple in some cases. QDA, on the other hand, can be more flexible and can handle more complex data. However, it can also be more prone to overfitting.\n\n        SVMs are a very powerful model for classification tasks. They work by finding the hyperplane that separates the classes with the largest margin. They do this by maximizing the margin between the classes. The points that are closest to the hyperplane are called support vectors. The margin is the distance between the hyperplane and the support vectors. The larger the margin, the better the model. SVMs can also handle non-linear data by using kernel functions. These functions transform the data into a higher-dimensional space where the data is linearly separable. There are many types of kernel functions like linear, polynomial, and radial basis function (RBF) kernels. The choice of kernel function depends on the data you have. While the linear kernel is the simplest, it can be too simple in some cases. The polynomial kernel can handle more complex data, but it can also be more prone to overfitting. The RBF kernel is the most complex and can handle the most complex data. However, it can also be the most prone to overfitting. SVMs are particularly effective when the data is non-linear and when the classes are well separated. They are also very robust and can handle noise very well without overfitting. However, it some cases, they can be computationally expensive and can be difficult to interpret. They also require careful parameter tuning to get the best performance and extrapolation to new data. \n\n        For Classification tasks, accruacy is a good metric to evaluate the model. It is the ratio of the correctly predicted instances to the total instances in the dataset.\n        However, one flaw of accuracy is that it does not take into account the class imbalance. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n        This provides us with the percentage of correctly predicted positive observations. Recall is the ratio of correctly predicted positive observations to the all observations in actual class.\n        F1-score is the harmonic mean of Precision and Recall. It takes into account false positives and false negatives, which could be very beneficial in some cases.\n        \n\n\nLogistic Regression\n\n\nCode\n# Logistic Regression\nparams = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\nclassifier = Classifier(params, 'logistic')\nclassifier.fit(x_train, y_train)\nclassifier.plot(x_test, y_test)\nclassifier.report(x_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        27\n           1       0.89      0.89      0.89        35\n           2       0.97      0.94      0.96        36\n           3       0.91      1.00      0.95        29\n           4       0.97      1.00      0.98        30\n           5       0.97      0.97      0.97        40\n           6       0.98      0.98      0.98        44\n           7       1.00      0.95      0.97        39\n           8       0.88      0.90      0.89        39\n           9       0.95      0.90      0.93        41\n\n    accuracy                           0.95       360\n   macro avg       0.95      0.95      0.95       360\nweighted avg       0.95      0.95      0.95       360\n\n\n\nLDA\n\n\nCode\n# LDA\nparams = {}\nclassifier = Classifier(params, 'lda')\nclassifier.fit(x_train, y_train)\nclassifier.plot(x_test, y_test)\nclassifier.report(x_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98        27\n           1       0.94      0.91      0.93        35\n           2       1.00      0.92      0.96        36\n           3       0.90      0.97      0.93        29\n           4       0.97      0.97      0.97        30\n           5       1.00      0.93      0.96        40\n           6       1.00      0.98      0.99        44\n           7       0.93      1.00      0.96        39\n           8       0.90      0.97      0.94        39\n           9       0.90      0.88      0.89        41\n\n    accuracy                           0.95       360\n   macro avg       0.95      0.95      0.95       360\nweighted avg       0.95      0.95      0.95       360\n\n\n\nQDA\n\n\nCode\n# QDA\nparams = {}\nclassifier = Classifier(params, 'qda')\nclassifier.fit(x_train, y_train)\nclassifier.plot(x_test, y_test)\nclassifier.report(x_test, y_test)\n\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 2 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 3 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 4 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 5 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 6 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 7 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 8 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n/Users/jbm/miniforge3/envs/gu/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning:\n\nThe covariance matrix of class 9 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        27\n           1       0.77      0.97      0.86        35\n           2       0.93      0.78      0.85        36\n           3       0.93      0.86      0.89        29\n           4       1.00      0.57      0.72        30\n           5       0.80      1.00      0.89        40\n           6       1.00      0.77      0.87        44\n           7       0.72      0.97      0.83        39\n           8       0.71      0.95      0.81        39\n           9       1.00      0.63      0.78        41\n\n    accuracy                           0.85       360\n   macro avg       0.89      0.85      0.85       360\nweighted avg       0.88      0.85      0.85       360\n\n\n\nSVM\n\n\nCode\n# SVM\nparams = {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'}\nclassifier = Classifier(params, 'svm')\nclassifier.fit(x_train, y_train)\nclassifier.plot(x_test, y_test)\nclassifier.report(x_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        27\n           1       0.97      1.00      0.99        35\n           2       1.00      1.00      1.00        36\n           3       1.00      1.00      1.00        29\n           4       1.00      1.00      1.00        30\n           5       0.97      0.97      0.97        40\n           6       1.00      1.00      1.00        44\n           7       1.00      1.00      1.00        39\n           8       1.00      0.97      0.99        39\n           9       0.98      0.98      0.98        41\n\n    accuracy                           0.99       360\n   macro avg       0.99      0.99      0.99       360\nweighted avg       0.99      0.99      0.99       360"
  },
  {
    "objectID": "Text_class.html",
    "href": "Text_class.html",
    "title": "Lab-4.1: Text Classification",
    "section": "",
    "text": "Submission:\n\nYou need to upload ONE document to Canvas when you are done\n\n\nA PDF (or HTML) of the completed form of this notebook\n\n\nThe final uploaded version should NOT have any code-errors present\nAll outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\nInstructions\n\nIn a Jupyter notebook named lab-4.1.ipynb\n\nTrain both an LSTM and a GRU on the IMDB dataset\nSplit the data into training, test, cross validation\nmanually tune the hyper-parameters and ANN architecture to get the highest accuracy that you can\nYou only need to do this week’s example with Keras (i.e. no PyTorch)\nNormalize the data as needed\nVisualize the results at the end where possible\nPartition data into training, validation, and test\nMonitor training and validation throughout training by plotting\nPrint training, validation, and test errors at the very end\nYou MUST use early stopping: click here\nDo MANUAL hyper parameter tuning to try to achieve an optimal fit model\n\ni.e. best training/validation loss without over-fitting\nExplore L1 and L2 regularization and dropout\nExplore different optimizers\nUse the loss functions specified in the textbook\nExplore different options for activation functions, network size/depth, etc\n\n\nDocument what is going on in the code, as needed, with narrative markdown text between cells.\nSubmit the version with hyper parameters that provide the optimal fit\n\ni.e. you don’t need to show the outputs of your hyper-parameter tuning process\nSee the Chollet textbook for reference code"
  },
  {
    "objectID": "Text_class.html#lab-4.1-text-classification",
    "href": "Text_class.html#lab-4.1-text-classification",
    "title": "Lab-4.1: Text Classification",
    "section": "",
    "text": "Submission:\n\nYou need to upload ONE document to Canvas when you are done\n\n\nA PDF (or HTML) of the completed form of this notebook\n\n\nThe final uploaded version should NOT have any code-errors present\nAll outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\nInstructions\n\nIn a Jupyter notebook named lab-4.1.ipynb\n\nTrain both an LSTM and a GRU on the IMDB dataset\nSplit the data into training, test, cross validation\nmanually tune the hyper-parameters and ANN architecture to get the highest accuracy that you can\nYou only need to do this week’s example with Keras (i.e. no PyTorch)\nNormalize the data as needed\nVisualize the results at the end where possible\nPartition data into training, validation, and test\nMonitor training and validation throughout training by plotting\nPrint training, validation, and test errors at the very end\nYou MUST use early stopping: click here\nDo MANUAL hyper parameter tuning to try to achieve an optimal fit model\n\ni.e. best training/validation loss without over-fitting\nExplore L1 and L2 regularization and dropout\nExplore different optimizers\nUse the loss functions specified in the textbook\nExplore different options for activation functions, network size/depth, etc\n\n\nDocument what is going on in the code, as needed, with narrative markdown text between cells.\nSubmit the version with hyper parameters that provide the optimal fit\n\ni.e. you don’t need to show the outputs of your hyper-parameter tuning process\nSee the Chollet textbook for reference code"
  },
  {
    "objectID": "Text_class.html#libraries",
    "href": "Text_class.html#libraries",
    "title": "Lab-4.1: Text Classification",
    "section": "Libraries",
    "text": "Libraries\nHere we load the libraries we will use:\n\n\nCode\n# libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.datasets import imdb\nfrom keras import preprocessing\nimport numpy as np\nfrom keras.models import Sequential \nfrom keras import layers\nimport matplotlib.pyplot as plt\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import KFold\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.regularizers import l1_l2"
  },
  {
    "objectID": "Text_class.html#params",
    "href": "Text_class.html#params",
    "title": "Lab-4.1: Text Classification",
    "section": "Params",
    "text": "Params\nNow, we are setting the parameters that will be later used in our models:\n\n\nCode\nmax_features = 10000 \nbatch_size   = 1000   \nmaxlen       = 250      \nverbose      = 1\nembed_dim    = 16    \nreduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.2, min_lr=1e-6)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
  },
  {
    "objectID": "Text_class.html#data",
    "href": "Text_class.html#data",
    "title": "Lab-4.1: Text Classification",
    "section": "Data",
    "text": "Data\nThis code obtained from professor’s notebook loads the IMDB dataset and splits it into training, validation, and test sets:\n\n\nCode\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(x_train[0][0:10]) # ,y_train.shape)\n\n#truncating='pre' --&gt; KEEPS THE LAST 20 WORDS\n#truncating='post' --&gt; KEEPS THE FIRST 20 WORDS\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen,truncating='post')\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen,truncating='post')\n# print('input_train shape:', x_train.shape)\nprint(x_train[0][0:10]) # ,y_train.shape)\n# print('input_train shape:', x_train.shape)\n\n#PARTITION DATA\nrand_indices = np.random.permutation(x_train.shape[0])\nCUT=int(0.8*x_train.shape[0]); \ntrain_idx, val_idx = rand_indices[:CUT], rand_indices[CUT:]\nx_val=x_train[val_idx]; y_val=y_train[val_idx]\nx_train=x_train[train_idx]; y_train=y_train[train_idx]\nprint('input_train shape:', x_train.shape)\n\n\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\n[0 0 0 0 0 0 0 0 0 0]\ninput_train shape: (20000, 250)"
  },
  {
    "objectID": "Text_class.html#lstm",
    "href": "Text_class.html#lstm",
    "title": "Lab-4.1: Text Classification",
    "section": "LSTM",
    "text": "LSTM\nHere we are compiling a simple LSTM model with the following architecture:\n\n\nCode\n# create model LSTM\nmodel = Sequential() \nmodel.add(layers.Embedding(max_features, embed_dim, input_length=maxlen))\nmodel.add(layers.LSTM(32, dropout= 0.1, recurrent_dropout= 0.5, kernel_regularizer=l1_l2(l1=0, l2=0)))\nmodel.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=l1_l2(l1=1e-5, l2=0)))\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n\nModel: \"sequential_24\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_17 (Embedding)    (None, 250, 16)           160000    \n                                                                 \n lstm_18 (LSTM)              (None, 32)                6272      \n                                                                 \n dense_23 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 166305 (649.63 KB)\nTrainable params: 166305 (649.63 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nNow, we will train it (we are printing the process to ensure the progress is being made):\n\n\nCode\n# train model with training and validation set\nhistory = model.fit(x_train, y_train, epochs=100, batch_size=batch_size, validation_data=(x_val, y_val), verbose=verbose, callbacks=[reduce_lr, early_stopping])\n\n\nEpoch 1/100\n20/20 [==============================] - 15s 666ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4918 - lr: 1.0000e-04\nEpoch 2/100\n20/20 [==============================] - 12s 612ms/step - loss: 0.6929 - accuracy: 0.5145 - val_loss: 0.6931 - val_accuracy: 0.4952 - lr: 1.0000e-04\nEpoch 3/100\n20/20 [==============================] - 13s 638ms/step - loss: 0.6926 - accuracy: 0.5318 - val_loss: 0.6929 - val_accuracy: 0.5164 - lr: 1.0000e-04\nEpoch 4/100\n20/20 [==============================] - 13s 647ms/step - loss: 0.6922 - accuracy: 0.5481 - val_loss: 0.6925 - val_accuracy: 0.5294 - lr: 1.0000e-04\nEpoch 5/100\n20/20 [==============================] - 13s 657ms/step - loss: 0.6916 - accuracy: 0.5659 - val_loss: 0.6920 - val_accuracy: 0.5442 - lr: 1.0000e-04\nEpoch 6/100\n20/20 [==============================] - 13s 659ms/step - loss: 0.6908 - accuracy: 0.5895 - val_loss: 0.6912 - val_accuracy: 0.5662 - lr: 1.0000e-04\nEpoch 7/100\n20/20 [==============================] - 13s 667ms/step - loss: 0.6895 - accuracy: 0.6181 - val_loss: 0.6898 - val_accuracy: 0.5944 - lr: 1.0000e-04\nEpoch 8/100\n20/20 [==============================] - 12s 608ms/step - loss: 0.6871 - accuracy: 0.6312 - val_loss: 0.6870 - val_accuracy: 0.6134 - lr: 1.0000e-04\nEpoch 9/100\n20/20 [==============================] - 13s 644ms/step - loss: 0.6812 - accuracy: 0.6520 - val_loss: 0.6772 - val_accuracy: 0.6306 - lr: 1.0000e-04\nEpoch 10/100\n20/20 [==============================] - 12s 611ms/step - loss: 0.6508 - accuracy: 0.6685 - val_loss: 0.6131 - val_accuracy: 0.7280 - lr: 1.0000e-04\nEpoch 11/100\n20/20 [==============================] - 12s 620ms/step - loss: 0.5942 - accuracy: 0.7432 - val_loss: 0.5599 - val_accuracy: 0.7724 - lr: 1.0000e-04\nEpoch 12/100\n20/20 [==============================] - 12s 610ms/step - loss: 0.5623 - accuracy: 0.7761 - val_loss: 0.5351 - val_accuracy: 0.7978 - lr: 1.0000e-04\nEpoch 13/100\n20/20 [==============================] - 14s 716ms/step - loss: 0.5415 - accuracy: 0.7887 - val_loss: 0.5164 - val_accuracy: 0.8084 - lr: 1.0000e-04\nEpoch 14/100\n20/20 [==============================] - 13s 647ms/step - loss: 0.5210 - accuracy: 0.8038 - val_loss: 0.5032 - val_accuracy: 0.8126 - lr: 1.0000e-04\nEpoch 15/100\n20/20 [==============================] - 13s 639ms/step - loss: 0.5022 - accuracy: 0.8112 - val_loss: 0.4822 - val_accuracy: 0.8148 - lr: 1.0000e-04\nEpoch 16/100\n20/20 [==============================] - 13s 665ms/step - loss: 0.4839 - accuracy: 0.8164 - val_loss: 0.4645 - val_accuracy: 0.8206 - lr: 1.0000e-04\nEpoch 17/100\n20/20 [==============================] - 13s 640ms/step - loss: 0.4646 - accuracy: 0.8235 - val_loss: 0.4423 - val_accuracy: 0.8300 - lr: 1.0000e-04\nEpoch 18/100\n20/20 [==============================] - 13s 634ms/step - loss: 0.4422 - accuracy: 0.8364 - val_loss: 0.4278 - val_accuracy: 0.8348 - lr: 1.0000e-04\nEpoch 19/100\n20/20 [==============================] - 13s 629ms/step - loss: 0.4224 - accuracy: 0.8488 - val_loss: 0.4181 - val_accuracy: 0.8416 - lr: 1.0000e-04\nEpoch 20/100\n20/20 [==============================] - 13s 642ms/step - loss: 0.4066 - accuracy: 0.8579 - val_loss: 0.4128 - val_accuracy: 0.8452 - lr: 1.0000e-04\nEpoch 21/100\n20/20 [==============================] - 13s 650ms/step - loss: 0.3889 - accuracy: 0.8636 - val_loss: 0.4041 - val_accuracy: 0.8522 - lr: 1.0000e-04\nEpoch 22/100\n20/20 [==============================] - 13s 657ms/step - loss: 0.3821 - accuracy: 0.8714 - val_loss: 0.4067 - val_accuracy: 0.8530 - lr: 1.0000e-04\nEpoch 23/100\n20/20 [==============================] - 14s 675ms/step - loss: 0.3714 - accuracy: 0.8765 - val_loss: 0.3980 - val_accuracy: 0.8574 - lr: 1.0000e-04\nEpoch 24/100\n20/20 [==============================] - 13s 663ms/step - loss: 0.3604 - accuracy: 0.8816 - val_loss: 0.3987 - val_accuracy: 0.8586 - lr: 1.0000e-04\nEpoch 25/100\n20/20 [==============================] - 13s 662ms/step - loss: 0.3495 - accuracy: 0.8860 - val_loss: 0.3822 - val_accuracy: 0.8654 - lr: 1.0000e-04\nEpoch 26/100\n20/20 [==============================] - 13s 636ms/step - loss: 0.3476 - accuracy: 0.8882 - val_loss: 0.3717 - val_accuracy: 0.8740 - lr: 1.0000e-04\nEpoch 27/100\n20/20 [==============================] - 13s 626ms/step - loss: 0.3413 - accuracy: 0.8934 - val_loss: 0.3878 - val_accuracy: 0.8650 - lr: 1.0000e-04\nEpoch 28/100\n20/20 [==============================] - 13s 628ms/step - loss: 0.3317 - accuracy: 0.8963 - val_loss: 0.3789 - val_accuracy: 0.8706 - lr: 1.0000e-04\nEpoch 29/100\n20/20 [==============================] - 13s 641ms/step - loss: 0.3218 - accuracy: 0.8985 - val_loss: 0.3855 - val_accuracy: 0.8702 - lr: 1.0000e-04\nEpoch 30/100\n20/20 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.8979\nEpoch 30: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n20/20 [==============================] - 13s 653ms/step - loss: 0.3197 - accuracy: 0.8979 - val_loss: 0.3836 - val_accuracy: 0.8686 - lr: 1.0000e-04\nEpoch 31/100\n20/20 [==============================] - 12s 616ms/step - loss: 0.3129 - accuracy: 0.9042 - val_loss: 0.3781 - val_accuracy: 0.8744 - lr: 2.0000e-05\nEpoch 31: early stopping\n\n\nNow, let’s explore the training and validation on a plot:\n\n\nCode\n# Plot\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe model seems to have a good fit for training and validation data, so now we will evaluate it on the test set to see how well our model generalizes to unseen data:\n\n\nCode\n# Evaluate the model on test data\ntest_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n\nprint(f'Test Loss: {test_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy:.4f}')\n\n\n782/782 - 15s - loss: 0.4207 - accuracy: 0.8468 - 15s/epoch - 19ms/step\nTest Loss: 0.4207\nTest Accuracy: 0.8468\n\n\nWe will use this and compare it to the GRU model."
  },
  {
    "objectID": "Text_class.html#gru",
    "href": "Text_class.html#gru",
    "title": "Lab-4.1: Text Classification",
    "section": "GRU",
    "text": "GRU\nHere we are compiling a simple GRU model with the following architecture:\n\n\nCode\n# create model LSTM\nmodel = Sequential() \nmodel.add(layers.Embedding(max_features, embed_dim, input_length=maxlen))\nmodel.add(layers.GRU(64, dropout= 0.1, recurrent_dropout= 0.5, kernel_regularizer=l1_l2(l1=0, l2=0))) \nmodel.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=l1_l2(l1=1e-5, l2=0)))\nmodel.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['acc']) \nmodel.summary()\n\n\nModel: \"sequential_25\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_18 (Embedding)    (None, 250, 16)           160000    \n                                                                 \n gru_6 (GRU)                 (None, 64)                15744     \n                                                                 \n dense_24 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 175809 (686.75 KB)\nTrainable params: 175809 (686.75 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nNow, we will train it (we are printing the process to ensure the progress is being made):\n\n\nCode\n# train model with training and validation set\nhistory = model.fit(x_train, y_train, epochs=50, batch_size=batch_size, validation_data=(x_val, y_val), verbose=verbose, callbacks=[reduce_lr, early_stopping])\n\n\nEpoch 1/50\n20/20 [==============================] - 20s 889ms/step - loss: 0.6919 - acc: 0.5342 - val_loss: 0.6897 - val_acc: 0.5388 - lr: 0.0010\nEpoch 2/50\n20/20 [==============================] - 17s 865ms/step - loss: 0.6703 - acc: 0.6256 - val_loss: 0.5996 - val_acc: 0.7212 - lr: 0.0010\nEpoch 3/50\n20/20 [==============================] - 19s 936ms/step - loss: 0.5100 - acc: 0.7646 - val_loss: 0.3972 - val_acc: 0.8314 - lr: 0.0010\nEpoch 4/50\n20/20 [==============================] - 19s 922ms/step - loss: 0.3792 - acc: 0.8408 - val_loss: 0.3612 - val_acc: 0.8472 - lr: 0.0010\nEpoch 5/50\n20/20 [==============================] - 18s 900ms/step - loss: 0.3162 - acc: 0.8740 - val_loss: 0.3323 - val_acc: 0.8602 - lr: 0.0010\nEpoch 6/50\n20/20 [==============================] - 17s 852ms/step - loss: 0.2679 - acc: 0.8978 - val_loss: 0.3089 - val_acc: 0.8744 - lr: 0.0010\nEpoch 7/50\n20/20 [==============================] - 17s 849ms/step - loss: 0.2345 - acc: 0.9143 - val_loss: 0.3124 - val_acc: 0.8782 - lr: 0.0010\nEpoch 8/50\n20/20 [==============================] - 17s 839ms/step - loss: 0.2074 - acc: 0.9276 - val_loss: 0.3241 - val_acc: 0.8752 - lr: 0.0010\nEpoch 9/50\n20/20 [==============================] - 17s 850ms/step - loss: 0.1946 - acc: 0.9324 - val_loss: 0.3557 - val_acc: 0.8772 - lr: 0.0010\nEpoch 10/50\n20/20 [==============================] - ETA: 0s - loss: 0.1881 - acc: 0.9358\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n20/20 [==============================] - 17s 854ms/step - loss: 0.1881 - acc: 0.9358 - val_loss: 0.3354 - val_acc: 0.8744 - lr: 0.0010\nEpoch 11/50\n20/20 [==============================] - 17s 856ms/step - loss: 0.1534 - acc: 0.9488 - val_loss: 0.3446 - val_acc: 0.8774 - lr: 2.0000e-04\nEpoch 11: early stopping\n\n\nNow, let’s explore the training and validation on a plot:\n\n\nCode\n# Plot\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe model seems to have a good fit for training and validation data, so now we will evaluate it on the test set to see how well our model generalizes to unseen data:\n\n\nCode\n# Evaluate the model on test data\ntest_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n\nprint(f'Test Loss: {test_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy:.4f}')\n\n\n782/782 - 15s - loss: 0.3914 - acc: 0.8582 - 15s/epoch - 19ms/step\nTest Loss: 0.3914\nTest Accuracy: 0.8582\n\n\nThis accruacy is very similar to the one obtained with the LSTM model, which is expected since both models are very similar in terms of architecture and hyperparameters."
  },
  {
    "objectID": "MNIST2.html",
    "href": "MNIST2.html",
    "title": "CNN Multi-class classification Keras",
    "section": "",
    "text": "In this lab we perform multi-class classification with a CNN using PyTorch AND Keras.\nSubmission:\n\nYou need to upload TWO documents to Canvas when you are done\n\n\nA PDF (or HTML) of the completed form of the completed MNIST.ipynb notebook (MNIST in Keras)\n\n\nA PDF (or HTML) of the completed form of the completed CIFAR.ipynb notebook (CIFAR in PyTorch)\n\n\nThe final uploaded version should NOT have any code-errors present\nAll outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\nInstructions\n\nCode Multi-class classification using LeNet type architecture CNN to train\n\n\nMNIST training with Keras, in MNIST.ipynb.\n\n\nCIFAR training with PyTorch, in CIFAR.ipynb.\n\nIf you use an online reference, please cite the source and don’t copy more than a few lines of code\nNormalize the data as needed\nVisualize the results at the end where possible\nPartition data into training, validation, and test\nMonitor training and validation throughout training by plotting\nFor the final fit, report the confusion matrix at the end\nPrint training, validation, and test errors at the very end\nYou MUST use early stopping:\nDo basic MANUAL hyper-parameter tuning to try to achieve an optimal fit model\n\ni.e. best training/validation loss without over-fitting\nExplore L1 and L2 regularization and dropout\nExplore different optimizers\nUse the loss functions specified in the textbook\nExplore different options for activation functions, network size/depth, etc\n\n\nDocument what is going on in the code, as needed, with narrative markdown text between cells.\nSubmit the version with hyper parameters that provide the optimal fit\n\ni.e. you don’t need to show the outputs of your hyper-parameter tuning process"
  },
  {
    "objectID": "MNIST2.html#lab-3.1-cnn-multi-class-classification",
    "href": "MNIST2.html#lab-3.1-cnn-multi-class-classification",
    "title": "CNN Multi-class classification Keras",
    "section": "",
    "text": "In this lab we perform multi-class classification with a CNN using PyTorch AND Keras.\nSubmission:\n\nYou need to upload TWO documents to Canvas when you are done\n\n\nA PDF (or HTML) of the completed form of the completed MNIST.ipynb notebook (MNIST in Keras)\n\n\nA PDF (or HTML) of the completed form of the completed CIFAR.ipynb notebook (CIFAR in PyTorch)\n\n\nThe final uploaded version should NOT have any code-errors present\nAll outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\nInstructions\n\nCode Multi-class classification using LeNet type architecture CNN to train\n\n\nMNIST training with Keras, in MNIST.ipynb.\n\n\nCIFAR training with PyTorch, in CIFAR.ipynb.\n\nIf you use an online reference, please cite the source and don’t copy more than a few lines of code\nNormalize the data as needed\nVisualize the results at the end where possible\nPartition data into training, validation, and test\nMonitor training and validation throughout training by plotting\nFor the final fit, report the confusion matrix at the end\nPrint training, validation, and test errors at the very end\nYou MUST use early stopping:\nDo basic MANUAL hyper-parameter tuning to try to achieve an optimal fit model\n\ni.e. best training/validation loss without over-fitting\nExplore L1 and L2 regularization and dropout\nExplore different optimizers\nUse the loss functions specified in the textbook\nExplore different options for activation functions, network size/depth, etc\n\n\nDocument what is going on in the code, as needed, with narrative markdown text between cells.\nSubmit the version with hyper parameters that provide the optimal fit\n\ni.e. you don’t need to show the outputs of your hyper-parameter tuning process"
  },
  {
    "objectID": "MNIST2.html#architecture",
    "href": "MNIST2.html#architecture",
    "title": "CNN Multi-class classification Keras",
    "section": "Architecture:",
    "text": "Architecture:\nI will be using LeNet 5 architecture for this task, as we know it works specially well for MNIST dataset. Furthermore, the hyperparameters have been tuned as described and my optimal ones are the ones in this version.\n\n# libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport pandas as pd\nfrom keras import regularizers\n\n2024-03-12 10:01:00.637993: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nHere we load the data and reshape it to the correct format. We also normalize it.\n\n# Load dataset\n(train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n\n# Normalize data\ntrain_data = train_data.astype('float32') / 255.0\ntest_data = test_data.astype('float32') / 255.0\n\n# Reshape the data to add dimension chanel\ntrain_data = train_data.reshape((-1, 28, 28, 1))\ntest_data = test_data.reshape((-1, 28, 28, 1))\n\nprint(\"Train data shape before padding:\", train_data.shape)\nprint(\"Test data shape before padding:\", test_data.shape)\n\nTrain data shape before padding: (60000, 28, 28, 1)\nTest data shape before padding: (10000, 28, 28, 1)\n\n\n\n# making labels to categorical\ntrain_y = to_categorical(train_labels, num_classes = 10)\ntest_y = to_categorical(test_labels, num_classes = 10)\n\nWe split training data into training and validation data for training purposes.\n\n# creating validation data\ntrain_data, val_data, train_y, val_y = train_test_split(train_data, train_y, test_size=0.2, random_state=42)\n\nThis code creates the model with the architecture set in the example in the website of LeNet 5. The hyperparameters have been tuned to the optimal ones.\n\n# Model\nmodel = Sequential()\nmodel.add(Conv2D(filters=6, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1), kernel_regularizer=regularizers.l2(0.0), bias_regularizer=regularizers.l1(0.01)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(84, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n\n\nmodel.build()\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 28, 28, 6)         156       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 14, 14, 6)         0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 5, 5, 16)          0         \n g2D)                                                            \n                                                                 \n flatten (Flatten)           (None, 400)               0         \n                                                                 \n dense (Dense)               (None, 256)               102656    \n                                                                 \n dropout (Dropout)           (None, 256)               0         \n                                                                 \n dense_1 (Dense)             (None, 84)                21588     \n                                                                 \n dense_2 (Dense)             (None, 10)                850       \n                                                                 \n=================================================================\nTotal params: 127666 (498.70 KB)\nTrainable params: 127666 (498.70 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nWe now compile the model with the Adam optimizer and the categorical crossentropy loss function. We also set the metrics to accuracy.\n\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n\nWe have found in keras and tensorflow documentation that there is a way to reduce the learning rate when the validation loss stops improving. This is done with the ReduceLROnPlateau callback. We will use this callback to reduce the learning rate by a factor of 0.2 if the validation loss does not improve in 3 epochs.\n\n# learning rate annealer\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', \n                              patience=3, \n                              verbose=1, \n                              factor=0.2, \n                              min_lr=1e-6)\n\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=4, verbose=1)\n\nWe fit the model, train it, and test it.\n\nhistory = model.fit(train_data, train_y, epochs=50, batch_size=64, validation_data=(val_data, val_y), callbacks=[reduce_lr, early_stopping])\n\nEpoch 1/50\n750/750 [==============================] - 14s 17ms/step - loss: 0.7968 - accuracy: 0.7721 - val_loss: 0.2479 - val_accuracy: 0.9270 - lr: 1.0000e-04\nEpoch 2/50\n750/750 [==============================] - 13s 18ms/step - loss: 0.2313 - accuracy: 0.9294 - val_loss: 0.1574 - val_accuracy: 0.9542 - lr: 1.0000e-04\nEpoch 3/50\n750/750 [==============================] - 13s 18ms/step - loss: 0.1645 - accuracy: 0.9502 - val_loss: 0.1158 - val_accuracy: 0.9669 - lr: 1.0000e-04\nEpoch 4/50\n750/750 [==============================] - 13s 18ms/step - loss: 0.1295 - accuracy: 0.9612 - val_loss: 0.0942 - val_accuracy: 0.9726 - lr: 1.0000e-04\nEpoch 5/50\n750/750 [==============================] - 15s 20ms/step - loss: 0.1071 - accuracy: 0.9669 - val_loss: 0.0847 - val_accuracy: 0.9749 - lr: 1.0000e-04\nEpoch 6/50\n750/750 [==============================] - 14s 19ms/step - loss: 0.0939 - accuracy: 0.9713 - val_loss: 0.0728 - val_accuracy: 0.9784 - lr: 1.0000e-04\nEpoch 7/50\n750/750 [==============================] - 15s 20ms/step - loss: 0.0833 - accuracy: 0.9745 - val_loss: 0.0677 - val_accuracy: 0.9808 - lr: 1.0000e-04\nEpoch 8/50\n750/750 [==============================] - 14s 18ms/step - loss: 0.0729 - accuracy: 0.9780 - val_loss: 0.0626 - val_accuracy: 0.9824 - lr: 1.0000e-04\nEpoch 9/50\n750/750 [==============================] - 14s 19ms/step - loss: 0.0685 - accuracy: 0.9791 - val_loss: 0.0578 - val_accuracy: 0.9844 - lr: 1.0000e-04\nEpoch 10/50\n750/750 [==============================] - 14s 19ms/step - loss: 0.0616 - accuracy: 0.9803 - val_loss: 0.0561 - val_accuracy: 0.9839 - lr: 1.0000e-04\nEpoch 11/50\n750/750 [==============================] - 16s 21ms/step - loss: 0.0565 - accuracy: 0.9827 - val_loss: 0.0535 - val_accuracy: 0.9847 - lr: 1.0000e-04\nEpoch 12/50\n750/750 [==============================] - 14s 19ms/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9854 - lr: 1.0000e-04\nEpoch 13/50\n750/750 [==============================] - 14s 18ms/step - loss: 0.0501 - accuracy: 0.9848 - val_loss: 0.0498 - val_accuracy: 0.9854 - lr: 1.0000e-04\nEpoch 14/50\n750/750 [==============================] - 14s 18ms/step - loss: 0.0463 - accuracy: 0.9851 - val_loss: 0.0466 - val_accuracy: 0.9863 - lr: 1.0000e-04\nEpoch 15/50\n750/750 [==============================] - 14s 18ms/step - loss: 0.0435 - accuracy: 0.9863 - val_loss: 0.0463 - val_accuracy: 0.9866 - lr: 1.0000e-04\nEpoch 16/50\n750/750 [==============================] - 15s 19ms/step - loss: 0.0418 - accuracy: 0.9866 - val_loss: 0.0435 - val_accuracy: 0.9876 - lr: 1.0000e-04\nEpoch 17/50\n750/750 [==============================] - 16s 21ms/step - loss: 0.0394 - accuracy: 0.9875 - val_loss: 0.0436 - val_accuracy: 0.9875 - lr: 1.0000e-04\nEpoch 18/50\n750/750 [==============================] - 16s 21ms/step - loss: 0.0369 - accuracy: 0.9880 - val_loss: 0.0421 - val_accuracy: 0.9883 - lr: 1.0000e-04\nEpoch 19/50\n750/750 [==============================] - 16s 22ms/step - loss: 0.0344 - accuracy: 0.9889 - val_loss: 0.0448 - val_accuracy: 0.9875 - lr: 1.0000e-04\nEpoch 20/50\n750/750 [==============================] - 14s 18ms/step - loss: 0.0328 - accuracy: 0.9897 - val_loss: 0.0426 - val_accuracy: 0.9885 - lr: 1.0000e-04\nEpoch 21/50\n750/750 [==============================] - 14s 19ms/step - loss: 0.0314 - accuracy: 0.9899 - val_loss: 0.0422 - val_accuracy: 0.9878 - lr: 1.0000e-04\nEpoch 22/50\n750/750 [==============================] - 16s 21ms/step - loss: 0.0295 - accuracy: 0.9905 - val_loss: 0.0444 - val_accuracy: 0.9876 - lr: 1.0000e-04\nEpoch 23/50\n750/750 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9909\nEpoch 23: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n750/750 [==============================] - 15s 21ms/step - loss: 0.0279 - accuracy: 0.9909 - val_loss: 0.0421 - val_accuracy: 0.9886 - lr: 1.0000e-04\nEpoch 24/50\n750/750 [==============================] - 15s 20ms/step - loss: 0.0227 - accuracy: 0.9932 - val_loss: 0.0390 - val_accuracy: 0.9889 - lr: 2.0000e-05\nEpoch 25/50\n750/750 [==============================] - 15s 20ms/step - loss: 0.0228 - accuracy: 0.9930 - val_loss: 0.0379 - val_accuracy: 0.9894 - lr: 2.0000e-05\nEpoch 26/50\n750/750 [==============================] - 15s 21ms/step - loss: 0.0216 - accuracy: 0.9933 - val_loss: 0.0373 - val_accuracy: 0.9901 - lr: 2.0000e-05\nEpoch 27/50\n750/750 [==============================] - 16s 21ms/step - loss: 0.0211 - accuracy: 0.9935 - val_loss: 0.0370 - val_accuracy: 0.9897 - lr: 2.0000e-05\nEpoch 28/50\n750/750 [==============================] - 15s 19ms/step - loss: 0.0207 - accuracy: 0.9934 - val_loss: 0.0371 - val_accuracy: 0.9900 - lr: 2.0000e-05\nEpoch 29/50\n750/750 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9935\nEpoch 29: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n750/750 [==============================] - 16s 21ms/step - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.0376 - val_accuracy: 0.9898 - lr: 2.0000e-05\nEpoch 30/50\n750/750 [==============================] - 16s 21ms/step - loss: 0.0198 - accuracy: 0.9938 - val_loss: 0.0370 - val_accuracy: 0.9900 - lr: 4.0000e-06\nEpoch 30: early stopping\n\n\n\n# Plot training and validation accuracy/loss\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\ntest_loss, test_accuracy = model.evaluate(test_data, test_y)\nprint(\"Test accuracy:\", test_accuracy)\n\n313/313 [==============================] - 1s 3ms/step - loss: 0.0276 - accuracy: 0.9912\nTest accuracy: 0.9911999702453613\n\n\n\n# model.predict\npredictions = model.predict(test_data)\nprint(\"Predictions shape:\", predictions.shape)\n\n# for debugging\nprint(\"Test labels shape (one-hot):\", test_y.shape)\n\n# Confusion Matrix\ncm = confusion_matrix(np.argmax(test_y, axis=1), np.argmax(predictions, axis=1))\n\nplt.figure(figsize=(8, 8))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.title('Confusion Matrix')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\n313/313 [==============================] - 1s 3ms/step\nPredictions shape: (10000, 10)\nTest labels shape (one-hot): (10000, 10)"
  },
  {
    "objectID": "VoterSentiment_interactive.html",
    "href": "VoterSentiment_interactive.html",
    "title": "Sentiment Analysis",
    "section": "",
    "text": "The goal of this tab is to display the sentiment analysis procedure followed in the project. In this method, we have used the pretrained model sentimentdl_use_twitter from John Snow labs, as it has been built to leverage spark NLP while being trained on Twitter data.\n\n\n\n\n\n\n\nIn this project we are assuming that the slang utilized in Reddit and Twitter is pretty similar, especially on political comments. Thus, we will be relying on this model to classify the sentiment of the Reddit posts, which is a crucial step in our labeling procedure."
  },
  {
    "objectID": "VoterSentiment_interactive.html#overview",
    "href": "VoterSentiment_interactive.html#overview",
    "title": "Sentiment Analysis",
    "section": "",
    "text": "The goal of this tab is to display the sentiment analysis procedure followed in the project. In this method, we have used the pretrained model sentimentdl_use_twitter from John Snow labs, as it has been built to leverage spark NLP while being trained on Twitter data.\n\n\n\n\n\n\n\nIn this project we are assuming that the slang utilized in Reddit and Twitter is pretty similar, especially on political comments. Thus, we will be relying on this model to classify the sentiment of the Reddit posts, which is a crucial step in our labeling procedure."
  },
  {
    "objectID": "VoterSentiment_interactive.html#process",
    "href": "VoterSentiment_interactive.html#process",
    "title": "Sentiment Analysis",
    "section": "Process",
    "text": "Process\n\nImporting libraries and initializing spark session\n\n\n\nCode\nfrom pyspark.sql.functions import *\n# from pyspark.sql import SparkSession\nimport sparknlp\nfrom sparknlp.base import DocumentAssembler\nfrom sparknlp.annotator import UniversalSentenceEncoder, SentimentDLModel\nfrom sparknlp.pretrained import PretrainedPipeline\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\n\n\n\nReading in the data\n\n\n\nCode\ndataset_submissions = spark.read.parquet(\"azureml://datastores/workspaceblobstore/paths/cleandata/submissions_w_keywords_no_filter.parquet\") \\\n  .withColumnRenamed(\"Roe v. Wade\", \"Roe_v_Wade\").withColumnRenamed(\"selftext\", \"body\")\n\ndataset_comments = spark.read.parquet(\"azureml://datastores/workspaceblobstore/paths/cleandata/comments_w_keywords_no_filter.parquet\") \\\n  .withColumnRenamed(\"Roe v. Wade\", \"Roe_v_Wade\")\n\n\n\nSetting up the sentiment analysis pipeline and fitting the model\n\n\n\nCode\ndocumentAssembler = DocumentAssembler()\\\n    .setInputCol(\"body\")\\\n    .setOutputCol(\"document\")\n    \nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n .setInputCols([\"document\"])\\\n .setOutputCol(\"sentence_embeddings\")\n\n\nsentimentdl = SentimentDLModel.pretrained(name=\"sentimentdl_use_twitter\", lang=\"en\")\\\n    .setInputCols([\"sentence_embeddings\"])\\\n    .setOutputCol(\"sentiment\")\n\nnlpPipeline = Pipeline(\n    stages = [\n        documentAssembler,\n        use,\n        sentimentdl\n        ])\n\n\n\n\nCode\nresult_submissions = nlpPipeline.fit(dataset_submissions).transform(dataset_submissions)\nresult_comments = nlpPipeline.fit(dataset_comments).transform(dataset_comments)\n\n\n\nExtracting only the sentiment from the model and saving the data\n\n\n\nCode\nresult_comments = result_comments.withColumn(\n    \"simple_sentiment\",\n    col(\"sentiment\").getItem(0)[\"result\"]\n)\n\nresult_submissions = result_submissions.withColumn(\n    \"simple_sentiment\",\n    col(\"sentiment\").getItem(0)[\"result\"]\n)\n\n\n\n\nCode\nresult_submissions.write.mode(\"overwrite\").parquet(\"azureml://datastores/workspaceblobstore/paths/cleandata/submissions_w_keywords_and_sentiment.parquet\")\nresult_comments.write.mode(\"overwrite\").parquet(\"azureml://datastores/workspaceblobstore/paths/cleandata/comments_w_keywords_and_sentiment.parquet\")"
  },
  {
    "objectID": "VoterSentiment_interactive.html#data",
    "href": "VoterSentiment_interactive.html#data",
    "title": "Sentiment Analysis",
    "section": "Data",
    "text": "Data\nTake a look at how the data looks after the sentiment analysis has been performed. Keep in mind that all the keyword columns added in the previous work are still present as we will use them along with the sentiment to label the data.\nNote: This model outputs additional information not needed for our purposes, so in step 4 we have extracted the overall sentiment value of the post.\n\nSubmissions\n\n\nCode\nresult_submissions.show(5)\n\n\nStatementMeta(, , , Waiting, , Waiting)\n\n\n+------+--------------------+--------------------+-------------------+------------+--------------------+----+-----+--------------+--------------+------------+----------+------------+---------+--------+-----------+------+-------------+----------+---------------------+----------------+--------+---------------+------------+------------------+-----------+---------+-----------+--------+--------+---------------+-----------------------+-----------------+-------------+---------------+------------------------+---------------+-------------+-----------------+----------+--------------+--------+----------------+-------------+----------------+---------------+---------------+------------------+-----------------+------------+-----------------+-----+--------+-----------------+-------------+---------------+--------------+-------------+-------+-----------+-------------------+-------------------+---------------+--------------+----------+-----+----------------------+---------------------+----------------+---------------+---------+----------------+----------+----------+------------+---------------------------+-------------+------------+------------------+-----+-------------+-------+--------------+--------------+-------+--------------+------+-------------------+-----------+---------------+-----------------+--------------+-----------------------+------------------+----+------------+--------------------+---------+--------+----------------+---+--------+----------------+----------+---+----------+-----------+--------------------+------------+--------------------+-------+-------------+-----------------------+-----+---------+-------------+---------+-------------+------+--------------------+--------+----------------+-------------------+-----+--------------------+---------------+------+-------------------+-------+-------------+----------------+----------------+--------------+----------------------+-------------------+----------+------------------------+------------+--------------------+----------+------+-------------------+------+---------------------+--------------+--------------+--------------+---------+--------------------+------------+------------+----------------+--------+--------------+----------------+---------------+------------+--------+------------------+-----------------+------------+--------------+----------+--------+--------------------+---------------+--------+--------------------+-------------+-----------+------------------+-------------------+------------+-------------+--------+---------+-----------+----------+-------------+-----------------+-----------------+---+-----------------+------------+----------------+--------+--------------------+--------+--------------+----------------+---------+----------+--------------------+---------------+------------+--------------+------------+---------------+------------+---------------+-------+----------------------+--------------------+--------------------+--------------------+----------------+\n|    id|              author|               title|        created_utc|num_comments|                body|year|month|     subreddit|tough on crime|Bidenflation|pro-choice|solar energy|insurance|pro-life|immigration|border|public safety|Roe_v_Wade|common-sense gun laws|sanctuary cities|students|climate justice|minimum wage|late-term abortion|living wage|NRA lobby|voter fraud|Medicare|Medicaid|corporate greed|Second Amendment rights|background checks|redistricting|abortion access|Soros-funded prosecutors|law enforcement|soft on crime|electric vehicles|gas prices|mass migration|fracking|renewable energy|red flag laws|racial profiling|social programs|parental rights|election integrity|defund the police|free college|my body my choice|fetus|migrants|Voting Rights Act|border patrol|stimulus checks|global warming|police reform|polling|citizenship|woke indoctrination|illegal immigration|stolen election|Green New Deal|Republican|crime|mental health services|police accountability|carbon footprint|teacher funding|education|vaccine mandates|healthcare|wind power|fossil fuels|healthcare is a human right|school choice|tax the rich|protect the unborn|AR-15|public health|firearm|big government|cost of living|tuition|heartbeat bill|SCOTUS|energy independence|Republicans|systemic racism|detention centers|build the wall|personal responsibility|disenfranchisement|DACA|gun violence|standardized testing|overtaxed|Democrat|economic freedom|ACA|Dreamers|carbon emissions|carbon tax|GDP|Democratic|January 6th|reproductive justice|climate hoax|inclusive curriculum|economy|border crisis|Department of Education|wages|recession|green tyranny|coal jobs|violent crime|ballot|mail-in ballot abuse|abortion|Second Amendment|reproductive rights|taxes|critical race theory|Federal Reserve|safety|equity in education|college|mass shooting|immigrant rights|Medicare for All|mass shootings|pathway to citizenship|socialized medicine|drop boxes|student loan forgiveness|abortion law|environmental crisis|curriculum|asylum|ban assault weapons|voting|protect gun ownership|climate change|women’s rights|gerrymandering|book bans|expand voting rights|clean energy|trigger laws|voucher programs|net zero|gun-free zones|school shootings|Paris Agreement|early voting|midterms|Planned Parenthood|wealth inequality|forced birth|Dobbs decision|gun rights|policing|universal healthcare|absentee ballot|Title 42|public health crisis|voter turnout|gun control|Black Lives Matter|heartbeat detection|unemployment|prison reform|teaching|Democrats|bail reform|open carry|law and order|private insurance|Keystone pipeline|ICE|voter suppression|abortion ban|Obamacare repeal|election|pharmaceutical greed|COVID-19|interest rates|migrant caravans|inflation|fair wages|family reunification|border security|debt ceiling|illegal aliens|supply chain|concealed carry|unborn child|rigged election|schools|school board elections|            document| sentence_embeddings|           sentiment|simple_sentiment|\n+------+--------------------+--------------------+-------------------+------------+--------------------+----+-----+--------------+--------------+------------+----------+------------+---------+--------+-----------+------+-------------+----------+---------------------+----------------+--------+---------------+------------+------------------+-----------+---------+-----------+--------+--------+---------------+-----------------------+-----------------+-------------+---------------+------------------------+---------------+-------------+-----------------+----------+--------------+--------+----------------+-------------+----------------+---------------+---------------+------------------+-----------------+------------+-----------------+-----+--------+-----------------+-------------+---------------+--------------+-------------+-------+-----------+-------------------+-------------------+---------------+--------------+----------+-----+----------------------+---------------------+----------------+---------------+---------+----------------+----------+----------+------------+---------------------------+-------------+------------+------------------+-----+-------------+-------+--------------+--------------+-------+--------------+------+-------------------+-----------+---------------+-----------------+--------------+-----------------------+------------------+----+------------+--------------------+---------+--------+----------------+---+--------+----------------+----------+---+----------+-----------+--------------------+------------+--------------------+-------+-------------+-----------------------+-----+---------+-------------+---------+-------------+------+--------------------+--------+----------------+-------------------+-----+--------------------+---------------+------+-------------------+-------+-------------+----------------+----------------+--------------+----------------------+-------------------+----------+------------------------+------------+--------------------+----------+------+-------------------+------+---------------------+--------------+--------------+--------------+---------+--------------------+------------+------------+----------------+--------+--------------+----------------+---------------+------------+--------+------------------+-----------------+------------+--------------+----------+--------+--------------------+---------------+--------+--------------------+-------------+-----------+------------------+-------------------+------------+-------------+--------+---------+-----------+----------+-------------+-----------------+-----------------+---+-----------------+------------+----------------+--------+--------------------+--------+--------------+----------------+---------+----------+--------------------+---------------+------------+--------------+------------+---------------+------------+---------------+-------+----------------------+--------------------+--------------------+--------------------+----------------+\n|v5fek3|       Lionstar-2472|Building better s...|2022-06-05 15:14:09|           0| It should be a t...|2022|    6|         ReVeR|             0|           0|         0|           0|        0|       0|          0|     0|            0|         0|                    0|               0|       0|              0|           0|                 0|          0|        0|          0|       0|       0|              0|                      0|                0|            0|              0|                       0|              0|            0|                0|         0|             0|       0|               0|            0|               0|              0|              0|                 0|                0|           0|                0|    0|       0|                0|            0|              0|             0|            0|      0|          0|                  0|                  0|              0|             0|         0|    0|                     0|                    0|               0|              0|        0|               0|         1|         0|           0|                          0|            0|           0|                 0|    0|            0|      0|             0|             0|      0|             0|     0|                  0|          0|              0|                0|             0|                      0|                 0|   0|           0|                   0|        0|       0|               0|  0|       0|               0|         0|  0|         0|          0|                   0|           0|                   0|      0|            0|                      0|    0|        0|            0|        0|            0|     0|                   0|       0|               0|                  0|    0|                   0|              0|     0|                  0|      0|            0|               0|               0|             0|                     0|                  0|         0|                       0|           0|                   0|         0|     0|                  0|     0|                    0|             0|             0|             0|        0|                   0|           0|           0|               0|       0|             0|               0|              0|           0|       0|                 0|                0|           0|             0|         0|       0|                   0|              0|       0|                   0|            0|          0|                 0|                  0|           0|            0|       0|        0|          0|         0|            0|                0|                0|  0|                0|           0|               0|       0|                   0|       0|             0|               0|        1|         0|                   0|              0|           0|             0|           1|              0|           0|              0|      0|                     0|[{document, 0, 21...|[{sentence_embedd...|[{category, 0, 21...|        positive|\n|xpde8t|     purple-emotions|Will I ever get u...|2022-09-27 10:52:39|           5|Came from a schoo...|2022|    9|        peyups|             0|           0|         0|           0|        0|       0|          0|     0|            0|         0|                    0|               0|       0|              0|           0|                 0|          0|        0|          0|       0|       0|              0|                      0|                0|            0|              0|                       0|              0|            0|                0|         0|             0|       0|               0|            0|               0|              0|              0|                 0|                0|           0|                0|    0|       0|                0|            0|              0|             0|            0|      0|          0|                  0|                  0|              0|             0|         0|    0|                     0|                    0|               0|              0|        0|               0|         0|         0|           0|                          0|            0|           0|                 0|    0|            0|      0|             0|             0|      0|             0|     0|                  0|          0|              0|                0|             0|                      0|                 0|   0|           0|                   0|        0|       0|               0|  0|       0|               0|         0|  0|         0|          0|                   0|           0|                   0|      0|            0|                      0|    0|        0|            0|        0|            0|     0|                   0|       0|               0|                  0|    0|                   0|              0|     0|                  0|      0|            0|               0|               0|             0|                     0|                  0|         0|                       0|           0|                   0|         0|     0|                  0|     0|                    0|             0|             0|             0|        0|                   0|           0|           0|               0|       0|             0|               0|              0|           0|       0|                 0|                0|           0|             0|         0|       0|                   0|              0|       0|                   0|            0|          0|                 0|                  0|           0|            0|       0|        0|          0|         0|            0|                0|                0|  0|                0|           0|               0|       0|                   0|       0|             0|               0|        0|         0|                   0|              0|           0|             0|           0|              0|           0|              0|      0|                     0|[{document, 0, 97...|[{sentence_embedd...|[{category, 0, 97...|        negative|\n|v76b06|      justinstigator|I am critical of ...|2022-06-07 20:11:37|          62|Various Central B...|2022|    6|      Buttcoin|             0|           0|         0|           0|        0|       0|          0|     0|            0|         0|                    0|               0|       0|              0|           0|                 0|          0|        0|          0|       0|       0|              0|                      0|                0|            0|              0|                       0|              0|            0|                0|         0|             0|       0|               0|            0|               0|              0|              0|                 0|                0|           0|                0|    0|       0|                0|            0|              0|             0|            0|      0|          0|                  0|                  0|              0|             0|         0|    0|                     0|                    0|               0|              0|        0|               0|         0|         0|           0|                          0|            0|           0|                 0|    0|            0|      0|             0|             0|      0|             0|     0|                  0|          0|              0|                0|             0|                      0|                 0|   0|           0|                   0|        0|       0|               0|  0|       0|               0|         0|  0|         0|          0|                   0|           0|                   0|      0|            0|                      0|    0|        0|            0|        0|            0|     0|                   0|       0|               0|                  0|    0|                   0|              0|     0|                  0|      0|            0|               0|               0|             0|                     0|                  0|         0|                       0|           0|                   0|         0|     0|                  0|     0|                    0|             0|             0|             0|        0|                   0|           0|           0|               0|       0|             0|               0|              0|           0|       0|                 0|                0|           0|             0|         0|       0|                   0|              0|       0|                   0|            0|          0|                 0|                  0|           0|            0|       0|        0|          0|         0|            0|                0|                0|  0|                0|           0|               0|       0|                   0|       0|             1|               0|        1|         0|                   0|              0|           0|             0|           0|              0|           0|              0|      0|                     0|[{document, 0, 12...|[{sentence_embedd...|[{category, 0, 12...|         neutral|\n|veto4k|             C4pt41n|Park &amp; Ride info|2022-06-18 00:01:35|           1|Are there any mee...|2022|    6|     asheville|             0|           0|         0|           0|        0|       0|          0|     0|            0|         0|                    0|               0|       0|              0|           0|                 0|          0|        0|          0|       0|       0|              0|                      0|                0|            0|              0|                       0|              0|            0|                0|         1|             0|       0|               0|            0|               0|              0|              0|                 0|                0|           0|                0|    0|       0|                0|            0|              0|             0|            0|      0|          0|                  0|                  0|              0|             0|         0|    0|                     0|                    0|               0|              0|        0|               0|         0|         0|           0|                          0|            0|           0|                 0|    0|            0|      0|             0|             0|      0|             0|     0|                  0|          0|              0|                0|             0|                      0|                 0|   0|           0|                   0|        0|       0|               0|  0|       0|               0|         0|  0|         0|          0|                   0|           0|                   0|      0|            0|                      0|    0|        0|            0|        0|            0|     0|                   0|       0|               0|                  0|    0|                   0|              0|     0|                  0|      0|            0|               0|               0|             0|                     0|                  0|         0|                       0|           0|                   0|         0|     0|                  0|     0|                    0|             0|             0|             0|        0|                   0|           0|           0|               0|       0|             0|               0|              0|           0|       0|                 0|                0|           0|             0|         0|       0|                   0|              0|       0|                   0|            0|          0|                 0|                  0|           0|            0|       0|        0|          0|         0|            0|                0|                0|  0|                0|           0|               0|       0|                   0|       0|             0|               0|        0|         0|                   0|              0|           0|             0|           0|              0|           0|              0|      0|                     0|[{document, 0, 20...|[{sentence_embedd...|[{category, 0, 20...|        negative|\n|yq7jq5|TheGloryoftheCobolds|Is validation in ...|2022-11-09 03:39:59|           2|If I am correct, ...|2022|   11|JordanPeterson|             0|           0|         0|           0|        0|       0|          0|     0|            0|         0|                    0|               0|       0|              0|           0|                 0|          0|        0|          0|       0|       0|              0|                      0|                0|            0|              0|                       0|              0|            0|                0|         0|             0|       0|               0|            0|               0|              0|              0|                 0|                0|           0|                0|    0|       0|                0|            0|              0|             0|            0|      0|          0|                  0|                  0|              0|             0|         0|    0|                     0|                    0|               0|              0|        0|               0|         0|         0|           0|                          0|            0|           0|                 0|    0|            0|      0|             0|             0|      0|             0|     0|                  0|          0|              0|                0|             0|                      0|                 0|   0|           0|                   0|        0|       0|               0|  0|       0|               0|         0|  0|         0|          0|                   0|           0|                   0|      0|            0|                      0|    0|        0|            0|        0|            0|     0|                   0|       0|               0|                  0|    0|                   0|              0|     0|                  0|      0|            0|               0|               0|             0|                     0|                  0|         0|                       0|           0|                   0|         0|     0|                  0|     0|                    0|             0|             0|             0|        0|                   0|           0|           0|               0|       0|             0|               0|              0|           0|       0|                 0|                0|           0|             0|         0|       0|                   0|              0|       0|                   0|            0|          0|                 0|                  0|           0|            0|       0|        0|          0|         0|            0|                0|                0|  0|                0|           0|               0|       0|                   0|       0|             0|               0|        0|         0|                   0|              0|           0|             0|           0|              0|           0|              0|      0|                     0|[{document, 0, 67...|[{sentence_embedd...|[{category, 0, 67...|        positive|\n+------+--------------------+--------------------+-------------------+------------+--------------------+----+-----+--------------+--------------+------------+----------+------------+---------+--------+-----------+------+-------------+----------+---------------------+----------------+--------+---------------+------------+------------------+-----------+---------+-----------+--------+--------+---------------+-----------------------+-----------------+-------------+---------------+------------------------+---------------+-------------+-----------------+----------+--------------+--------+----------------+-------------+----------------+---------------+---------------+------------------+-----------------+------------+-----------------+-----+--------+-----------------+-------------+---------------+--------------+-------------+-------+-----------+-------------------+-------------------+---------------+--------------+----------+-----+----------------------+---------------------+----------------+---------------+---------+----------------+----------+----------+------------+---------------------------+-------------+------------+------------------+-----+-------------+-------+--------------+--------------+-------+--------------+------+-------------------+-----------+---------------+-----------------+--------------+-----------------------+------------------+----+------------+--------------------+---------+--------+----------------+---+--------+----------------+----------+---+----------+-----------+--------------------+------------+--------------------+-------+-------------+-----------------------+-----+---------+-------------+---------+-------------+------+--------------------+--------+----------------+-------------------+-----+--------------------+---------------+------+-------------------+-------+-------------+----------------+----------------+--------------+----------------------+-------------------+----------+------------------------+------------+--------------------+----------+------+-------------------+------+---------------------+--------------+--------------+--------------+---------+--------------------+------------+------------+----------------+--------+--------------+----------------+---------------+------------+--------+------------------+-----------------+------------+--------------+----------+--------+--------------------+---------------+--------+--------------------+-------------+-----------+------------------+-------------------+------------+-------------+--------+---------+-----------+----------+-------------+-----------------+-----------------+---+-----------------+------------+----------------+--------+--------------------+--------+--------------+----------------+---------+----------+--------------------+---------------+------------+--------------+------------+---------------+------------+---------------+-------+----------------------+--------------------+--------------------+--------------------+----------------+\nonly showing top 5 rows\n\n\n\n\nComments\n\n\nCode\nresult_comments.show(5)\n\n\nStatementMeta(11387b7c-91c2-4dd1-a4f8-a460b59b7bdd, 48, 33, Finished, Available, Finished)\n\n\n+-------+--------------+--------------------+-------------------+----+-----+-----------------+--------------+------------+----------+------------+---------+--------+-----------+------+-------------+----------+---------------------+----------------+--------+---------------+------------+------------------+-----------+---------+-----------+--------+--------+---------------+-----------------------+-----------------+-------------+---------------+------------------------+---------------+-------------+-----------------+----------+--------------+--------+----------------+-------------+----------------+---------------+---------------+------------------+-----------------+------------+-----------------+-----+--------+-----------------+-------------+---------------+--------------+-------------+-------+-----------+-------------------+-------------------+---------------+--------------+----------+-----+----------------------+---------------------+----------------+---------------+---------+----------------+----------+----------+------------+---------------------------+-------------+------------+------------------+-----+-------------+-------+--------------+--------------+-------+--------------+------+-------------------+-----------+---------------+-----------------+--------------+-----------------------+------------------+----+------------+--------------------+---------+--------+----------------+---+--------+----------------+----------+---+----------+-----------+--------------------+------------+--------------------+-------+-------------+-----------------------+-----+---------+-------------+---------+-------------+------+--------------------+--------+----------------+-------------------+-----+--------------------+---------------+------+-------------------+-------+-------------+----------------+----------------+--------------+----------------------+-------------------+----------+------------------------+------------+--------------------+----------+------+-------------------+------+---------------------+--------------+--------------+--------------+---------+--------------------+------------+------------+----------------+--------+--------------+----------------+---------------+------------+--------+------------------+-----------------+------------+--------------+----------+--------+--------------------+---------------+--------+--------------------+-------------+-----------+------------------+-------------------+------------+-------------+--------+---------+-----------+----------+-------------+-----------------+-----------------+---+-----------------+------------+----------------+--------+--------------------+--------+--------------+----------------+---------+----------+--------------------+---------------+------------+--------------+------------+---------------+------------+---------------+-------+----------------------+--------------------+--------------------+--------------------+----------------+\n|     id|        author|                body|        created_utc|year|month|        subreddit|tough on crime|Bidenflation|pro-choice|solar energy|insurance|pro-life|immigration|border|public safety|Roe_v_Wade|common-sense gun laws|sanctuary cities|students|climate justice|minimum wage|late-term abortion|living wage|NRA lobby|voter fraud|Medicare|Medicaid|corporate greed|Second Amendment rights|background checks|redistricting|abortion access|Soros-funded prosecutors|law enforcement|soft on crime|electric vehicles|gas prices|mass migration|fracking|renewable energy|red flag laws|racial profiling|social programs|parental rights|election integrity|defund the police|free college|my body my choice|fetus|migrants|Voting Rights Act|border patrol|stimulus checks|global warming|police reform|polling|citizenship|woke indoctrination|illegal immigration|stolen election|Green New Deal|Republican|crime|mental health services|police accountability|carbon footprint|teacher funding|education|vaccine mandates|healthcare|wind power|fossil fuels|healthcare is a human right|school choice|tax the rich|protect the unborn|AR-15|public health|firearm|big government|cost of living|tuition|heartbeat bill|SCOTUS|energy independence|Republicans|systemic racism|detention centers|build the wall|personal responsibility|disenfranchisement|DACA|gun violence|standardized testing|overtaxed|Democrat|economic freedom|ACA|Dreamers|carbon emissions|carbon tax|GDP|Democratic|January 6th|reproductive justice|climate hoax|inclusive curriculum|economy|border crisis|Department of Education|wages|recession|green tyranny|coal jobs|violent crime|ballot|mail-in ballot abuse|abortion|Second Amendment|reproductive rights|taxes|critical race theory|Federal Reserve|safety|equity in education|college|mass shooting|immigrant rights|Medicare for All|mass shootings|pathway to citizenship|socialized medicine|drop boxes|student loan forgiveness|abortion law|environmental crisis|curriculum|asylum|ban assault weapons|voting|protect gun ownership|climate change|women’s rights|gerrymandering|book bans|expand voting rights|clean energy|trigger laws|voucher programs|net zero|gun-free zones|school shootings|Paris Agreement|early voting|midterms|Planned Parenthood|wealth inequality|forced birth|Dobbs decision|gun rights|policing|universal healthcare|absentee ballot|Title 42|public health crisis|voter turnout|gun control|Black Lives Matter|heartbeat detection|unemployment|prison reform|teaching|Democrats|bail reform|open carry|law and order|private insurance|Keystone pipeline|ICE|voter suppression|abortion ban|Obamacare repeal|election|pharmaceutical greed|COVID-19|interest rates|migrant caravans|inflation|fair wages|family reunification|border security|debt ceiling|illegal aliens|supply chain|concealed carry|unborn child|rigged election|schools|school board elections|            document| sentence_embeddings|           sentiment|simple_sentiment|\n+-------+--------------+--------------------+-------------------+----+-----+-----------------+--------------+------------+----------+------------+---------+--------+-----------+------+-------------+----------+---------------------+----------------+--------+---------------+------------+------------------+-----------+---------+-----------+--------+--------+---------------+-----------------------+-----------------+-------------+---------------+------------------------+---------------+-------------+-----------------+----------+--------------+--------+----------------+-------------+----------------+---------------+---------------+------------------+-----------------+------------+-----------------+-----+--------+-----------------+-------------+---------------+--------------+-------------+-------+-----------+-------------------+-------------------+---------------+--------------+----------+-----+----------------------+---------------------+----------------+---------------+---------+----------------+----------+----------+------------+---------------------------+-------------+------------+------------------+-----+-------------+-------+--------------+--------------+-------+--------------+------+-------------------+-----------+---------------+-----------------+--------------+-----------------------+------------------+----+------------+--------------------+---------+--------+----------------+---+--------+----------------+----------+---+----------+-----------+--------------------+------------+--------------------+-------+-------------+-----------------------+-----+---------+-------------+---------+-------------+------+--------------------+--------+----------------+-------------------+-----+--------------------+---------------+------+-------------------+-------+-------------+----------------+----------------+--------------+----------------------+-------------------+----------+------------------------+------------+--------------------+----------+------+-------------------+------+---------------------+--------------+--------------+--------------+---------+--------------------+------------+------------+----------------+--------+--------------+----------------+---------------+------------+--------+------------------+-----------------+------------+--------------+----------+--------+--------------------+---------------+--------+--------------------+-------------+-----------+------------------+-------------------+------------+-------------+--------+---------+-----------+----------+-------------+-----------------+-----------------+---+-----------------+------------+----------------+--------+--------------------+--------+--------------+----------------+---------+----------+--------------------+---------------+------------+--------------+------------+---------------+------------+---------------+-------+----------------------+--------------------+--------------------+--------------------+----------------+\n|it1r9ji| EmperorBoston|&gt; People calli...|2022-10-20 09:14:27|2022|   10|           movies|             0|           0|         0|           0|        0|       0|          0|     0|            0|         0|                    0|               0|       0|              0|           0|                 0|          0|        0|          0|       0|       0|              0|                      0|                0|            0|              0|                       0|              0|            0|                0|         0|             0|       0|               0|            0|               0|              0|              0|                 0|                0|           0|                0|    0|       0|                0|            0|              0|             0|            0|      0|          0|                  0|                  0|              0|             0|         0|    0|                     0|                    0|               0|              0|        0|               0|         0|         0|           0|                          0|            0|           0|                 0|    0|            0|      0|             0|             0|      0|             0|     0|                  0|          0|              0|                0|             0|                      0|                 0|   0|           0|                   0|        0|       0|               0|  0|       0|               0|         0|  0|         0|          0|                   0|           0|                   0|      0|            0|                      0|    0|        0|            0|        0|            0|     0|                   0|       0|               0|                  0|    0|                   0|              0|     0|                  0|      0|            0|               0|               0|             0|                     0|                  0|         0|                       0|           0|                   0|         0|     0|                  0|     0|                    0|             0|             0|             0|        0|                   0|           0|           0|               0|       0|             0|               0|              0|           0|       0|                 0|                0|           0|             0|         0|       0|                   0|              0|       0|                   0|            0|          0|                 0|                  0|           0|            0|       0|        0|          0|         0|            0|                0|                0|  0|                0|           0|               0|       0|                   0|       0|             0|               0|        0|         0|                   0|              0|           0|             0|           0|              0|           0|              0|      0|                     0|[{document, 0, 29...|[{sentence_embedd...|[{category, 0, 29...|        positive|\n|i6u2pdo|     [deleted]|You know, I am no...|2022-04-30 21:31:54|2022|    4|         antiwork|             0|           0|         0|           0|        0|       0|          0|     0|            0|         0|                    0|               0|       0|              0|           0|                 0|          0|        0|          0|       0|       0|              0|                      0|                0|            0|              0|                       0|              0|            0|                0|         0|             0|       0|               0|            0|               0|              0|              0|                 0|                0|           0|                0|    0|       0|                0|            0|              0|             0|            0|      0|          0|                  0|                  0|              0|             0|         0|    0|                     0|                    0|               0|              0|        0|               0|         0|         0|           0|                          0|            0|           0|                 0|    0|            0|      0|             0|             0|      0|             0|     0|                  0|          0|              0|                0|             0|                      0|                 0|   0|           0|                   0|        0|       0|               0|  0|       0|               0|         0|  0|         0|          0|                   0|           0|                   0|      0|            0|                      0|    0|        0|            0|        0|            0|     0|                   0|       0|               0|                  0|    0|                   0|              0|     0|                  0|      0|            0|               0|               0|             0|                     0|                  0|         0|                       0|           0|                   0|         0|     0|                  0|     0|                    0|             0|             0|             0|        0|                   0|           0|           0|               0|       0|             0|               0|              0|           0|       0|                 0|                0|           0|             0|         0|       0|                   0|              0|       0|                   0|            0|          0|                 0|                  0|           0|            0|       0|        0|          0|         0|            0|                0|                0|  0|                0|           0|               0|       0|                   0|       0|             0|               0|        0|         0|                   0|              0|           0|             0|           0|              0|           0|              0|      0|                     0|[{document, 0, 97...|[{sentence_embedd...|[{category, 0, 97...|        negative|\n|hvtfif9|nerdywhitemale|But once the huma...|2022-02-06 14:59:25|2022|    2|              HFY|             0|           0|         0|           0|        0|       0|          0|     0|            0|         0|                    0|               0|       0|              0|           0|                 0|          0|        0|          0|       0|       0|              0|                      0|                0|            0|              0|                       0|              0|            0|                0|         0|             0|       0|               0|            0|               0|              0|              0|                 0|                0|           0|                0|    0|       0|                0|            0|              0|             0|            0|      0|          0|                  0|                  0|              0|             0|         0|    0|                     0|                    0|               0|              0|        0|               0|         0|         0|           0|                          0|            0|           0|                 0|    0|            0|      0|             0|             0|      0|             0|     0|                  0|          0|              0|                0|             0|                      0|                 0|   0|           0|                   0|        0|       0|               0|  0|       0|               0|         0|  0|         0|          0|                   0|           0|                   0|      0|            0|                      0|    0|        0|            0|        0|            0|     0|                   0|       0|               0|                  0|    0|                   0|              0|     1|                  0|      0|            0|               0|               0|             0|                     0|                  0|         0|                       0|           0|                   0|         0|     0|                  0|     0|                    0|             0|             0|             0|        0|                   0|           0|           0|               0|       0|             0|               0|              0|           0|       0|                 0|                0|           0|             0|         0|       0|                   0|              0|       0|                   0|            0|          0|                 0|                  0|           0|            0|       0|        0|          0|         0|            0|                0|                0|  0|                0|           0|               0|       0|                   0|       0|             0|               0|        0|         0|                   0|              0|           0|             0|           0|              0|           0|              0|      0|                     0|[{document, 0, 15...|[{sentence_embedd...|[{category, 0, 15...|        negative|\n|iuxws3u| actual_lettuc|You paid under $7...|2022-11-03 20:32:02|2022|   11|         IWantOut|             0|           0|         0|           0|        1|       0|          0|     0|            0|         0|                    0|               0|       0|              0|           0|                 0|          0|        0|          0|       0|       0|              0|                      0|                0|            0|              0|                       0|              0|            0|                0|         0|             0|       0|               0|            0|               0|              0|              0|                 0|                0|           0|                0|    0|       0|                0|            0|              0|             0|            0|      0|          0|                  0|                  0|              0|             0|         0|    0|                     0|                    0|               0|              0|        0|               0|         0|         0|           0|                          0|            0|           0|                 0|    0|            0|      0|             0|             0|      0|             0|     0|                  0|          0|              0|                0|             0|                      0|                 0|   0|           0|                   0|        0|       0|               0|  0|       0|               0|         0|  0|         0|          0|                   0|           0|                   0|      0|            0|                      0|    0|        0|            0|        0|            0|     0|                   0|       0|               0|                  0|    0|                   0|              0|     0|                  0|      0|            0|               0|               0|             0|                     0|                  0|         0|                       0|           0|                   0|         0|     0|                  0|     0|                    0|             0|             0|             0|        0|                   0|           0|           0|               0|       0|             0|               0|              0|           0|       0|                 0|                0|           0|             0|         0|       0|                   0|              0|       0|                   0|            0|          0|                 0|                  0|           0|            0|       0|        0|          0|         0|            0|                0|                0|  0|                0|           0|               0|       0|                   0|       0|             0|               0|        0|         0|                   0|              0|           0|             0|           0|              0|           0|              0|      0|                     0|[{document, 0, 64...|[{sentence_embedd...|[{category, 0, 64...|        negative|\n|ivirbjq|      Psydator|People who don't ...|2022-11-08 06:58:45|2022|   11|interestingasfuck|             0|           0|         0|           0|        0|       0|          0|     0|            0|         0|                    0|               0|       0|              0|           0|                 0|          0|        0|          0|       0|       0|              0|                      0|                0|            0|              0|                       0|              0|            0|                0|         0|             0|       0|               0|            0|               0|              0|              0|                 0|                0|           0|                0|    0|       0|                0|            0|              0|             0|            0|      0|          0|                  0|                  0|              0|             0|         0|    0|                     0|                    0|               0|              0|        0|               0|         0|         0|           0|                          0|            0|           0|                 0|    0|            0|      0|             0|             0|      0|             0|     0|                  0|          0|              0|                0|             0|                      0|                 0|   0|           0|                   0|        0|       0|               0|  0|       0|               0|         0|  0|         0|          0|                   0|           0|                   0|      0|            0|                      0|    0|        0|            0|        0|            0|     0|                   0|       0|               0|                  0|    1|                   0|              0|     0|                  0|      0|            0|               0|               0|             0|                     0|                  0|         0|                       0|           0|                   0|         0|     0|                  0|     0|                    0|             0|             0|             0|        0|                   0|           0|           0|               0|       0|             0|               0|              0|           0|       0|                 0|                0|           0|             0|         0|       0|                   0|              0|       0|                   0|            0|          0|                 0|                  0|           0|            0|       0|        0|          0|         0|            0|                0|                0|  0|                0|           0|               0|       0|                   0|       0|             0|               0|        0|         0|                   0|              0|           0|             0|           0|              0|           0|              0|      0|                     0|[{document, 0, 60...|[{sentence_embedd...|[{category, 0, 60...|        negative|\n+-------+--------------+--------------------+-------------------+----+-----+-----------------+--------------+------------+----------+------------+---------+--------+-----------+------+-------------+----------+---------------------+----------------+--------+---------------+------------+------------------+-----------+---------+-----------+--------+--------+---------------+-----------------------+-----------------+-------------+---------------+------------------------+---------------+-------------+-----------------+----------+--------------+--------+----------------+-------------+----------------+---------------+---------------+------------------+-----------------+------------+-----------------+-----+--------+-----------------+-------------+---------------+--------------+-------------+-------+-----------+-------------------+-------------------+---------------+--------------+----------+-----+----------------------+---------------------+----------------+---------------+---------+----------------+----------+----------+------------+---------------------------+-------------+------------+------------------+-----+-------------+-------+--------------+--------------+-------+--------------+------+-------------------+-----------+---------------+-----------------+--------------+-----------------------+------------------+----+------------+--------------------+---------+--------+----------------+---+--------+----------------+----------+---+----------+-----------+--------------------+------------+--------------------+-------+-------------+-----------------------+-----+---------+-------------+---------+-------------+------+--------------------+--------+----------------+-------------------+-----+--------------------+---------------+------+-------------------+-------+-------------+----------------+----------------+--------------+----------------------+-------------------+----------+------------------------+------------+--------------------+----------+------+-------------------+------+---------------------+--------------+--------------+--------------+---------+--------------------+------------+------------+----------------+--------+--------------+----------------+---------------+------------+--------+------------------+-----------------+------------+--------------+----------+--------+--------------------+---------------+--------+--------------------+-------------+-----------+------------------+-------------------+------------+-------------+--------+---------+-----------+----------+-------------+-----------------+-----------------+---+-----------------+------------+----------------+--------+--------------------+--------+--------------+----------------+---------+----------+--------------------+---------------+------------+--------------+------------+---------------+------------+---------------+-------+----------------------+--------------------+--------------------+--------------------+----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "VoterSentiment_interactive.html#positive-and-negative-counts",
    "href": "VoterSentiment_interactive.html#positive-and-negative-counts",
    "title": "Sentiment Analysis",
    "section": "Positive and negative counts",
    "text": "Positive and negative counts\n\n\nCode\nspark\nimport matplotlib.ticker as ticker\nfrom azureml.core import Workspace, Datastore, Dataset\nimport matplotlib.pyplot as plt\n\nsubscription_id = 'My_ID'\nresource_group = 'project-group-11'\nworkspace_name = 'project-group-11'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndatastore = Datastore.get(workspace, \"workspaceblobstore\")\n\ndataset_submissions = spark.read.parquet(\"azureml://datastores/workspaceblobstore/paths/cleandata/submissions_w_keywords_and_sentiment.parquet\")\ndataset_comments = spark.read.parquet(\"azureml://datastores/workspaceblobstore/paths/cleandata/comments_w_keywords_and_sentiment.parquet\")\n\nsentiment_counts_submissions = dataset_submissions.groupBy(\"simple_sentiment\").count()\nsentiment_counts_comments = dataset_comments.groupBy(\"simple_sentiment\").count()\n\npd_sentiment_counts_submissions = sentiment_counts_submissions.toPandas()\npd_sentiment_counts_comments = sentiment_counts_comments.toPandas()\n\n# Create fig\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 9))\n\n# Plot for submissions\naxes[0].bar(pd_sentiment_counts_submissions['simple_sentiment'], pd_sentiment_counts_submissions['count'], color='#EC9A29', tick_label=pd_sentiment_counts_submissions['simple_sentiment'])\naxes[0].set_title('Sentiment Counts in Submissions')\naxes[0].set_xlabel(' ')\naxes[0].set_ylabel('Count in Millions')\naxes[0].spines['top'].set_visible(False)\naxes[0].spines['right'].set_visible(False)\nformatter = ticker.FuncFormatter(lambda x, pos: f'{int(x/1e6)}M')\naxes[0].yaxis.set_major_formatter(formatter)\n\n# Plot for comments\naxes[1].bar(pd_sentiment_counts_comments['simple_sentiment'], pd_sentiment_counts_comments['count'], color='#00AEF3', tick_label=pd_sentiment_counts_comments['simple_sentiment'])\naxes[1].set_title('Sentiment Counts in Comments')\naxes[1].set_xlabel(' ')\naxes[1].set_ylabel(' ')\naxes[1].spines['top'].set_visible(False)\naxes[1].spines['right'].set_visible(False)\nformatter = ticker.FuncFormatter(lambda x, pos: f'{int(x/1e6)}M')\naxes[1].yaxis.set_major_formatter(formatter)\n\n# Show plot\nplt.tight_layout()\nplt.show()\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 82, 17, Finished, Available, Finished)"
  },
  {
    "objectID": "Data_cleaning.html",
    "href": "Data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This tab aims to clean the data sets that will be used in all the others. The goal is to have our data sets ready to be used in the analysis. Before doing so, we will explain some common processes that are done or checked by most data scientists. However, it is worth mentioning that data cleaning is not a linear process, and it is not always possible to follow a specific order. In fact, it is common to go back and forth between the different steps.\n\n\nThe first most common step is to import and visualize how the data looks like. This is shown already in our data gathering tab by displaying the top 4 or 5 rows. It is also common to display the number of columns and rows that the data has to give us a sense of the size of the data set. In addition, it is important to check the names the data types of each column in order to see whether it will be beneficial to change them for your use (depending on the type of analysis you will do).\n\n\n\nDealing with missing values can be complicated. There is not a right or wrong way to do it. However, the goal should be to deal with them in a way that the data-set integrity is mantained while being able to properly perform and assess the analysis intended. The most important thing is to record the process of dealing with missing values and to explain why you chose to do it in that way. This is important because it will allow others to understand your process and to replicate it if needed while also reporting the limitations of your analysis due to those missing values.\n\n\n\nAnother common practice is to deal with outliers. If you detect outliers, you should decide whether to keep them or not. This depends on the type of analysis that you do and will also depend on the type of data that you have.\n\n\n\nIt is also important to check for duplicates. You do not want to count a data point twice just because of an administrative error. If you do so, you will give more importance to that data point than it should have.\n\n\n\nFor these three data sets, we will cheack for NaN values and select the columns (variables) that we are going to use. These data sets do not hold duplicates. We want to keep outliers for our tests due to the fact that they may be important for our analysis. However, in each tab, this will be crosschecked. Additiopnally, there are no diuplicates in our data sets."
  },
  {
    "objectID": "Data_cleaning.html#data-cleaning",
    "href": "Data_cleaning.html#data-cleaning",
    "title": "Data Cleaning",
    "section": "",
    "text": "This tab aims to clean the data sets that will be used in all the others. The goal is to have our data sets ready to be used in the analysis. Before doing so, we will explain some common processes that are done or checked by most data scientists. However, it is worth mentioning that data cleaning is not a linear process, and it is not always possible to follow a specific order. In fact, it is common to go back and forth between the different steps.\n\n\nThe first most common step is to import and visualize how the data looks like. This is shown already in our data gathering tab by displaying the top 4 or 5 rows. It is also common to display the number of columns and rows that the data has to give us a sense of the size of the data set. In addition, it is important to check the names the data types of each column in order to see whether it will be beneficial to change them for your use (depending on the type of analysis you will do).\n\n\n\nDealing with missing values can be complicated. There is not a right or wrong way to do it. However, the goal should be to deal with them in a way that the data-set integrity is mantained while being able to properly perform and assess the analysis intended. The most important thing is to record the process of dealing with missing values and to explain why you chose to do it in that way. This is important because it will allow others to understand your process and to replicate it if needed while also reporting the limitations of your analysis due to those missing values.\n\n\n\nAnother common practice is to deal with outliers. If you detect outliers, you should decide whether to keep them or not. This depends on the type of analysis that you do and will also depend on the type of data that you have.\n\n\n\nIt is also important to check for duplicates. You do not want to count a data point twice just because of an administrative error. If you do so, you will give more importance to that data point than it should have.\n\n\n\nFor these three data sets, we will cheack for NaN values and select the columns (variables) that we are going to use. These data sets do not hold duplicates. We want to keep outliers for our tests due to the fact that they may be important for our analysis. However, in each tab, this will be crosschecked. Additiopnally, there are no diuplicates in our data sets."
  },
  {
    "objectID": "Data_cleaning.html#information-about-us-census-blocks-dataset",
    "href": "Data_cleaning.html#information-about-us-census-blocks-dataset",
    "title": "Data Cleaning",
    "section": "Information about US census blocks dataset",
    "text": "Information about US census blocks dataset\nThe followin code attempts to clean the EPA_SmartLocationDatabase_V3_Jan_2021_Final.csv file. It keeps the columns that will be relevant for the analysis and clustering of the census block data of the US cities. The columns excluded are all the administrative variables, the individualistic columns such as IDs that would not allow us to perform clustering properly, and other columns that are not relevant for the analysis (all columns droped are found under the comment “# Drop unwanted columns” in the code chunk, and their definitions inside EPA pdf inside the data folder).\nAll variables have been checked and there are no missing values that would affect our study.\n\n\nCode\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Final.csv'\n\ndf = pd.read_csv(file_path)\n\n# Drop unwanted columns\ndf.drop(columns=[\"OBJECTID\", \"GEOID10\", \"GEOID20\", \"STATEFP\", \"COUNTYFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\", \"CSA\", \"CSA_Name\", \"CBSA\", \"CBSA_Name\", \"CountHU\", \"HH\", \"D1A\", \"D1C8_OFF\", \"D4D\", \"D4E\", \"D2A_JPHH\", \"D2B_E5MIX\", \"D2B_E5MIXA\", \"D2B_E8MIX\",\"D2B_E8MIXA\",\"D2A_EPHHM\",\"D2C_TRPMX1\",\"D2C_TRPMX2\",\"D2C_TRIPEQ\",\"D2R_JOBPOP\",\"D2R_WRKEMP\",\"D2A_WRKEMP\",\"D2C_WREMLX\",\"D4A\",\"D4B025\",\"D4B050\",\"D4C\",\"D5AR\",\"D5AE\",\"D5BR\",\"D5BE\",\"D5CR\",\"D5CRI\",\"D5CE\",\"D5CEI\",\"D5DR\",\"D5DRI\",\"D5DE\",\"D5DEI\"], inplace=True)\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nprint(df.isna().sum())\n\ndf.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv', index=False)"
  },
  {
    "objectID": "Data_cleaning.html#text-data",
    "href": "Data_cleaning.html#text-data",
    "title": "Data Cleaning",
    "section": "Text data",
    "text": "Text data\nThe following code cleans our text data file top_pub_transp_urls.json. It removes the escape sequences, non-alphabetic characters, the extra white spaces and strips the text. Then, it saves it in the clean data folder as cleaned_text_data.json (it can be found inside the reddit data on the raw data folder, but also inside the clean data folder).\nNote: the sentiment analysis was already extracted and saved in the data tab.\n\n\nCode\nimport json\nimport re\n\nwith open('top_pub_transp_urls.json', 'r') as json_file:\n    data = json.load(json_file)\n\ndef clean_text(text):\n    cleaned_text = re.sub(r'\\\\u....', '', text) \n    cleaned_text = re.sub(r'[^A-Za-z\\s]', ' ', cleaned_text) \n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text) \n    cleaned_text = cleaned_text.strip() \n    return cleaned_text\n\ncleaned_text_data = []\n\nfor item in data:\n    cleaned_text = clean_text(item.get(\"text\", \"\"))\n    cleaned_text_data.append({\"url\": item[\"url\"], \"cleaned_text\": cleaned_text})\n\nwith open('cleaned_text_data.json', 'w') as json_file:\n    json.dump(cleaned_text_data, json_file, indent=4)\n\n#print(\"Text data cleaned and saved to cleaned_text_data.json.\")"
  },
  {
    "objectID": "Data_cleaning.html#fuel-and-energy-data",
    "href": "Data_cleaning.html#fuel-and-energy-data",
    "title": "Data Cleaning",
    "section": "Fuel and Energy data",
    "text": "Fuel and Energy data\nThe following code selects the columns that are relevant for the analysis and drops the rest. Since this data set will be used for modeling purposes, the redundant columns that were directly connected to the target variable have been dropped (in this case, since we will focuss on predicting avg miles per galon of fossil fuels, all other information of usage of fossil fuels is being dropped). It also creates a new column that averages the mpg between diesel and gasoline, and then drops all the rows that have missing values in this new (since they will not be able to be used for our modeling purposes). The final data set is saved in the clean data folder as 2021_Fuel_and_Energy_mpg_fossil_fuels.csv.\nNote: the columns kept are: City, Agency, Organization Type, Primary UZA Population, Mode, TOS, Electric Battery, and the new column that averages the mpg between diesel and gasoline (called avg_mpg_fossil_fuels).\n\n\nCode\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/2021_Fuel_and Energy.xlsm'\n\ndf = pd.read_excel(file_path, sheet_name='Fuel and Energy')\n\ncolumns_to_keep = ['City', 'Agency', 'Organization Type', 'Primary UZA Population', 'Mode', 'TOS', 'Electric Battery', 'Diesel (mpg)', 'Gasoline (mpg)']\n\n# Keep only the specified columns\ndf_filtered = df[columns_to_keep]\n\ndf_filtered['avg_mpg_fossil_fuels'] = df_filtered[['Diesel (mpg)', 'Gasoline (mpg)']].mean(axis=1, skipna=True)\n\ndf_filtered = df_filtered.dropna(subset=['avg_mpg_fossil_fuels'])\n\ndf_filtered = df_filtered.drop(['Diesel (mpg)', 'Gasoline (mpg)'], axis=1)\n\ndf_filtered.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/2021_Fuel_and_Energy_mpg_fossil_fuels.csv', index=False)"
  },
  {
    "objectID": "Data_cleaning.html#border-crossing-entry-data",
    "href": "Data_cleaning.html#border-crossing-entry-data",
    "title": "Data Cleaning",
    "section": "Border Crossing Entry Data",
    "text": "Border Crossing Entry Data\nThe following code cleans the data of Border_Crossing_Entry_Data_20231103.csv (found in the raw data folder). Since this data set will be used for modeling purposes (in the Naive Bayes tab), we will use the columns: Value, State, Measure. The cleaning code will only select this columns and save them into a new csv called Clean_Border_Crossing_Entry_Data_20231103.csv in the clean data folder.\nNote: NaN values have been checked and there are none\n\n\nCode\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/Border_Crossing_Entry_Data_20231103.csv'\ndf = pd.read_csv(file_path)\n\ncolumns_to_keep = [\"Value\", \"State\", \"Measure\"]\n\ndf=df[columns_to_keep]\n\ndf.isna().sum()\n\ndf.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/Clean_Border_Crossing_Entry_Data_20231103.csv', index=False)"
  },
  {
    "objectID": "RNN_text_gen.html",
    "href": "RNN_text_gen.html",
    "title": "Hw 4: RNNs",
    "section": "",
    "text": "Recurrent Neural Networks\n\n\nWrite code for a character-based RNN in PyTorch. You can use his source code and any code online to assist. If you do, just reference where you sought help from.\n\nChoose a large text corpus, such as a collection of novels from Project Gutenburg. You can use other data, but you should explain where your data comes from.\n\nText Data: I will use the Harry Potter series by J.K. Rowling. We obtained the seven novels during our bootcamp.\nINSERT ANSWER HERE\n\nPerform any necessary preprocessing, explaining what steps you take. In particular, what forms of normalization do you use? Do you define characters with special meaning?\n\n\n\nCode\n# INSERT CODE\n\n# import libraries\nimport os\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import silhouette_samples\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom tqdm import tqdm\n\n\nHere, we first take out chapter titles, pages’ number information, skipping empty lines, and we remove double white spaces to make it single ones. Then, we combine all books into a single string and save it.\n\n\nCode\n# function to identify titles\ndef Chapter_title_identificator(previous_line, current_line, next_line):\n    if previous_line.strip() == '' and next_line.strip() == '':\n        if current_line.isupper():\n            return True\n    return False\n\n# load and preprocess data\ndef data_preprocessor(file_paths):\n    all_text = []\n    # utf-8\n    for file_path in file_paths:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            lines = file.readlines()\n\n        # Filter out lines containing 'Page |'\n        lines = [line for line in lines if \"Page |\" not in line]\n\n        # store lines\n        processed_lines = []\n        num_lines = len(lines)\n\n        # Iterate through lines\n        i = 0\n        while i &lt; num_lines:\n            # avoid boundaries for titles (empty lines before and after title line)\n            if i &gt; 0 and i &lt; num_lines - 1:\n                if Chapter_title_identificator(lines[i-1], lines[i], lines[i+1]):\n                    # Skip only chapter title line\n                    i += 1\n                    continue\n            # Skip lines that start with '/' \n            if not lines[i].startswith('/'):\n                # Skip empty lines\n                if lines[i].strip(): \n                    processed_lines.append(lines[i])\n            i += 1\n\n        # put lines into a single string\n        text = \"\".join(processed_lines)\n\n        # Removing double spaces\n        text = ' '.join(text.split())\n\n        all_text.append(text)\n\n    # All text to one string\n    full_text = \" \".join(all_text)\n\n    return full_text\n\n# Paths\nfile_paths = [f\"./corpus/hp_{i}.txt\" for i in range(1, 8)]\n\n# Preprocess novels\nprocessed_text = data_preprocessor(file_paths)\nprint(processed_text[:0])\n\n# Save file\nwith open('processed_text.txt', 'w', encoding='utf-8') as file:\n    file.write(processed_text)\n\n\n\n\n\nNow, we are keeping all the words and only selected symbols that can carry meaning: whitespace, dot, coma, interrogation and exclamation marks, colon and semi-colon, and dash. We are also making all lower case for better interpretation. The reason behind all these choices is that we want to mantain as much information as possible while not utilizing unuseful tokens in training. Then we preprocess the text and we turn characters into integers:\n\n\nCode\n# read preprocessed text\nwith open('processed_text.txt', 'r', encoding='utf-8') as file:\n    text = file.read()\n\n# make it lower-case\ntext = text.lower()\n\n# remove all not common alphanumeric characters\nallowed_chars = string.ascii_letters + string.digits + \" .,!?;:-'\" \ntext = \"\".join([c if c in allowed_chars else \" \" for c in text])\n\n# Characters to integers and vice versa\nvocab = sorted(set(text))\ncharacter_to_index = {c: i for i, c in enumerate(vocab)}\nindex_to_character = {i: c for i, c in enumerate(vocab)}\n\n# Convert the text to integers\ntext_as_integer = [character_to_index[c] for c in text]\n\n\n\nEfficiently load and batch the dataset for training using a DataLoader. Make sure to reserve some of the data for validation and testing. Describe how you handle batching and sequence lengths.\n\nHere, we are making the sequences of length 500, so they are long enough to carry meaning while not too large to process. We are batching in 64 sequences. We are also splitting the data into training, validation, and test sets.\n\n\nCode\n# INSERT CODE\n\n# lists to store sequences and targets\nsequences = []\ntargets = []\n\n# sequences\nsequence_length = 500\nsamp_per_epoch = len(text) // (sequence_length + 1)\n\n# batch\nbatch_count = 64\n\n# dataset\ncharacter_tensor = torch.tensor(text_as_integer, dtype=torch.long)\n\n# create sequences and targets\nfor i in range(0, len(character_tensor) - sequence_length, sequence_length):\n    sequences.append(character_tensor[i:i+sequence_length])\n    targets.append(character_tensor[i+1:i+sequence_length+1])\n\n# create dataset as tensor\ndataset = TensorDataset(torch.stack(sequences), torch.stack(targets))\n\n# split into train, val, test\nsize_train = int(0.8 * len(dataset))\nsize_val = int(0.2 * len(dataset))\nsize_test = len(dataset) - size_train - size_val\n\ntrain_data, val_data, test_data = random_split(dataset, [size_train, size_val, size_test])\n\n# dataloader\ntrain_dataloader = DataLoader(train_data, batch_size=batch_count, shuffle=True)\nval_dataloader = DataLoader(val_data, batch_size=batch_count)\ntest_dataloader = DataLoader(test_data, batch_size=batch_count)\n\n\n\nDefine your RNN model. Discuss the number of layers, hidden units, and the type of RNN cells you use. What is the total number of parameters in your model? Explain the rationale behind your architectural choices.\n\nThe architecture is printed below. The reason to use LSTM cell is that they are better to prevent the vanishing gradient problem. We are also using 2 layers as more will take very long to train in my computer and the size of our corpus is not that big (so we do not want to overfit). I have also chosen 256 hidden units as it seems a good number based on the previous size of layers chosen. I have also used 128 embedding dimension as when trying to increase it my kernel was dying (so it was the maximum I could use).\n\n\nCode\n# RNN \nclass RNN(nn.Module):\n    # Initialize model\n    def __init__(self, size_vocabulary, embedding, hidden_unit_count, layer_count):\n        # Call parent function\n        super(RNN, self).__init__()\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(size_vocabulary, embedding)\n        \n        # RNN layer\n        self.rnn = nn.LSTM(embedding, hidden_unit_count, layer_count, batch_first=True)\n        \n        # output\n        self.fc = nn.Linear(hidden_unit_count, size_vocabulary)\n\n    # Forward pass\n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.rnn(x)\n        x = self.fc(x)\n        # return output\n        return x\n\n\n\n\nCode\n# hyperparameters\nsize_vocabulary = len(vocab)\nnum_layers = 2\nhidden_dimension = 256\nembedding_dimension = 128\n\n# model\nmodel = RNN(size_vocabulary, embedding_dimension, hidden_dimension, num_layers)\n\n# parameter count\ndef parameter_counter(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# count parameters\nparam_count = parameter_counter(model)\n\n# print model architecture\ndisplay(model)\n\n# print number of parameters\nprint(f'Total number of parameters: {param_count}')\n\n\nRNN(\n  (embedding): Embedding(45, 128)\n  (rnn): LSTM(128, 256, num_layers=2, batch_first=True)\n  (fc): Linear(in_features=256, out_features=45, bias=True)\n)\n\n\nTotal number of parameters: 938925\n\n\n\nWrite the training loop.\n\n\n\nCode\n# Parameters\ntotal_epochs = 10\nlr = 0.01\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\n# lists to store losses\ntrain_loss = []\nval_loss = []\n\n# Training loop\nfor epoch in range(total_epochs):\n    \n    # Training\n    model.train()\n\n    # initialize total training loss\n    total_train_loss = 0.0\n    \n    # Training loop\n    train_loop = tqdm(enumerate(train_dataloader), total=len(train_dataloader), leave=False)\n\n    for batch_index, (data, target) in train_loop:\n        optimizer.zero_grad()\n        output = model(data)\n        output = output.permute(0, 2, 1)\n        \n        # calculate loss\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        # add loss to total training loss\n        total_train_loss += loss.item()\n        # update progress bar\n        train_loop.set_description(f\"Epoch [{epoch+1}/{total_epochs}]\")\n        train_loop.set_postfix(loss=loss.item())\n    \n    # avg training loss and perplexity per epoch\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    train_loss.append(avg_train_loss)\n    train_perplexity = torch.exp(torch.tensor(avg_train_loss)).item()\n    \n    # Validation initialization\n    model.eval()\n    total_val_loss = 0.0\n    val_loop = tqdm(enumerate(val_dataloader), total=len(val_dataloader), leave=False)\n    \n    # Validation\n    with torch.no_grad():\n        for batch_index, (data, target) in val_loop:\n            output = model(data)\n            output = output.permute(0, 2, 1)\n            # Calculate loss\n            loss = criterion(output, target)\n            total_val_loss += loss.item()\n            # Update progress bar\n            val_loop.set_description(f\"Validation Epoch [{epoch+1}/{total_epochs}]\")\n            val_loop.set_postfix(val_loss=loss.item())\n    \n    # Avg validation loss and perplexity per epoch\n    avg_val_loss = total_val_loss / len(val_dataloader)\n    val_loss.append(avg_val_loss)\n    val_perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n    \n    # Epoch end summary\n    print(f\"Epoch {epoch+1}/{total_epochs}, Train Loss: {avg_train_loss:.4f}, Train Perplexity: {train_perplexity:.4f}, Val Loss: {avg_val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}\")\n\n\n                                                                                       \n\n\nEpoch 1/10, Train Loss: 1.7381, Train Perplexity: 5.6867, Val Loss: 1.4618, Val Perplexity: 4.3136\n\n\n                                                                                       \n\n\nEpoch 2/10, Train Loss: 1.3720, Train Perplexity: 3.9432, Val Loss: 1.3172, Val Perplexity: 3.7328\n\n\n                                                                                       \n\n\nEpoch 3/10, Train Loss: 1.2841, Train Perplexity: 3.6114, Val Loss: 1.2641, Val Perplexity: 3.5400\n\n\n                                                                                       \n\n\nEpoch 4/10, Train Loss: 1.2399, Train Perplexity: 3.4552, Val Loss: 1.2320, Val Perplexity: 3.4281\n\n\n                                                                                       \n\n\nEpoch 5/10, Train Loss: 1.2116, Train Perplexity: 3.3588, Val Loss: 1.2111, Val Perplexity: 3.3573\n\n\n                                                                                       \n\n\nEpoch 6/10, Train Loss: 1.1901, Train Perplexity: 3.2873, Val Loss: 1.1940, Val Perplexity: 3.3001\n\n\n                                                                                       \n\n\nEpoch 7/10, Train Loss: 1.1743, Train Perplexity: 3.2357, Val Loss: 1.1853, Val Perplexity: 3.2715\n\n\n                                                                                       \n\n\nEpoch 8/10, Train Loss: 1.1625, Train Perplexity: 3.1979, Val Loss: 1.1719, Val Perplexity: 3.2282\n\n\n                                                                                       \n\n\nEpoch 9/10, Train Loss: 1.1503, Train Perplexity: 3.1591, Val Loss: 1.1622, Val Perplexity: 3.1969\n\n\n                                                                                        \n\n\nEpoch 10/10, Train Loss: 1.1418, Train Perplexity: 3.1323, Val Loss: 1.1567, Val Perplexity: 3.1794\n\n\n\n\n\n\nMonitor and report on the training progress by tracking the loss. After training, evaluate the model’s performance using a suitable evaluation metric (e.g., perplexity) on a validation dataset or a held-out portion of the training data. Discuss the results.\n\nThis was done above (as I run the training-loop right after defining it). Throughout the ten epochs trained, the loss has gotten better for both, validation and training, and even more notizable for the perplexity. However, it seems that for the last three epochs the improvement hasn’t been too large, it seems that it was settling down. The training loss seems decently low for both sets, and similar values which indicates that there shouldn’t be much overfitting. Moreover, the perplexity\n\nSpecify the hyper-parameters (for example, model hyper-parameters, as well as sampling size and beam width from below) used in your model. Find suitable settings using a validation split.\n\nI have used the following hyperparameters:\n\nNumber of epochs: 10 (more epochs would take too long to train and it seems that the model is already settling down)\nLearning rate: 0.01 (it seems to be a good value for the model, and smaller will take too long for my laptop to train)\nBatch size: 64 (it seems to be a good value for the model)\n\nNotes from before:\n\nSequence length: 500\nNumber of hidden units: 256\nNumber of layers: 2\nEmbedding dimension: 128\nImplement a text generation function using the trained RNN model. Provide a prompt or seed text, and use the RNN to generate a sequence of characters. Experiment with different prompt texts and observe how the generated text changes. Discuss any interesting patterns or observations you make during the text generation process.\n\nHere we created the function and ran it for some random prompts I came up with (some Harry Potter related to test if it would work better for these).\n\n\nCode\n# INSERT CODE\n\n# function to generate text\ndef text_generator(model, start_sentence, length, temperature=1.0):\n\n    # evaluation mode\n    model.eval()\n\n    # convert start sentence to tensor\n    input_eval = [character_to_index[s] for s in start_sentence]\n    input_eval = torch.tensor(input_eval, dtype=torch.long).unsqueeze(0)\n\n    # generated text list\n    generated_text = []\n\n    with torch.no_grad():\n        \n        # loop through the length specified\n        for i in range(length):\n\n            # get the output\n            output = model(input_eval)\n            output = output[:, -1, :] / temperature  \n            probabilities = nn.functional.softmax(output, dim=-1)\n            \n            # sample the next character\n            predicted_id = torch.multinomial(probabilities, num_samples=1)\n            \n            # add the predicted character to the generated text\n            input_eval = torch.cat([input_eval, predicted_id], dim=1)\n\n            # convert the predicted character to a string and append to the generated text\n            generated_text.append(index_to_character[predicted_id.item()])\n\n    # return text\n    return start_sentence + ''.join(generated_text)\n\n# Use the funtion to generate text\n# My Prompts\nprompts = [\"is it\", \"harry potter and\", \"yesterday i went to\", \"harry, listen to me\", \"once upon a time\", \"the end of the world is near\", \"harry, i want\", \"ron and i\", \"the dark lord\", \"i am a wizard\", \"my dear harry\", \"the forbidden forest\", \"the master of\", \"only if you\", \"the ball was\", \"make me some\"]\n\n# generate text for each prompt\nfor prompt in prompts:\n    print(f\"prompt: {prompt}\")\n    print(text_generator(model, prompt, 100))\n    print('-' * 100)\n\n\nprompt: is it\nis it. he could, i used sirius was not without spotty grafficance much fifty- water if harry and harry wa\n----------------------------------------------------------------------------------------------------\nprompt: harry potter and\nharry potter and the walls, portrait metsier across the unucularly curtains?   good loudly,  said sirius and who did\n----------------------------------------------------------------------------------------------------\nprompt: yesterday i went to\nyesterday i went to get thousand over the magoss us anymore, but your mother much, ever insisting red leg in the worlin\n----------------------------------------------------------------------------------------------------\nprompt: harry, listen to me\nharry, listen to me!  said ron s face cleaning explosing and pretending from a moment where all heavencing when she cro\n----------------------------------------------------------------------------------------------------\nprompt: once upon a time\nonce upon a time he s cart the fact that each rose, a leamed as she with. nothing during the dares of a starting dow\n----------------------------------------------------------------------------------------------------\nprompt: the end of the world is near\nthe end of the world is nearly git.  i have thought the minister plimpy to see a wet?  he rolled aftermined? who had lauded and \n----------------------------------------------------------------------------------------------------\nprompt: harry, i want\nharry, i want to five,  said harry, still deep the knock him.  and they!  the school strile, to crashed night pro\n----------------------------------------------------------------------------------------------------\nprompt: ron and i\nron and it had been killed through the first time forward to be stay-new, and mother kept his side,  they kne\n----------------------------------------------------------------------------------------------------\nprompt: the dark lord\nthe dark lord to enjoy us possibly,  said voldemort.  below kreacher,  said harry memberteenth your father inside\n----------------------------------------------------------------------------------------------------\nprompt: i am a wizard\ni am a wizardan. but i knew that turn. i just go to his care   does this, i ve been back the rightful, bun,  said\n----------------------------------------------------------------------------------------------------\nprompt: my dear harry\nmy dear harry s shoulder; cho sselver within hagrid would pass off an  detention,  said harry hissing grunting up\n----------------------------------------------------------------------------------------------------\nprompt: the forbidden forest\nthe forbidden forest, scrambled, and their things ever, when he ran along would have been a horror, damp her, followed h\n----------------------------------------------------------------------------------------------------\nprompt: the master of\nthe master of somebody on the story; vermonituants he didn t let snape found helping themselves woken the bag eru\n----------------------------------------------------------------------------------------------------\nprompt: only if you\nonly if you must have emals.  in during doge in its indesh all three pile up the others, we master has a curses\n----------------------------------------------------------------------------------------------------\nprompt: the ball was\nthe ball was backward a love the alicia was this. krum has amartments and chairing, and came sent perhaps danger\n----------------------------------------------------------------------------------------------------\nprompt: make me some\nmake me somebody else of ministry, ron.   the end of the wizards i always thought that were bringing out to real\n----------------------------------------------------------------------------------------------------\n\n\nWe see that while some words make sense, the longer you go in the sentence, the less sense it makes. This is because the model is not able to understand the context of the words, and it is just predicting the next character based on the previous ones. This is a common problem with character-based RNNs. However, it is interesting to see the model generating some Harry Potter related content in some scenarios. Some sentences (the begginings since the endings stop makling sense) seem to be taken straight from the books, while others are just random words. An example of this is when I prompted “Harry, listen to me” and the model mentioned Ron, or “make me some” and it mentions ron again. I just found it interesting how much the model is affected by the training data."
  },
  {
    "objectID": "RNN_text_gen.html#problem-3",
    "href": "RNN_text_gen.html#problem-3",
    "title": "Hw 4: RNNs",
    "section": "",
    "text": "Write code for a character-based RNN in PyTorch. You can use his source code and any code online to assist. If you do, just reference where you sought help from.\n\nChoose a large text corpus, such as a collection of novels from Project Gutenburg. You can use other data, but you should explain where your data comes from.\n\nText Data: I will use the Harry Potter series by J.K. Rowling. We obtained the seven novels during our bootcamp.\nINSERT ANSWER HERE\n\nPerform any necessary preprocessing, explaining what steps you take. In particular, what forms of normalization do you use? Do you define characters with special meaning?\n\n\n\nCode\n# INSERT CODE\n\n# import libraries\nimport os\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import silhouette_samples\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom tqdm import tqdm\n\n\nHere, we first take out chapter titles, pages’ number information, skipping empty lines, and we remove double white spaces to make it single ones. Then, we combine all books into a single string and save it.\n\n\nCode\n# function to identify titles\ndef Chapter_title_identificator(previous_line, current_line, next_line):\n    if previous_line.strip() == '' and next_line.strip() == '':\n        if current_line.isupper():\n            return True\n    return False\n\n# load and preprocess data\ndef data_preprocessor(file_paths):\n    all_text = []\n    # utf-8\n    for file_path in file_paths:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            lines = file.readlines()\n\n        # Filter out lines containing 'Page |'\n        lines = [line for line in lines if \"Page |\" not in line]\n\n        # store lines\n        processed_lines = []\n        num_lines = len(lines)\n\n        # Iterate through lines\n        i = 0\n        while i &lt; num_lines:\n            # avoid boundaries for titles (empty lines before and after title line)\n            if i &gt; 0 and i &lt; num_lines - 1:\n                if Chapter_title_identificator(lines[i-1], lines[i], lines[i+1]):\n                    # Skip only chapter title line\n                    i += 1\n                    continue\n            # Skip lines that start with '/' \n            if not lines[i].startswith('/'):\n                # Skip empty lines\n                if lines[i].strip(): \n                    processed_lines.append(lines[i])\n            i += 1\n\n        # put lines into a single string\n        text = \"\".join(processed_lines)\n\n        # Removing double spaces\n        text = ' '.join(text.split())\n\n        all_text.append(text)\n\n    # All text to one string\n    full_text = \" \".join(all_text)\n\n    return full_text\n\n# Paths\nfile_paths = [f\"./corpus/hp_{i}.txt\" for i in range(1, 8)]\n\n# Preprocess novels\nprocessed_text = data_preprocessor(file_paths)\nprint(processed_text[:0])\n\n# Save file\nwith open('processed_text.txt', 'w', encoding='utf-8') as file:\n    file.write(processed_text)\n\n\n\n\n\nNow, we are keeping all the words and only selected symbols that can carry meaning: whitespace, dot, coma, interrogation and exclamation marks, colon and semi-colon, and dash. We are also making all lower case for better interpretation. The reason behind all these choices is that we want to mantain as much information as possible while not utilizing unuseful tokens in training. Then we preprocess the text and we turn characters into integers:\n\n\nCode\n# read preprocessed text\nwith open('processed_text.txt', 'r', encoding='utf-8') as file:\n    text = file.read()\n\n# make it lower-case\ntext = text.lower()\n\n# remove all not common alphanumeric characters\nallowed_chars = string.ascii_letters + string.digits + \" .,!?;:-'\" \ntext = \"\".join([c if c in allowed_chars else \" \" for c in text])\n\n# Characters to integers and vice versa\nvocab = sorted(set(text))\ncharacter_to_index = {c: i for i, c in enumerate(vocab)}\nindex_to_character = {i: c for i, c in enumerate(vocab)}\n\n# Convert the text to integers\ntext_as_integer = [character_to_index[c] for c in text]\n\n\n\nEfficiently load and batch the dataset for training using a DataLoader. Make sure to reserve some of the data for validation and testing. Describe how you handle batching and sequence lengths.\n\nHere, we are making the sequences of length 500, so they are long enough to carry meaning while not too large to process. We are batching in 64 sequences. We are also splitting the data into training, validation, and test sets.\n\n\nCode\n# INSERT CODE\n\n# lists to store sequences and targets\nsequences = []\ntargets = []\n\n# sequences\nsequence_length = 500\nsamp_per_epoch = len(text) // (sequence_length + 1)\n\n# batch\nbatch_count = 64\n\n# dataset\ncharacter_tensor = torch.tensor(text_as_integer, dtype=torch.long)\n\n# create sequences and targets\nfor i in range(0, len(character_tensor) - sequence_length, sequence_length):\n    sequences.append(character_tensor[i:i+sequence_length])\n    targets.append(character_tensor[i+1:i+sequence_length+1])\n\n# create dataset as tensor\ndataset = TensorDataset(torch.stack(sequences), torch.stack(targets))\n\n# split into train, val, test\nsize_train = int(0.8 * len(dataset))\nsize_val = int(0.2 * len(dataset))\nsize_test = len(dataset) - size_train - size_val\n\ntrain_data, val_data, test_data = random_split(dataset, [size_train, size_val, size_test])\n\n# dataloader\ntrain_dataloader = DataLoader(train_data, batch_size=batch_count, shuffle=True)\nval_dataloader = DataLoader(val_data, batch_size=batch_count)\ntest_dataloader = DataLoader(test_data, batch_size=batch_count)\n\n\n\nDefine your RNN model. Discuss the number of layers, hidden units, and the type of RNN cells you use. What is the total number of parameters in your model? Explain the rationale behind your architectural choices.\n\nThe architecture is printed below. The reason to use LSTM cell is that they are better to prevent the vanishing gradient problem. We are also using 2 layers as more will take very long to train in my computer and the size of our corpus is not that big (so we do not want to overfit). I have also chosen 256 hidden units as it seems a good number based on the previous size of layers chosen. I have also used 128 embedding dimension as when trying to increase it my kernel was dying (so it was the maximum I could use).\n\n\nCode\n# RNN \nclass RNN(nn.Module):\n    # Initialize model\n    def __init__(self, size_vocabulary, embedding, hidden_unit_count, layer_count):\n        # Call parent function\n        super(RNN, self).__init__()\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(size_vocabulary, embedding)\n        \n        # RNN layer\n        self.rnn = nn.LSTM(embedding, hidden_unit_count, layer_count, batch_first=True)\n        \n        # output\n        self.fc = nn.Linear(hidden_unit_count, size_vocabulary)\n\n    # Forward pass\n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.rnn(x)\n        x = self.fc(x)\n        # return output\n        return x\n\n\n\n\nCode\n# hyperparameters\nsize_vocabulary = len(vocab)\nnum_layers = 2\nhidden_dimension = 256\nembedding_dimension = 128\n\n# model\nmodel = RNN(size_vocabulary, embedding_dimension, hidden_dimension, num_layers)\n\n# parameter count\ndef parameter_counter(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# count parameters\nparam_count = parameter_counter(model)\n\n# print model architecture\ndisplay(model)\n\n# print number of parameters\nprint(f'Total number of parameters: {param_count}')\n\n\nRNN(\n  (embedding): Embedding(45, 128)\n  (rnn): LSTM(128, 256, num_layers=2, batch_first=True)\n  (fc): Linear(in_features=256, out_features=45, bias=True)\n)\n\n\nTotal number of parameters: 938925\n\n\n\nWrite the training loop.\n\n\n\nCode\n# Parameters\ntotal_epochs = 10\nlr = 0.01\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\n# lists to store losses\ntrain_loss = []\nval_loss = []\n\n# Training loop\nfor epoch in range(total_epochs):\n    \n    # Training\n    model.train()\n\n    # initialize total training loss\n    total_train_loss = 0.0\n    \n    # Training loop\n    train_loop = tqdm(enumerate(train_dataloader), total=len(train_dataloader), leave=False)\n\n    for batch_index, (data, target) in train_loop:\n        optimizer.zero_grad()\n        output = model(data)\n        output = output.permute(0, 2, 1)\n        \n        # calculate loss\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        # add loss to total training loss\n        total_train_loss += loss.item()\n        # update progress bar\n        train_loop.set_description(f\"Epoch [{epoch+1}/{total_epochs}]\")\n        train_loop.set_postfix(loss=loss.item())\n    \n    # avg training loss and perplexity per epoch\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    train_loss.append(avg_train_loss)\n    train_perplexity = torch.exp(torch.tensor(avg_train_loss)).item()\n    \n    # Validation initialization\n    model.eval()\n    total_val_loss = 0.0\n    val_loop = tqdm(enumerate(val_dataloader), total=len(val_dataloader), leave=False)\n    \n    # Validation\n    with torch.no_grad():\n        for batch_index, (data, target) in val_loop:\n            output = model(data)\n            output = output.permute(0, 2, 1)\n            # Calculate loss\n            loss = criterion(output, target)\n            total_val_loss += loss.item()\n            # Update progress bar\n            val_loop.set_description(f\"Validation Epoch [{epoch+1}/{total_epochs}]\")\n            val_loop.set_postfix(val_loss=loss.item())\n    \n    # Avg validation loss and perplexity per epoch\n    avg_val_loss = total_val_loss / len(val_dataloader)\n    val_loss.append(avg_val_loss)\n    val_perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n    \n    # Epoch end summary\n    print(f\"Epoch {epoch+1}/{total_epochs}, Train Loss: {avg_train_loss:.4f}, Train Perplexity: {train_perplexity:.4f}, Val Loss: {avg_val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}\")\n\n\n                                                                                       \n\n\nEpoch 1/10, Train Loss: 1.7381, Train Perplexity: 5.6867, Val Loss: 1.4618, Val Perplexity: 4.3136\n\n\n                                                                                       \n\n\nEpoch 2/10, Train Loss: 1.3720, Train Perplexity: 3.9432, Val Loss: 1.3172, Val Perplexity: 3.7328\n\n\n                                                                                       \n\n\nEpoch 3/10, Train Loss: 1.2841, Train Perplexity: 3.6114, Val Loss: 1.2641, Val Perplexity: 3.5400\n\n\n                                                                                       \n\n\nEpoch 4/10, Train Loss: 1.2399, Train Perplexity: 3.4552, Val Loss: 1.2320, Val Perplexity: 3.4281\n\n\n                                                                                       \n\n\nEpoch 5/10, Train Loss: 1.2116, Train Perplexity: 3.3588, Val Loss: 1.2111, Val Perplexity: 3.3573\n\n\n                                                                                       \n\n\nEpoch 6/10, Train Loss: 1.1901, Train Perplexity: 3.2873, Val Loss: 1.1940, Val Perplexity: 3.3001\n\n\n                                                                                       \n\n\nEpoch 7/10, Train Loss: 1.1743, Train Perplexity: 3.2357, Val Loss: 1.1853, Val Perplexity: 3.2715\n\n\n                                                                                       \n\n\nEpoch 8/10, Train Loss: 1.1625, Train Perplexity: 3.1979, Val Loss: 1.1719, Val Perplexity: 3.2282\n\n\n                                                                                       \n\n\nEpoch 9/10, Train Loss: 1.1503, Train Perplexity: 3.1591, Val Loss: 1.1622, Val Perplexity: 3.1969\n\n\n                                                                                        \n\n\nEpoch 10/10, Train Loss: 1.1418, Train Perplexity: 3.1323, Val Loss: 1.1567, Val Perplexity: 3.1794\n\n\n\n\n\n\nMonitor and report on the training progress by tracking the loss. After training, evaluate the model’s performance using a suitable evaluation metric (e.g., perplexity) on a validation dataset or a held-out portion of the training data. Discuss the results.\n\nThis was done above (as I run the training-loop right after defining it). Throughout the ten epochs trained, the loss has gotten better for both, validation and training, and even more notizable for the perplexity. However, it seems that for the last three epochs the improvement hasn’t been too large, it seems that it was settling down. The training loss seems decently low for both sets, and similar values which indicates that there shouldn’t be much overfitting. Moreover, the perplexity\n\nSpecify the hyper-parameters (for example, model hyper-parameters, as well as sampling size and beam width from below) used in your model. Find suitable settings using a validation split.\n\nI have used the following hyperparameters:\n\nNumber of epochs: 10 (more epochs would take too long to train and it seems that the model is already settling down)\nLearning rate: 0.01 (it seems to be a good value for the model, and smaller will take too long for my laptop to train)\nBatch size: 64 (it seems to be a good value for the model)\n\nNotes from before:\n\nSequence length: 500\nNumber of hidden units: 256\nNumber of layers: 2\nEmbedding dimension: 128\nImplement a text generation function using the trained RNN model. Provide a prompt or seed text, and use the RNN to generate a sequence of characters. Experiment with different prompt texts and observe how the generated text changes. Discuss any interesting patterns or observations you make during the text generation process.\n\nHere we created the function and ran it for some random prompts I came up with (some Harry Potter related to test if it would work better for these).\n\n\nCode\n# INSERT CODE\n\n# function to generate text\ndef text_generator(model, start_sentence, length, temperature=1.0):\n\n    # evaluation mode\n    model.eval()\n\n    # convert start sentence to tensor\n    input_eval = [character_to_index[s] for s in start_sentence]\n    input_eval = torch.tensor(input_eval, dtype=torch.long).unsqueeze(0)\n\n    # generated text list\n    generated_text = []\n\n    with torch.no_grad():\n        \n        # loop through the length specified\n        for i in range(length):\n\n            # get the output\n            output = model(input_eval)\n            output = output[:, -1, :] / temperature  \n            probabilities = nn.functional.softmax(output, dim=-1)\n            \n            # sample the next character\n            predicted_id = torch.multinomial(probabilities, num_samples=1)\n            \n            # add the predicted character to the generated text\n            input_eval = torch.cat([input_eval, predicted_id], dim=1)\n\n            # convert the predicted character to a string and append to the generated text\n            generated_text.append(index_to_character[predicted_id.item()])\n\n    # return text\n    return start_sentence + ''.join(generated_text)\n\n# Use the funtion to generate text\n# My Prompts\nprompts = [\"is it\", \"harry potter and\", \"yesterday i went to\", \"harry, listen to me\", \"once upon a time\", \"the end of the world is near\", \"harry, i want\", \"ron and i\", \"the dark lord\", \"i am a wizard\", \"my dear harry\", \"the forbidden forest\", \"the master of\", \"only if you\", \"the ball was\", \"make me some\"]\n\n# generate text for each prompt\nfor prompt in prompts:\n    print(f\"prompt: {prompt}\")\n    print(text_generator(model, prompt, 100))\n    print('-' * 100)\n\n\nprompt: is it\nis it. he could, i used sirius was not without spotty grafficance much fifty- water if harry and harry wa\n----------------------------------------------------------------------------------------------------\nprompt: harry potter and\nharry potter and the walls, portrait metsier across the unucularly curtains?   good loudly,  said sirius and who did\n----------------------------------------------------------------------------------------------------\nprompt: yesterday i went to\nyesterday i went to get thousand over the magoss us anymore, but your mother much, ever insisting red leg in the worlin\n----------------------------------------------------------------------------------------------------\nprompt: harry, listen to me\nharry, listen to me!  said ron s face cleaning explosing and pretending from a moment where all heavencing when she cro\n----------------------------------------------------------------------------------------------------\nprompt: once upon a time\nonce upon a time he s cart the fact that each rose, a leamed as she with. nothing during the dares of a starting dow\n----------------------------------------------------------------------------------------------------\nprompt: the end of the world is near\nthe end of the world is nearly git.  i have thought the minister plimpy to see a wet?  he rolled aftermined? who had lauded and \n----------------------------------------------------------------------------------------------------\nprompt: harry, i want\nharry, i want to five,  said harry, still deep the knock him.  and they!  the school strile, to crashed night pro\n----------------------------------------------------------------------------------------------------\nprompt: ron and i\nron and it had been killed through the first time forward to be stay-new, and mother kept his side,  they kne\n----------------------------------------------------------------------------------------------------\nprompt: the dark lord\nthe dark lord to enjoy us possibly,  said voldemort.  below kreacher,  said harry memberteenth your father inside\n----------------------------------------------------------------------------------------------------\nprompt: i am a wizard\ni am a wizardan. but i knew that turn. i just go to his care   does this, i ve been back the rightful, bun,  said\n----------------------------------------------------------------------------------------------------\nprompt: my dear harry\nmy dear harry s shoulder; cho sselver within hagrid would pass off an  detention,  said harry hissing grunting up\n----------------------------------------------------------------------------------------------------\nprompt: the forbidden forest\nthe forbidden forest, scrambled, and their things ever, when he ran along would have been a horror, damp her, followed h\n----------------------------------------------------------------------------------------------------\nprompt: the master of\nthe master of somebody on the story; vermonituants he didn t let snape found helping themselves woken the bag eru\n----------------------------------------------------------------------------------------------------\nprompt: only if you\nonly if you must have emals.  in during doge in its indesh all three pile up the others, we master has a curses\n----------------------------------------------------------------------------------------------------\nprompt: the ball was\nthe ball was backward a love the alicia was this. krum has amartments and chairing, and came sent perhaps danger\n----------------------------------------------------------------------------------------------------\nprompt: make me some\nmake me somebody else of ministry, ron.   the end of the wizards i always thought that were bringing out to real\n----------------------------------------------------------------------------------------------------\n\n\nWe see that while some words make sense, the longer you go in the sentence, the less sense it makes. This is because the model is not able to understand the context of the words, and it is just predicting the next character based on the previous ones. This is a common problem with character-based RNNs. However, it is interesting to see the model generating some Harry Potter related content in some scenarios. Some sentences (the begginings since the endings stop makling sense) seem to be taken straight from the books, while others are just random words. An example of this is when I prompted “Harry, listen to me” and the model mentioned Ron, or “make me some” and it mentions ron again. I just found it interesting how much the model is affected by the training data."
  },
  {
    "objectID": "Stat_learn.html",
    "href": "Stat_learn.html",
    "title": "Behind the beats: Classifying Spotify Data Across Genres and Decades",
    "section": "",
    "text": "Through this project, our group sought to bridge the gap between music and data science by employing Spotify’s extensive datasets to perform assorted classification tasks. Using various metrics provided by Spotify, such as danceability, energy, and loudness, we aimed to categorize music into genres and decades. Our goal was to implement sophisticated data analysis techniques learned in class to perform classification at both the artist and track levels. This project explores the feasibility of such classifications and tests the effectiveness of different machine-learning models in handling uniquely complex data, such as music. By applying these methods, we aim to deepen our understanding of musical trends and patterns, enhancing our ability to predict and categorize musical genres more effectively. Furthermore, this study is a practical example of how data science can be applied in creative industries, demonstrating the potential for machine learning to influence and innovate within the music sector."
  },
  {
    "objectID": "Stat_learn.html#artist-level-genre-classification",
    "href": "Stat_learn.html#artist-level-genre-classification",
    "title": "Behind the beats: Classifying Spotify Data Across Genres and Decades",
    "section": "Artist Level Genre Classification",
    "text": "Artist Level Genre Classification\n\nData Setup\nIn the initial steps of our project, we utilized essential Python libraries, such as numpy and pandas, which are crucial for numerical operations and data manipulation. We loaded our primary dataset, artists.csv, into a pandas DataFrame. This dataset contains detailed information about music artists, including their names and associated genres. An immediate examination of the dataset provided an understanding of its structure, which includes various columns critical to our analysis.\n\n\nData Exploration and Cleansing\nDuring our data exploration phase, we identified rows where the genre information was missing key data for our genre classification model. Specific corrections included rectifying genre assignments for artists like “The Beach Boys” and removing entries with corrupted data, such as “Roger Miller.” This cleansing step ensured the accuracy and reliability of our dataset, setting a solid foundation for further analysis and modeling.\n\n\nAssigning a Primary Genre\nAs musicians can belong to numerous genres, we initially investigated performing multi-label, multi-class classification. This effort proved ineffective with such a small dataset of approximately 400 artists. Next, we assigned a primary genre for each artist and perform single-label, multi-class classification. To do this, we first removed four of the ten genres: pop, rap, jazz, and funk. These genres were chosen for a combination of two reasons: cannibalization with other genres and lacking a specific genre identity. We were then left with six distinct genres: country, EDM, hip-hop, Latin, rock, and soul. While we had removed much of the cannibalization, numerous artists still had more than one Spotify-assigned genre. To address this issue, we used the Spotify rankings within each genre. We created a function capable of comparing their rank across genres in hopes of determining what genre an artist was more important to. These comparisons allowed us to assign a primary genre for each artist in our dataset, leaving us with about 300 artists while ensuring that our classifications are accurate. This comprehensive approach to data preparation prepared our dataset for effective machine learning applications.\n\n\nMachine Learning Preparation\nWe prepared our dataset for machine learning by splitting it into training and test sets, which is a standard practice for evaluating a model’s performance on unseen data. We also converted categorical variables into numerical format using techniques such as one-hot encoding. These transformations are crucial for training machine learning models that require numerical input, setting the stage for robust model training.\n\n\nModel Training and Evaluation\nIn this section of our project, we evaluated several machine learning models to determine the most effective approach for classifying music genres based on artist data. Each model was assessed based upon its accuracy and computation time, which are crucial factors in practical applications. Below is a summary of the outcomes for each model:\n\nLogistic Regression\n\nTest Accuracy: 81.03%\nComputation Time: 3.2 seconds\nComments: Logistic Regression provided a robust balance between speed and accuracy, proving to be highly efficient for scenarios where prompt results are necessary without significantly compromising performance.\n\n\n\nSupport Vector Machine (SVM)\n\nTest Accuracy: 77.59%\nComputation Time: 3.4 seconds\nComments: SVM displayed slightly lower accuracy compared to Logistic Regression and had a comparable speed. It is generally well-suited for linearly separable data, but showed moderate performance in our multi-class classification task.\n\n\n\nNeural Network\n\nTrain Accuracy: 68.85%\nTest Accuracy: 70.69%\nComputation Time: 11 minutes 46.2 seconds\nComments: The Neural Network required considerably more training time and yielded lower accuracy even with adjusted hyperparameters. Despite their capabilities, neural networks generally require a lot of data to learn effectively and might overfit or underperform on smaller datasets where simpler models may be more effective.\n\n\n\nRandom Forest\n\nTest Accuracy: 79.31%\nComputation Time: 1 minute 6.4 seconds\nComments: Random Forest achieved good accuracy, suggesting effective handling of the diverse and feature-rich data through its ensemble method. The computational time was reasonable for an ensemble approach, making it a strong contender.\n\n\n\nXGBoost\n\nTest Accuracy: 77.59%\nComputation Time: 24.5 seconds\nComments: XGBoost is well known for its high performance in structured data problems. Here, it offered competitive accuracy with SVM but required longer training time. Its efficient management of various data structures makes it a valuable model despite the higher computational cost.\n\n\n\n\nHyperparameter Tuning\nWe implemented grid searches to fine tune each model’s hyperparameters and optimize its performance. This step was crucial in identifying the most effective model settings for our specific dataset and artist-based genre classification task. With the chosen hyperparameters, the models were evaluated on the test set to determine how they perform on unseen data. These results provided a definitive assessment of our model’s performance and its ability to generalize on new data.\n\n\nConclusion: Classifying Artists into Genres\nThe genre classification at the artist level proved to be quite successful, routinely achieving test accuracies over 75%, with logistic regression peaking at 81%. This classification was significantly higher than a baseline model of random guessing, indicative of the effectiveness of our feature selection and machine learning techniques."
  },
  {
    "objectID": "Stat_learn.html#track-level-genre-classification",
    "href": "Stat_learn.html#track-level-genre-classification",
    "title": "Behind the beats: Classifying Spotify Data Across Genres and Decades",
    "section": "Track Level Genre Classification",
    "text": "Track Level Genre Classification\n\nData Setup\nWe imported necessary Python libraries such as numpy and pandas, essential for handling large datasets and numerical computations. The primary dataset, tracks.csv, was loaded into a pandas DataFrame. This dataset contains detailed attributes of music tracks such as danceability, energy, loudness, and other features that describe the audio characteristics of the tracks.\n\n\nData Cleaning and Preprocessing\nInitial data cleaning involved handling missing values, especially in the genre column, which is crucial for our classification task. We also discarded any irrelevant or redundant data points to avoid skewing our results and impairing our models’ efficiency. This cleaning step ensured a more focused and efficient dataset ready for the subsequent stages of our analysis.\n\n\nIntegration and Processing of Genre-Specific Data\nBy integrating genre_of_artists.csv with our track data, we linked each track with its corresponding artist’s genre, thereby enriching our dataset with essential classification labels. This integration streamlined the data and enriched it, providing a solid foundation for accurate genre classification. We decided to use this approach since we were unable to acquire genre information for individual tracks.\n\n\nFeature Engineering and Data Integration\nWe conducted feature engineering as well, which included encoding categorical variables, normalizing numerical values, and creating new features that could provide more insights into the genre classification. Features derived from the key_mode column, such as musical key and mode, were particularly significant, as these musical aspects are often strong indicators of genre.\n\n\nMachine Learning Preparation\nThe prepared dataset was split into training and test sets to validate the effectiveness of the models. This step is critical in assessing how well our model performs on new, unseen data and ensuring that we accurately gauge its real world applicability.\n\n\nModel Training and Evaluation\nWe trained and evaluated several machine learning models to classify tracks into genres. Each model’s performance was assessed based on accuracy and computation time. Below is a summary of how each model performed:\n\nLogistic Regression\n\nTest Accuracy: 51.38%\nComputation Time: 6 minutes 3.89 seconds\n\n\n\nSupport Vector Machine (SVM)\n\nTest Accuracy: 54.89%\nComputation Time: 80 minutes 4.4 seconds\n\n\n\nNeural Network\n\nTrain Accuracy: 54.45%\nTest Accuracy: 49.18%\nComputation Time: 44 minutes\n\n\n\nRandom Forest\n\nTest Accuracy: 61.68%\nComputation Time: 46 minutes 49.8 seconds\n\n\n\nXGBoost\n\nTest Accuracy: 63.03%\nComputation Time: 2 minutes 51.4 seconds\n\n\n\n\nHyperparameter Tuning\nWe performed hyperparameter tuning using techniques such GridSearchCV to optimize each model’s performance. This process involved systematically testing different combinations of parameters to find the best setup for each model. The tuning was particularly crucial for complex models like Neural Networks and XGBoost, where the right combination of parameters can significantly impact the model’s effectiveness and efficiency.\n\n\nConclusion: Classifying Tracks into Genres\nThe track-level genre classification performed less optimally due to the inherent variability in songs by the same artist, peaking at 63% accuracy with XGBoost. This suggests that while machine learning can significantly aid in genre classification, the complex nature of music genres often requires more sophisticated models or multi-modal data integration to improve accuracy."
  },
  {
    "objectID": "Data_Gathering.html",
    "href": "Data_Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "This tab shows how and where the data was gathered from. Moreover, it displays a few row of the row data in order to display the data structure and its content.\n\n\nIt is worth noting that not all data sets have been used for the study, but some just for some EDA as they could give us interesting insights for the study. The ones that have been used just for EDA purposes will be under “Other data sets”."
  },
  {
    "objectID": "Data_Gathering.html#data-gathering",
    "href": "Data_Gathering.html#data-gathering",
    "title": "Data Gathering",
    "section": "",
    "text": "This tab shows how and where the data was gathered from. Moreover, it displays a few row of the row data in order to display the data structure and its content.\n\n\nIt is worth noting that not all data sets have been used for the study, but some just for some EDA as they could give us interesting insights for the study. The ones that have been used just for EDA purposes will be under “Other data sets”."
  },
  {
    "objectID": "Data_Gathering.html#census-blocks-data-set",
    "href": "Data_Gathering.html#census-blocks-data-set",
    "title": "Data Gathering",
    "section": "Census blocks data set",
    "text": "Census blocks data set\nThis data set contains the information about all the census blocks in the US. It was downloaded here. The data set contains 117 columns and 220740 rows. Each row is a census block. The information about what each variable means can be found inside EPA pdf inside the data folder. The data looks like this:\n\n\nCode\nimport pandas as pd\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Final.csv'\n\ndf = pd.read_csv(file_path)\ndf.head()\n\n\n\n\n\n\n\n\n\nOBJECTID\nGEOID10\nGEOID20\nSTATEFP\nCOUNTYFP\nTRACTCE\nBLKGRPCE\nCSA\nCSA_Name\nCBSA\n...\nD5DRI\nD5DE\nD5DEI\nD2A_Ranked\nD2B_Ranked\nD3B_Ranked\nD4A_Ranked\nNatWalkInd\nShape_Length\nShape_Area\n\n\n\n\n0\n1\n4.811300e+11\n4.811300e+11\n48\n113\n7825\n4\n206.0\nDallas-Fort Worth, TX-OK\n19100.0\n...\n0.184697\n0.000476\n0.137707\n6\n14\n15\n17\n14.000000\n3110.360820\n297836.0831\n\n\n1\n2\n4.811300e+11\n4.811300e+11\n48\n113\n7825\n2\n206.0\nDallas-Fort Worth, TX-OK\n19100.0\n...\n0.323221\n0.000801\n0.231868\n3\n10\n12\n14\n10.833333\n3519.469110\n484945.1466\n\n\n2\n3\n4.811300e+11\n4.811300e+11\n48\n113\n7825\n3\n206.0\nDallas-Fort Worth, TX-OK\n19100.0\n...\n0.314628\n0.000736\n0.213146\n1\n1\n7\n17\n8.333333\n1697.091802\n106705.9281\n\n\n3\n4\n4.811300e+11\n4.811300e+11\n48\n113\n7824\n1\n206.0\nDallas-Fort Worth, TX-OK\n19100.0\n...\n0.229821\n0.000708\n0.205018\n16\n10\n17\n17\n15.666667\n2922.609204\n481828.4303\n\n\n4\n5\n4.811300e+11\n4.811300e+11\n48\n113\n7824\n2\n206.0\nDallas-Fort Worth, TX-OK\n19100.0\n...\n0.164863\n0.000433\n0.125296\n4\n7\n11\n14\n10.166667\n3731.971773\n687684.7752\n\n\n\n\n5 rows × 117 columns"
  },
  {
    "objectID": "Data_Gathering.html#fuel-and-energy-data-set",
    "href": "Data_Gathering.html#fuel-and-energy-data-set",
    "title": "Data Gathering",
    "section": "Fuel and energy data set",
    "text": "Fuel and energy data set\nThis data set contains information about public transportation, agencies, cities, and energy consumption. It was downloaded here. The data set contains 64 columns and 1315 rows. The data looks like this:\n\n\nCode\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/2021_Fuel_and Energy.xlsm'\n\ndf = pd.read_excel(file_path, sheet_name='Fuel and Energy')\n\ndf.head()\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nLegacy NTD ID\nNTD ID\nOrganization Type\nReporter Type\nPrimary UZA Population\nAgency VOMS\nMode\n...\nOther Fuel (mpg) Questionable\nElectric Propulsion (mi/kwh)\nElectric Propulsion (mi/kwh) Questionable\nElectric Battery (mi/kwh)\nElectric Battery (mi/kwh) Questionable\nAny data questionable?\nUnnamed: 60\nUnnamed: 61\n1\nUnnamed: 63\n\n\n\n\n0\nMTA New York City Transit\nBrooklyn\nNY\n2008\n20008\nSubsidiary Unit of a Transit Agency, Reporting...\nFull Reporter\n18351295\n10075\nDR\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNo\nNaN\nNaN\nHide questionable data tags\nNaN\n\n\n1\nMTA New York City Transit\nBrooklyn\nNY\n2008\n20008\nSubsidiary Unit of a Transit Agency, Reporting...\nFull Reporter\n18351295\n10075\nHR\n...\nNaN\n0.225575\nNaN\nNaN\nNaN\nNo\nNaN\nNaN\nShow questionable data tags\nNaN\n\n\n2\nMTA New York City Transit\nBrooklyn\nNY\n2008\n20008\nSubsidiary Unit of a Transit Agency, Reporting...\nFull Reporter\n18351295\n10075\nCB\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNo\nNaN\n1.0\n1\n1.0\n\n\n3\nMTA New York City Transit\nBrooklyn\nNY\n2008\n20008\nSubsidiary Unit of a Transit Agency, Reporting...\nFull Reporter\n18351295\n10075\nMB\n...\nNaN\nNaN\nNaN\n1.672273\nNaN\nNo\nNaN\nNaN\nNaN\nNaN\n\n\n4\nMTA New York City Transit\nBrooklyn\nNY\n2008\n20008\nSubsidiary Unit of a Transit Agency, Reporting...\nFull Reporter\n18351295\n10075\nRB\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNo\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 64 columns"
  },
  {
    "objectID": "Data_Gathering.html#reddit-data",
    "href": "Data_Gathering.html#reddit-data",
    "title": "Data Gathering",
    "section": "Reddit Data",
    "text": "Reddit Data\nThe following code extracted reddit urls from reddit posts about Public Transportation and saved them to a .json. We only focussed on those that specifically talked about public transportation in order to later analyze the sentiment of people’s opinions on public transportation.\n\n\nCode\nlibrary(RedditExtractoR)\nlibrary(jsonlite)\n\ntop_Pub_Transp_urls &lt;- find_thread_urls(keywords=\"public transportation\")\njsonlite::write_json(top_Pub_Transp_urls, \"top_pub_transp_urls.json\")\n\n\nThen, the following code extracted the content of those reddit posts and performed sentiment analysis on them, generated a data frame, and saved it to a .csv file\n\n\nCode\nimport pandas as pd\nimport json\n\nwith open('sentiment_scores.json', 'r') as json_file:\n    sentiment_scores = json.load(json_file)\n\nids = []\nneg_scores = []\nneu_scores = []\npos_scores = []\ncompound_scores = []\n\nfor idx, item in enumerate(sentiment_scores, start=1):\n    ids.append(idx)\n    sentiment_score = item.get('sentiment_score', {})\n    neg_scores.append(sentiment_score.get('neg', 0))\n    neu_scores.append(sentiment_score.get('neu', 0))\n    pos_scores.append(sentiment_score.get('pos', 0))\n    compound_scores.append(sentiment_score.get('compound', 0))\n\ndata = {\n    'ID': ids,\n    'Negative Score': neg_scores,\n    'Neutral Score': neu_scores,\n    'Positive Score': pos_scores,\n    'Compound Score': compound_scores\n}\n\ndf = pd.DataFrame(data)\n\ndf.to_csv('sentiment_scores.csv', index=False)\n\n\nThe final data’s first few rows look like this (Note that only the sentiments of the posts are shown here but the data set with the content can be found in our data folder under Reddit_sentiment_data):"
  },
  {
    "objectID": "Data_Gathering.html#border-crossing-data-set",
    "href": "Data_Gathering.html#border-crossing-data-set",
    "title": "Data Gathering",
    "section": "Border crossing data set",
    "text": "Border crossing data set\nThis data set contains information of the vehicles and their type that crossed the border between other countries and the US. It was downloaded here. The data set contains 10 columns and 386549 rows. The data looks like this:\n\n\nCode\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/Border_Crossing_Entry_Data_20231103.csv'\ndf = pd.read_csv(file_path)\n\ndf.head()\n\n\n\n\n\n\n\n\n\nPort Name\nState\nPort Code\nBorder\nDate\nMeasure\nValue\nLatitude\nLongitude\nPoint\n\n\n\n\n0\nDetroit\nMichigan\n3801\nUS-Canada Border\nAug 2023\nTrains\n128\n42.332\n-83.048\nPOINT (-83.047924 42.331685)\n\n\n1\nAlcan\nAlaska\n3104\nUS-Canada Border\nJul 2023\nBus Passengers\n696\n62.615\n-141.001\nPOINT (-141.001444 62.614961)\n\n\n2\nCalais\nMaine\n115\nUS-Canada Border\nJul 2023\nBuses\n16\n45.189\n-67.275\nPOINT (-67.275381 45.188548)\n\n\n3\nNoonan\nNorth Dakota\n3420\nUS-Canada Border\nJul 2023\nTrucks\n142\n48.999\n-103.004\nPOINT (-103.004361 48.999333)\n\n\n4\nWarroad\nMinnesota\n3423\nUS-Canada Border\nMay 2023\nBuses\n41\n48.999\n-95.377\nPOINT (-95.376555 48.999)"
  },
  {
    "objectID": "Data_Gathering.html#other-data-sets",
    "href": "Data_Gathering.html#other-data-sets",
    "title": "Data Gathering",
    "section": "Other data sets",
    "text": "Other data sets\n\nAPI for cityofchicago.org\nThe following code extracted the data frame about buses information in Chicago and saved it into a csv file. After careful analysis, while it was a very interesting dataset, others have been used for our analysis, but for requirement purposes, this has been left in the data folder. The data looks like this:\n\n\nCode\nimport pandas as pd\nfrom sodapy import Socrata\n\nclient = Socrata(\"data.cityofchicago.org\", None)\n\nresults = client.get(\"bynn-gwxy\", limit=2000)\n\nresults_df = pd.DataFrame.from_records(results)\n\nresults_df.to_csv('Chicago_avg_Buses.csv')\n\n\n\n\n\n\n\n\n\nEnergy consumed by passenger miles\nThe files energy_consumed_byMill_passenger_MILES.xlsx and vehicle_production_countries.xlsx were downloaded from: bts.gov.\nThese data sets focuses on the energy consumed by passenger miles and the vehicle production countries. The data looks like this (note that the data sets have many more columns but for the sake of space, only a few are shown here):\n\n\n\n\n\n\n\n\n\n\n\n\nDC Metro Scorecard\nThe file DC_Metro_Scorecard.xlsx and the zip folders: Walkable_distance_to_PubTrans, data.world\nThe data looks like this (in order of mention):"
  },
  {
    "objectID": "Dimension_red.html",
    "href": "Dimension_red.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Code\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom __future__ import print_function\nimport time\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport warnings"
  },
  {
    "objectID": "Dimension_red.html#explanation-of-dimensionality-reduction-methods",
    "href": "Dimension_red.html#explanation-of-dimensionality-reduction-methods",
    "title": "Dimensionality Reduction",
    "section": "Explanation of Dimensionality Reduction methods",
    "text": "Explanation of Dimensionality Reduction methods\nThe goal of this tab is to look closer at every variable in the EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv and see which variables explain the most variance in order to understand that data set better due to its large shape (71 columns). The columns that explain the most variance will be utilized for some methods of clustering due to computational limitations of my laptop and to also visualize the clusterings calculated. This will allow us not only reduce the size of our data set while keeping the most variance but also understand the dimensionality that explains our data and visualize it in a different perspective.\n\nPrincipal Component Analysis (PCA)\nPrincipal Component Analysis is a dimensionality reduction method that is commonly used to reduce the dimensionality of very large data sets like the one dealt in this tab. The goal is to keep as much variance as possible in the data while reducing the number of features as much as possible. For this, PCA finds the principal components of the data.\nFirstly, PCA centers the data by substracting the mean of each feature. Then, PCA uses the covariance matrix to look at the relationship between variables. After this step, the eigenvector are calculated and the ones with the highes eigenvalues are determined to be the princcipal ones. By selecting the variance that we want to keep (in our case will be at least 90%) and subset the first components until that variance is explained, we are able to reduce the dimensionality of our dataset. Then, the selected components are used to produce a projection matrix by combining the eigenvectors and use it to actually transform the original data into the lower-dimensionality space (Shlens 2014).\n\n\nt-distributed Stochastic Neighbor Embedding (t-SNE)\nt-distributed Stochastic Neighbor Embedding or t-SNE is a machine learning algorithm that focusses on dimensionality reduction and visualization of high-dimensionality data. Due to its great visualization capabilities, we will specifically use this method to visualize the clusters calculated and look to see whether there is overlap or not among them (on the first two most explanatory components).\nT-SNE is particularly good at finding relationships between data points, that is why it is a great tool to visualize the clusters calculated. T-SNE models pairwise comparisons etween data points in a high-dimensional space. Then, it uses the Student’s t-distribution to define the probability distribution over pairs of points in a low dimensional space and similarly define the distribution of the high-dimensional data points. After that, it focuses on reducing the divergence of the two distributions. Finally, it adjust the position of the data points to match the pairwise similarities while preserving the local relationships (Wang et al. 2021).\n\n\nPCA vs t-SNE\nPCA focuses on finding the principal components of the dataset while projecting it into a lower dimensional space. This process focusses on maintaining the most variance and rataining the pairwise distances between the data points. On the other hand, t-SNE focuses on maintaining small pairwise distances between the data points and it is not concerned with the variance of the data. Thus, for these reasons, we will use PCA to compute some of our more computational expensive clustering methods and t-SNE to visualize the clusters calculated."
  },
  {
    "objectID": "Dimension_red.html#pca",
    "href": "Dimension_red.html#pca",
    "title": "Dimensionality Reduction",
    "section": "PCA",
    "text": "PCA\nWe will use PCA to determine which variables are the ones that explain the most variance of our data set. We will use the new data set obtained for some clustering methods and a better understanding of the EPA Smart Location Database dataset. For this, we will plot the explained variance based on the number of components/variables kept and choose our ideal number of components based on the variance explained.\nBy rule of thumb, we want to keep at least 80% of the variance. However, to be more percise, we are going to choose the number of components such that the explained variance is greater or equal to 90%.\n\n\nCode\ndf=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv',)\n\nscaler = StandardScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df))\n\npca=PCA()\npca.fit(df_scaled)\n\npca.explained_variance_ratio_\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom this graph, we can seethat between 20 and 30 components, there is a thresshold of 90% of the variance explained. Thus, we will calculate the number of components that explain 90% of the variance in the following code chunk.\n\n\nCode\nnp.where(np.cumsum(pca.explained_variance_ratio_) &gt; 0.9)[0][0]\n\n\n25\n\n\n25 components explain 90% of the variance. Thus, we will use the first 25 components to reduce the dimensionality of our data set. We will generate a new data set with the first 25 components from PCA and use it for some clustering methods and visualization. The following code will generate this new data set.\n\n\nCode\npca=PCA(n_components=25)\npca.fit(df_scaled)\n\npca.transform(df_scaled)\nscores_pca = pca.transform(df_scaled)\n\nscores_pca_df = pd.DataFrame(scores_pca, columns=[f'PC{i}' for i in range(1, scores_pca.shape[1] + 1)])\nscores_pca_df.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv', index=False)\n\n\nWe saved the new data set with the 25 components in a new csv file called EPA_SmartLocationDatabase_V3_Jan_2021_Clean_PCA.csv (located in the clean data folder). This new data set will be used for some of the clustering methods due to our computational limitations. Furthermore, it is worth noting that the columns are ordered in descending order of explained variance (the first one being the variable that explains the most of the variance and the last one the least)."
  },
  {
    "objectID": "Dimension_red.html#t-sne",
    "href": "Dimension_red.html#t-sne",
    "title": "Dimensionality Reduction",
    "section": "t-SNE",
    "text": "t-SNE\nWe will use t-SNE to visualize the data set and the clusters calculated with k-means in 2 dimensions. This will help us understand the data set better and see if there are any clusters that can be found. We will plot our data set in 2 dimensions and color the points based on the clusters calculated by k-means. However, we will do this for different perplexity values to see if there is any difference in the clusters.\n\n\nCode\nX = df.iloc[:, :]\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nperplexity_values = [5, 20, 50]\n\nkmeans = KMeans(n_clusters=12, random_state=42)\n\nfig, axes = plt.subplots(1, len(perplexity_values), figsize=(15, 5))\n\nfor i, perplexity in enumerate(perplexity_values):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n        X_tsne = tsne.fit_transform(X)\n\n    clusters = kmeans.fit_predict(X_tsne)\n\n    tsne_df = pd.DataFrame(data=X_tsne, columns=['Dimension 1', 'Dimension 2'])\n    tsne_df['Cluster'] = clusters\n\n    sns.scatterplot(x='Dimension 1', y='Dimension 2', hue='Cluster', data=tsne_df, palette='viridis', legend='full', ax=axes[i])\n    axes[i].set_title(f't-SNE Visualization (Perplexity = {perplexity})')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis is a good visualization of the clusters found by k-means. More advanced methods of visualization could be done with higher computational power. However, it does not change too much from different perplexity values, which is a good sign. While the data looks like one big blob, this is probably due to the choice of the two dimensions, as we can see more clear differentiation of the clusters in the pairplots on the clustering tab. However, the clusters do not seem to overlap with eachother, which is a very good sign of the existence of clusters in the data set."
  },
  {
    "objectID": "Dimension_red.html#conclusion",
    "href": "Dimension_red.html#conclusion",
    "title": "Dimensionality Reduction",
    "section": "Conclusion",
    "text": "Conclusion\nWe can see that there are some variables that explain the most variance in the data set. Keeping 25 out of 71 variables will preserve more than 90% of the variance, which means there is a large number of redundant variables in the data set. Furthermore, visualizing the clusters using t_SNE in 2 dimensions was very helpful to see if there was any overlap on the clusters, which there is not an aparent one. Even though it could be seen as one giant cluster, this is probably due to the dimensions chosen. However, the fact that they do not overlap supports our claim that there are clusters on the data set (as the clustering studies also support this claim)."
  },
  {
    "objectID": "AI_Detection.html",
    "href": "AI_Detection.html",
    "title": "AI Image Detection",
    "section": "",
    "text": "This project focuses on detecting AI-generated images in election officials’ social media posts related to voter information. The tool will support Dr. Thessalia Merivaki and her team in analyzing voter information across the U.S. Her research has uncovered instances of AI-generated content being misused, making automated detection a crucial asset for identifying and mitigating misinformation.\nTo enhance detection accuracy, we are developing a multi-faceted approach that goes beyond standard classification models. Our methodology integrates object detection, Optical Character Recognition (OCR), rule-based systems, and classification algorithms to provide a more comprehensive and reliable solution for identifying AI-generated images."
  },
  {
    "objectID": "AI_Detection.html#in-progress",
    "href": "AI_Detection.html#in-progress",
    "title": "AI Image Detection",
    "section": "",
    "text": "This project focuses on detecting AI-generated images in election officials’ social media posts related to voter information. The tool will support Dr. Thessalia Merivaki and her team in analyzing voter information across the U.S. Her research has uncovered instances of AI-generated content being misused, making automated detection a crucial asset for identifying and mitigating misinformation.\nTo enhance detection accuracy, we are developing a multi-faceted approach that goes beyond standard classification models. Our methodology integrates object detection, Optical Character Recognition (OCR), rule-based systems, and classification algorithms to provide a more comprehensive and reliable solution for identifying AI-generated images."
  },
  {
    "objectID": "AI_Detection.html#check-your-ai-intuition",
    "href": "AI_Detection.html#check-your-ai-intuition",
    "title": "AI Image Detection",
    "section": "Check your AI intuition",
    "text": "Check your AI intuition\nThe problem is hard sometimes, even for humans! Try to identify which ones are AI generated and which ones are not."
  },
  {
    "objectID": "Tabula_RL.html#games",
    "href": "Tabula_RL.html#games",
    "title": "Demonstration of the Package created",
    "section": "Games",
    "text": "Games\nThis package contains three games: Boat, Gridworld and Geosearch.\n\nBoat:\nThis environment consists of a Boat that has to move left or right and the wind can also push it to the left or right. The goal is to maximize the rewards as shown in the picture below.\n\n\n\nGridworld:\nThis environment consists of a Gridworld with a start and a goal. The goal is to reach the goal / maximum reward as shown in the picture below. The environment is noisy and the agent can move in four directions: up, down, left and right. However, sometimes the agent can move in a different direction than the one chosen due to the noise.\n\n\n\nGeosearch:\nThe goal of this environment is to reach the resources and maximize the reward. While the two resources utilized are desired, one of them is more valuable than the other. The both have different probability distributions and size. The grid is 25 by 25 and the agent can move in four directions: up, down, left and right. The image below shows the intent of the environment (even though this is a simpler approximation)."
  },
  {
    "objectID": "Tabula_RL.html#information-about-the-package",
    "href": "Tabula_RL.html#information-about-the-package",
    "title": "Demonstration of the Package created",
    "section": "Information about the package",
    "text": "Information about the package\nEach game can be rendered by running the following commented code in order to visualize the game. It can also be put in the hyperparameters of the game to render the game automatically. The default is render = False.\nThe environments are: GeosearchEnv, GridWorldEnv, and BoatEnv. The solvers are: DPSolver, MCSolver, and TDSolver. We also have a function to plot the optimal policy of the environment. They are PolicyPlotter_MC_TD (for MC and TD) and PolicyPlotter_DP (only for DP). The code is docummented and I suggest visiting the code to see the details of the functions.\nBelow you will see a demonstration of the usage of the package created. DP solver has either value iteration or policy iteration (default is policy iteration). It also initializes its policy with a initial random policy where each transition is equally likely. MC solver has Exploring starts (default set as true) or On-Policy First-Visit Monte Carlo Control. It uses an epsilon greedy policy. Finally, the TD solver has the option for SARSA or Q-learning (default is SARSA). It also uses and epsilon greedy policy.\nDue to the differences of the games, even though some reporting has been standarized, others due to the large amount of states, it is not possible to show all the results and different techniques have been used. The results are shown in the code and the code is well documented.\nEach environment can be visualized by running the commented code below. The code is commented out to avoid the rendering of the games.\n\n\nCode\n# def visualize_grid_world():\n#     # Initialize the environment\n#     env = GeosearchEnv(), GridWorldEnv(), BoatEnv()\n#     env.reset()\n    \n#     # Render the initial state\n#     env.render()\n\n#     # Keep the window open for a specified amount of time\n#     import time\n#     time.sleep(25)  # Adjust time as needed\n\n#     # Close the environment\n#     env.close()\n\n# if __name__ == \"__main__\":\n#     visualize_grid_world()"
  },
  {
    "objectID": "Tabula_RL.html#boat-solved-with-dp",
    "href": "Tabula_RL.html#boat-solved-with-dp",
    "title": "Demonstration of the Package created",
    "section": "Boat Solved with DP",
    "text": "Boat Solved with DP\nWe will solve the boat environment using Dynamic Programming with Policy Iteration.\n\n\nCode\n# Create the environment\nenv = BoatEnv()\n\n# Do not render the environment\nenv.rendering_enabled = False\n\n# Solver\nsolver = DPSolver(env, gamma=0.99, theta=0.0001, use_value_iteration=False)\n\n# Solve\npolicy, V, Q = solver.solve()\n\n# Print the State-Value Function V\nprint(\"State-Value Function V:\")\nfor s in range(solver.state_space_size):\n    print(f\"State {s}: V = {V[s]:.4f}\")\n\n# Print the Action-Value Function Q\nprint(\"\\nAction-Value Function Q:\")\nfor s in range(solver.state_space_size):\n    for a in range(env.action_space.n):\n        print(f\"Q({s}, {a}) = {Q[s, a]:.4f}\")\n\n# Print the Optimal Policy\nprint(\"\\nOptimal Policy:\")\nfor s in range(solver.state_space_size):\n    action = policy[s]\n    action_name = \"East\" if action == 1 else \"West\"\n    print(f\"State {s}: Take action '{action_name}'\")\n\nboat_action_symbols = {0: '←', 1: '→'}\nboat_plotter = PolicyPlotter_DP((1, 2), policy, action_symbols=boat_action_symbols)\nboat_plotter.plot()\n\n\nState-Value Function V:\nState 0: V = 397.5733\nState 1: V = 399.9915\n\nAction-Value Function Q:\nQ(0, 0) = 394.5976\nQ(0, 1) = 397.5734\nQ(1, 0) = 397.9734\nQ(1, 1) = 399.9916\n\nOptimal Policy:\nState 0: Take action 'East'\nState 1: Take action 'East'\n\n\n\n\n\n\n\n\n\nThe results of the policy are as expected. The boat recieves greater rewards on its right side and the policy is to move to the right. It is a very simple environment and the policy is very clear. It also trains very fast.\n\n\nCode\n# Plot convergence of V\nV_history = np.array(solver.V_history)\nplt.figure(figsize=(8, 5))\nfor s in range(solver.state_space_size):\n    plt.plot(V_history[:, s], label=f\"State {s}\")\nplt.xlabel('Iterations')\nplt.ylabel('V(s)')\nplt.title('Convergence of State-Value Function V')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot convergence of Q\nQ_history = np.array(solver.Q_history)\nplt.figure(figsize=(8, 5))\nfor s in range(solver.state_space_size):\n    for a in range(env.action_space.n):\n        plt.plot(Q_history[:, s, a], label=f\"Q({s},{a})\")\nplt.xlabel('Iterations')\nplt.ylabel('Q(s,a)')\nplt.title('Convergence of Action-Value Function Q')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot convergence of policy\npolicy_history = np.array(solver.policy_history)\nplt.figure(figsize=(8, 5))\nfor s in range(solver.state_space_size):\n    plt.plot(policy_history[:, s], label=f\"Policy for State {s}\")\nplt.xlabel('Iterations')\nplt.ylabel('Action')\nplt.title('Convergence of Policy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that it only needs very few iterations to converge."
  },
  {
    "objectID": "Tabula_RL.html#gridworld-solved-with-dp",
    "href": "Tabula_RL.html#gridworld-solved-with-dp",
    "title": "Demonstration of the Package created",
    "section": "Gridworld Solved with DP",
    "text": "Gridworld Solved with DP\nWe will solve the gridworld environment using Dynamic Programming with Value Iteration.\n\n\nCode\n# Create the environment\nenv = GridWorldEnv()\n\n# Don't render\nenv.rendering_enabled = False\n\n# Solver\nsolver = DPSolver(env, gamma=0.99, theta=0.0001, use_value_iteration=True)\n\n# Solve\npolicy, V, Q = solver.solve()\n\n# Print the State-Value Function V\nprint(\"State-Value Function V:\")\nV_grid = V.reshape(env.grid_size)\nprint(V_grid)\n\n# Print the Action-Value Function Q\nprint(\"\\nAction-Value Function Q:\")\nfor s in range(solver.state_space_size):\n    state = solver.index_to_state(s)\n    print(f\"State {state}:\")\n    for a in range(env.action_space.n):\n        print(f\"  Action {a}: Q = {Q[s, a]:.4f}\")\n\n# Print the Optimal Policy\nprint(\"\\nOptimal Policy:\")\n# Up, Right, Down, Left\naction_symbols = ['↑', '→', '↓', '←']\nfor s in range(solver.state_space_size):\n    action = policy[s]\n    action_name = action_symbols[action]\n    print(f\"State {solver.index_to_state(s)}: Take action '{action_name}'\")\n\n# Convert terminal states to a list of coordinates for the plotter\nterminal_coords = list(env.terminal_states.keys())\n\n# Plot with walls and terminal states\nplotter = PolicyPlotter_DP(env.grid_size, policy, walls=list(env.walls), terminal_states=terminal_coords)\nplotter.plot()\n\n\nState-Value Function V:\n[[4536.43946403 4583.27228353 4630.57816083 4678.36187432 4726.62825061\n  4775.38216512]\n [4583.27228353 4630.57816083 4678.36187432 4726.62825061 4775.38216512\n  4824.62854246]\n [4630.57816083 4678.36187432 4726.62825061 4775.38216512 4824.62854246\n  4874.37235704]\n [4583.27237923 4630.57825557 4678.36196811 4824.62854246 4874.37235704\n  4924.61863347]\n [4536.43965543 4583.27247302 4824.62854246 4874.37235704 4924.61863347\n  4975.37244714]\n [4490.07525888 4536.43974829 4874.37235704 4924.61863347 4975.37244714\n  4924.61872266]]\n\nAction-Value Function Q:\nState 0:\n  Action 0: Q = 4490.0751\n  Action 1: Q = 4536.4396\n  Action 2: Q = 4536.4396\n  Action 3: Q = 4490.0751\nState 1:\n  Action 0: Q = 4536.4396\n  Action 1: Q = 4536.4396\n  Action 2: Q = 4583.2724\n  Action 3: Q = 4490.0751\nState 2:\n  Action 0: Q = 4583.2724\n  Action 1: Q = 4630.5783\n  Action 2: Q = 4583.2724\n  Action 3: Q = 4536.4396\nState 3:\n  Action 0: Q = 4630.5783\n  Action 1: Q = 4678.3620\n  Action 2: Q = 4678.3620\n  Action 3: Q = 4630.5783\nState 4:\n  Action 0: Q = 4678.3620\n  Action 1: Q = 4726.6283\n  Action 2: Q = 4677.6283\n  Action 3: Q = 4630.5783\nState 5:\n  Action 0: Q = 4726.6283\n  Action 1: Q = 4726.6283\n  Action 2: Q = 4775.3823\n  Action 3: Q = 4678.3620\nState 6:\n  Action 0: Q = 4490.0751\n  Action 1: Q = 4583.2724\n  Action 2: Q = 4583.2724\n  Action 3: Q = 4536.4396\nState 7:\n  Action 0: Q = 4536.4396\n  Action 1: Q = 4583.2724\n  Action 2: Q = 4630.5783\n  Action 3: Q = 4536.4396\nState 8:\n  Action 0: Q = 4630.5783\n  Action 1: Q = 4678.3620\n  Action 2: Q = 4678.3620\n  Action 3: Q = 4583.2724\nState 9:\n  Action 0: Q = 4630.5783\n  Action 1: Q = 4677.6283\n  Action 2: Q = 4726.6283\n  Action 3: Q = 4678.3620\nState 10:\n  Action 0: Q = 4678.3620\n  Action 1: Q = 4775.3823\n  Action 2: Q = 4775.3823\n  Action 3: Q = 4678.3620\nState 11:\n  Action 0: Q = 4726.6283\n  Action 1: Q = 4775.3823\n  Action 2: Q = 4824.6286\n  Action 3: Q = 4677.6283\nState 12:\n  Action 0: Q = 4536.4396\n  Action 1: Q = 4630.5783\n  Action 2: Q = 4536.4397\n  Action 3: Q = 4583.2724\nState 13:\n  Action 0: Q = 4583.2724\n  Action 1: Q = 4678.3620\n  Action 2: Q = 4583.2725\n  Action 3: Q = 4583.2724\nState 14:\n  Action 0: Q = 4678.3620\n  Action 1: Q = 4726.6283\n  Action 2: Q = 4678.3620\n  Action 3: Q = 4630.5783\nState 15:\n  Action 0: Q = 4678.3620\n  Action 1: Q = 4775.3823\n  Action 2: Q = 4726.6283\n  Action 3: Q = 4678.3620\nState 16:\n  Action 0: Q = 4677.6283\n  Action 1: Q = 4824.6286\n  Action 2: Q = 4775.3823\n  Action 3: Q = 4726.6283\nState 17:\n  Action 0: Q = 4775.3823\n  Action 1: Q = 4824.6286\n  Action 2: Q = 4874.3724\n  Action 3: Q = 4775.3823\nState 18:\n  Action 0: Q = 4583.2724\n  Action 1: Q = 4583.2725\n  Action 2: Q = 4490.0753\n  Action 3: Q = 4536.4397\nState 19:\n  Action 0: Q = 4630.5783\n  Action 1: Q = 4583.2725\n  Action 2: Q = 4536.4397\n  Action 3: Q = 4536.4397\nState 20:\n  Action 0: Q = 4678.3620\n  Action 1: Q = 4630.5783\n  Action 2: Q = 4630.5783\n  Action 3: Q = 4583.2725\nState 21:\n  Action 0: Q = 4726.6283\n  Action 1: Q = 4775.3823\n  Action 2: Q = 4824.6286\n  Action 3: Q = 4775.3823\nState 22:\n  Action 0: Q = 4775.3823\n  Action 1: Q = 4874.3724\n  Action 2: Q = 4874.3724\n  Action 3: Q = 4824.6286\nState 23:\n  Action 0: Q = 4824.6286\n  Action 1: Q = 4874.3724\n  Action 2: Q = 4924.6187\n  Action 3: Q = 4874.3724\nState 24:\n  Action 0: Q = 4536.4397\n  Action 1: Q = 4536.4397\n  Action 2: Q = 4395.1745\n  Action 3: Q = 4490.0753\nState 25:\n  Action 0: Q = 4583.2725\n  Action 1: Q = 4536.4397\n  Action 2: Q = 4490.0754\n  Action 3: Q = 4490.0753\nState 26:\n  Action 0: Q = 4775.3823\n  Action 1: Q = 4824.6286\n  Action 2: Q = 4775.3823\n  Action 3: Q = 4536.4397\nState 27:\n  Action 0: Q = 4824.6286\n  Action 1: Q = 4874.3724\n  Action 2: Q = 4874.3724\n  Action 3: Q = 4824.6286\nState 28:\n  Action 0: Q = 4874.3724\n  Action 1: Q = 4924.6187\n  Action 2: Q = 4924.6187\n  Action 3: Q = 4824.6286\nState 29:\n  Action 0: Q = 4874.3724\n  Action 1: Q = 4924.6187\n  Action 2: Q = 4975.3725\n  Action 3: Q = 4874.3724\nState 30:\n  Action 0: Q = 4490.0753\n  Action 1: Q = 4490.0754\n  Action 2: Q = 4444.1745\n  Action 3: Q = 4444.1745\nState 31:\n  Action 0: Q = 4536.4397\n  Action 1: Q = 4490.0754\n  Action 2: Q = 4490.0754\n  Action 3: Q = 4395.1745\nState 32:\n  Action 0: Q = 4824.6286\n  Action 1: Q = 4874.3724\n  Action 2: Q = 4824.6286\n  Action 3: Q = 4490.0754\nState 33:\n  Action 0: Q = 4824.6286\n  Action 1: Q = 4924.6187\n  Action 2: Q = 4874.3724\n  Action 3: Q = 4874.3724\nState 34:\n  Action 0: Q = 4874.3724\n  Action 1: Q = 4975.3725\n  Action 2: Q = 4924.6187\n  Action 3: Q = 4874.3724\nState 35:\n  Action 0: Q = 4924.6187\n  Action 1: Q = 4874.3725\n  Action 2: Q = 4874.3725\n  Action 3: Q = 4924.6187\n\nOptimal Policy:\nState 0: Take action '→'\nState 1: Take action '↓'\nState 2: Take action '→'\nState 3: Take action '→'\nState 4: Take action '→'\nState 5: Take action '↓'\nState 6: Take action '→'\nState 7: Take action '↓'\nState 8: Take action '→'\nState 9: Take action '↓'\nState 10: Take action '→'\nState 11: Take action '↓'\nState 12: Take action '→'\nState 13: Take action '→'\nState 14: Take action '→'\nState 15: Take action '→'\nState 16: Take action '→'\nState 17: Take action '↓'\nState 18: Take action '↑'\nState 19: Take action '↑'\nState 20: Take action '↑'\nState 21: Take action '↓'\nState 22: Take action '→'\nState 23: Take action '↓'\nState 24: Take action '↑'\nState 25: Take action '↑'\nState 26: Take action '→'\nState 27: Take action '→'\nState 28: Take action '→'\nState 29: Take action '↓'\nState 30: Take action '↑'\nState 31: Take action '↑'\nState 32: Take action '→'\nState 33: Take action '→'\nState 34: Take action '→'\nState 35: Take action '↑'\n\n\n\n\n\n\n\n\n\nThe results are as expected, trying to reach for the bottom right corner which has the maximum reward. It also trains very fast.\n\n\nCode\n# Plot convergence of V\nV_history = np.array(solver.V_history)\nplt.figure(figsize=(12, 6))\nfor s in range(solver.state_space_size):\n    plt.plot(V_history[:, s], label=f\"State {s}\")\nplt.xlabel('Iterations')\nplt.ylabel('V(s)')\nplt.title('Convergence of State-Value Function V')\nplt.tight_layout()\nplt.show()\n\n# Plot convergence of Q for a few state-action pairs\nQ_history = np.array(solver.Q_history)\nplt.figure(figsize=(12, 6))\nstates_to_plot = [0, env.grid_size[1]*env.grid_size[0]//2, solver.state_space_size - 1]\nactions = [0, 1, 2, 3]\nfor s in states_to_plot:\n    for a in actions:\n        plt.plot(Q_history[:, s, a], label=f\"Q({s},{a})\")\nplt.xlabel('Iterations')\nplt.ylabel('Q(s,a)')\nplt.title('Convergence of Action-Value Function Q')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# Plot convergence of policy for a few states\npolicy_history = np.array(solver.policy_history)\nplt.figure(figsize=(12, 6))\nfor s in states_to_plot:\n    plt.plot(policy_history[:, s], label=f\"Policy for State {s}\")\nplt.xlabel('Iterations')\nplt.ylabel('Action')\nplt.title('Convergence of Policy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on this plots, even though this environment is more complex than the other and has noise, it converges very fast."
  },
  {
    "objectID": "Tabula_RL.html#gesearch-solved-with-dp",
    "href": "Tabula_RL.html#gesearch-solved-with-dp",
    "title": "Demonstration of the Package created",
    "section": "Gesearch solved with DP",
    "text": "Gesearch solved with DP\nWe will solve the geosearch environment using Dynamic Programming with Value Iteration.\n\n\nCode\n# Create the environment\nenv = GeosearchEnv(render=False)\n\n# Solver\nsolver = DPSolver(env, gamma=0.99, theta=0.0001, use_value_iteration=True)\n\n# Solve\npolicy, V, Q = solver.solve()\n\n\n# Policy symbols\naction_symbols = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n\n# Plot the policy\nplotter = PolicyPlotter_DP(grid_size=(env.grid_size, env.grid_size), \n                        policy=policy, \n                        walls=[],\n                        terminal_states=[],\n                        action_symbols=action_symbols)\n\nplotter.plot()\n\n\n\n\n\n\n\n\n\nWhile this environment is harder to visualize, the policy indicates to move towards the to “heat” points (gold and water) as desired. You can try rendering the environment to see the results or see the “heat” points in the grid generated by our functions.\n\n\nCode\n# Reshape V into grid form\nV_grid = V.reshape((env.grid_size, env.grid_size))\n\n# Plot the State-Value Function as a heatmap\nplt.figure(figsize=(8, 6))\nplt.imshow(V_grid, cmap='hot', interpolation='nearest')\nplt.colorbar(label='V(s)')\nplt.title('State-Value Function V')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.gca().invert_yaxis()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Sample a few states to display Q(s, a)\nsample_states = [(0, 0), (12, 12), (24, 24), (20, 20), (10, 10)]\nprint(\"\\nAction-Value Function Q for a sample of States:\")\nfor state in sample_states:\n    s_index = env.state_to_index(state)\n    print(f\"State {state}:\")\n    for a in range(env.action_space.n):\n        print(f\"  Action {a}: Q = {Q[s_index, a]:.4f}\")\n\n\n\nAction-Value Function Q for a sample of States:\nState (0, 0):\n  Action 0: Q = 6.7410\n  Action 1: Q = 6.8092\n  Action 2: Q = 6.7410\n  Action 3: Q = 6.8092\nState (12, 12):\n  Action 0: Q = 8.3799\n  Action 1: Q = 8.5483\n  Action 2: Q = 8.3799\n  Action 3: Q = 8.5483\nState (24, 24):\n  Action 0: Q = 9.2650\n  Action 1: Q = 9.1724\n  Action 2: Q = 9.2650\n  Action 3: Q = 9.1724\nState (20, 20):\n  Action 0: Q = 9.7641\n  Action 1: Q = 9.7642\n  Action 2: Q = 9.7641\n  Action 3: Q = 9.7642\nState (10, 10):\n  Action 0: Q = 8.1483\n  Action 1: Q = 8.2478\n  Action 2: Q = 8.1483\n  Action 3: Q = 8.2478\n\n\n\n\nCode\n# Plot convergence of V for sample states\nV_history = np.array(solver.V_history)\nplt.figure(figsize=(10, 6))\nstate_indices = [env.state_to_index(s) for s in sample_states]\nfor idx, s in enumerate(state_indices):\n    plt.plot(V_history[:, s], label=f\"State {sample_states[idx]}\")\nplt.xlabel('Iterations')\nplt.ylabel('V(s)')\nplt.title('Convergence of State-Value Function V')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot convergence of Q for sample state-action pairs\nQ_history = np.array(solver.Q_history)\nplt.figure(figsize=(10, 6))\nactions = [0, 1, 2, 3]\nfor idx, s in enumerate(state_indices):\n    for a in actions:\n        plt.plot(Q_history[:, s, a], label=f\"Q({sample_states[idx]}, {a})\")\nplt.xlabel('Iterations')\nplt.ylabel('Q(s,a)')\nplt.title('Convergence of Action-Value Function Q')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n\n# Plot convergence of policy for sample states\npolicy_history = np.array(solver.policy_history)\nplt.figure(figsize=(10, 6))\nfor idx, s in enumerate(state_indices):\n    plt.plot(policy_history[:, s], label=f\"Policy for State {sample_states[idx]}\")\nplt.xlabel('Iterations')\nplt.ylabel('Action')\nplt.title('Convergence of Policy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile the convergence seems more spread, it is still very fast. This game, even though is a larger space, has no walls or noise, which makes it easier to solve."
  },
  {
    "objectID": "Tabula_RL.html#boat-solved-with-mc",
    "href": "Tabula_RL.html#boat-solved-with-mc",
    "title": "Demonstration of the Package created",
    "section": "Boat Solved with MC",
    "text": "Boat Solved with MC\nWe will solve the boat environment using Monte Carlo with Exploring Starts.\n\n\nCode\n# Initialize environment\nenv = BoatEnv(render=False)\n\n# Solver\nsolver = MCSolver(env, gamma=0.99, epsilon=0.1, exploring_starts=True, episodes=500)\n\n# Solve\npolicy = solver.run()\n\n# Get the final V and Q\nV = solver.V_history[-1]\nQ = solver.Q_history[-1]\n\n# Plot the optimal policy\nboat_action_symbols = {0: '←', 1: '→'}\nplotter = PolicyPlotter_MC_TD((1, 2), policy, action_symbols=boat_action_symbols)\nplotter.plot()\n\n\nEpisode 100/500 completed.\nEpisode 200/500 completed.\nEpisode 300/500 completed.\nEpisode 400/500 completed.\nEpisode 500/500 completed.\n\n\n\n\n\n\n\n\n\n500 episodes were more than enough to solve the environment. The policy is to move to the right.\n\n\nCode\n# Print the final V, Q, and policy\n\nprint(\"State-Value Function V:\")\nfor s in range(env.observation_space.n):\n    print(f\"State {s}: V = {V[s]:.4f}\")\n\nprint(\"\\nAction-Value Function Q:\")\nfor s in range(env.observation_space.n):\n    print(f\"State {s}:\")\n    for a in range(env.action_space.n):\n        print(f\"  Action {a}: Q = {Q[s, a]:.4f}\")\n\nprint(\"\\nOptimal Policy:\")\nfor s in range(env.observation_space.n):\n    action = policy[s]\n    action_name = \"West\" if action == 0 else \"East\"\n    print(f\"State {s}: Take action '{action_name}'\")\n\n\nState-Value Function V:\nState 0: V = 20.7441\nState 1: V = 13.1214\n\nAction-Value Function Q:\nState 0:\n  Action 0: Q = 20.6799\n  Action 1: Q = 20.7441\nState 1:\n  Action 0: Q = 13.1125\n  Action 1: Q = 13.1214\n\nOptimal Policy:\nState 0: Take action 'East'\nState 1: Take action 'East'\n\n\n\n\nCode\n# Plot convergence plots\n\nV_history = np.array(solver.V_history)\nplt.figure(figsize=(8, 5))\nfor s in range(env.observation_space.n):\n    plt.plot(V_history[:, s], label=f\"State {s}\")\nplt.xlabel('Episodes')\nplt.ylabel('V(s)')\nplt.title('Convergence of State-Value Function V')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nQ_history = np.array(solver.Q_history)\nplt.figure(figsize=(8, 5))\nfor s in range(env.observation_space.n):\n    for a in range(env.action_space.n):\n        plt.plot(Q_history[:, s, a], label=f\"Q({s},{a})\")\nplt.xlabel('Episodes')\nplt.ylabel('Q(s,a)')\nplt.title('Convergence of Action-Value Function Q')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# plot the policy convergence\n\npolicy_history = np.array(solver.policy_history)\nplt.figure(figsize=(8, 5))\nfor s in range(env.observation_space.n):\n    plt.plot(policy_history[:, s], label=f\"Policy for State {s}\")\nplt.xlabel('Episodes')\nplt.ylabel('Action')\nplt.title('Convergence of Policy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nConvergence using MC is much more noisy than DP, but it still converges. It makes sense as it is a stochastic method and bases itself on the episodes."
  },
  {
    "objectID": "Tabula_RL.html#gridworld-solved-with-mc",
    "href": "Tabula_RL.html#gridworld-solved-with-mc",
    "title": "Demonstration of the Package created",
    "section": "Gridworld Solved with MC",
    "text": "Gridworld Solved with MC\nWe will solve the gridworld environment using Monte Carlo with Exploring starts.\n\n\nCode\n# Create the environment\nenv = GridWorldEnv(render=False)\n\n# Solver\nsolver = MCSolver(env, gamma=0.99, epsilon=0.1, exploring_starts=True, episodes=50000)\n\n# Solve\npolicy = solver.run()\n\n# Get the final V and Q\nV = solver.V_history[-1]\nQ = solver.Q_history[-1]\n\nV_grid = V.reshape(env.grid_size)\n\nterminal_coords = list(env.terminal_states.keys())\n\nplotter = PolicyPlotter_MC_TD(env.grid_size, policy, walls=list(env.walls), terminal_states=terminal_coords)\nplotter.plot()\n\n\nEpisode 100/50000 completed.\nEpisode 200/50000 completed.\nEpisode 300/50000 completed.\nEpisode 400/50000 completed.\nEpisode 500/50000 completed.\nEpisode 600/50000 completed.\nEpisode 700/50000 completed.\nEpisode 800/50000 completed.\nEpisode 900/50000 completed.\nEpisode 1000/50000 completed.\nEpisode 1100/50000 completed.\nEpisode 1200/50000 completed.\nEpisode 1300/50000 completed.\nEpisode 1400/50000 completed.\nEpisode 1500/50000 completed.\nEpisode 1600/50000 completed.\nEpisode 1700/50000 completed.\nEpisode 1800/50000 completed.\nEpisode 1900/50000 completed.\nEpisode 2000/50000 completed.\nEpisode 2100/50000 completed.\nEpisode 2200/50000 completed.\nEpisode 2300/50000 completed.\nEpisode 2400/50000 completed.\nEpisode 2500/50000 completed.\nEpisode 2600/50000 completed.\nEpisode 2700/50000 completed.\nEpisode 2800/50000 completed.\nEpisode 2900/50000 completed.\nEpisode 3000/50000 completed.\nEpisode 3100/50000 completed.\nEpisode 3200/50000 completed.\nEpisode 3300/50000 completed.\nEpisode 3400/50000 completed.\nEpisode 3500/50000 completed.\nEpisode 3600/50000 completed.\nEpisode 3700/50000 completed.\nEpisode 3800/50000 completed.\nEpisode 3900/50000 completed.\nEpisode 4000/50000 completed.\nEpisode 4100/50000 completed.\nEpisode 4200/50000 completed.\nEpisode 4300/50000 completed.\nEpisode 4400/50000 completed.\nEpisode 4500/50000 completed.\nEpisode 4600/50000 completed.\nEpisode 4700/50000 completed.\nEpisode 4800/50000 completed.\nEpisode 4900/50000 completed.\nEpisode 5000/50000 completed.\nEpisode 5100/50000 completed.\nEpisode 5200/50000 completed.\nEpisode 5300/50000 completed.\nEpisode 5400/50000 completed.\nEpisode 5500/50000 completed.\nEpisode 5600/50000 completed.\nEpisode 5700/50000 completed.\nEpisode 5800/50000 completed.\nEpisode 5900/50000 completed.\nEpisode 6000/50000 completed.\nEpisode 6100/50000 completed.\nEpisode 6200/50000 completed.\nEpisode 6300/50000 completed.\nEpisode 6400/50000 completed.\nEpisode 6500/50000 completed.\nEpisode 6600/50000 completed.\nEpisode 6700/50000 completed.\nEpisode 6800/50000 completed.\nEpisode 6900/50000 completed.\nEpisode 7000/50000 completed.\nEpisode 7100/50000 completed.\nEpisode 7200/50000 completed.\nEpisode 7300/50000 completed.\nEpisode 7400/50000 completed.\nEpisode 7500/50000 completed.\nEpisode 7600/50000 completed.\nEpisode 7700/50000 completed.\nEpisode 7800/50000 completed.\nEpisode 7900/50000 completed.\nEpisode 8000/50000 completed.\nEpisode 8100/50000 completed.\nEpisode 8200/50000 completed.\nEpisode 8300/50000 completed.\nEpisode 8400/50000 completed.\nEpisode 8500/50000 completed.\nEpisode 8600/50000 completed.\nEpisode 8700/50000 completed.\nEpisode 8800/50000 completed.\nEpisode 8900/50000 completed.\nEpisode 9000/50000 completed.\nEpisode 9100/50000 completed.\nEpisode 9200/50000 completed.\nEpisode 9300/50000 completed.\nEpisode 9400/50000 completed.\nEpisode 9500/50000 completed.\nEpisode 9600/50000 completed.\nEpisode 9700/50000 completed.\nEpisode 9800/50000 completed.\nEpisode 9900/50000 completed.\nEpisode 10000/50000 completed.\nEpisode 10100/50000 completed.\nEpisode 10200/50000 completed.\nEpisode 10300/50000 completed.\nEpisode 10400/50000 completed.\nEpisode 10500/50000 completed.\nEpisode 10600/50000 completed.\nEpisode 10700/50000 completed.\nEpisode 10800/50000 completed.\nEpisode 10900/50000 completed.\nEpisode 11000/50000 completed.\nEpisode 11100/50000 completed.\nEpisode 11200/50000 completed.\nEpisode 11300/50000 completed.\nEpisode 11400/50000 completed.\nEpisode 11500/50000 completed.\nEpisode 11600/50000 completed.\nEpisode 11700/50000 completed.\nEpisode 11800/50000 completed.\nEpisode 11900/50000 completed.\nEpisode 12000/50000 completed.\nEpisode 12100/50000 completed.\nEpisode 12200/50000 completed.\nEpisode 12300/50000 completed.\nEpisode 12400/50000 completed.\nEpisode 12500/50000 completed.\nEpisode 12600/50000 completed.\nEpisode 12700/50000 completed.\nEpisode 12800/50000 completed.\nEpisode 12900/50000 completed.\nEpisode 13000/50000 completed.\nEpisode 13100/50000 completed.\nEpisode 13200/50000 completed.\nEpisode 13300/50000 completed.\nEpisode 13400/50000 completed.\nEpisode 13500/50000 completed.\nEpisode 13600/50000 completed.\nEpisode 13700/50000 completed.\nEpisode 13800/50000 completed.\nEpisode 13900/50000 completed.\nEpisode 14000/50000 completed.\nEpisode 14100/50000 completed.\nEpisode 14200/50000 completed.\nEpisode 14300/50000 completed.\nEpisode 14400/50000 completed.\nEpisode 14500/50000 completed.\nEpisode 14600/50000 completed.\nEpisode 14700/50000 completed.\nEpisode 14800/50000 completed.\nEpisode 14900/50000 completed.\nEpisode 15000/50000 completed.\nEpisode 15100/50000 completed.\nEpisode 15200/50000 completed.\nEpisode 15300/50000 completed.\nEpisode 15400/50000 completed.\nEpisode 15500/50000 completed.\nEpisode 15600/50000 completed.\nEpisode 15700/50000 completed.\nEpisode 15800/50000 completed.\nEpisode 15900/50000 completed.\nEpisode 16000/50000 completed.\nEpisode 16100/50000 completed.\nEpisode 16200/50000 completed.\nEpisode 16300/50000 completed.\nEpisode 16400/50000 completed.\nEpisode 16500/50000 completed.\nEpisode 16600/50000 completed.\nEpisode 16700/50000 completed.\nEpisode 16800/50000 completed.\nEpisode 16900/50000 completed.\nEpisode 17000/50000 completed.\nEpisode 17100/50000 completed.\nEpisode 17200/50000 completed.\nEpisode 17300/50000 completed.\nEpisode 17400/50000 completed.\nEpisode 17500/50000 completed.\nEpisode 17600/50000 completed.\nEpisode 17700/50000 completed.\nEpisode 17800/50000 completed.\nEpisode 17900/50000 completed.\nEpisode 18000/50000 completed.\nEpisode 18100/50000 completed.\nEpisode 18200/50000 completed.\nEpisode 18300/50000 completed.\nEpisode 18400/50000 completed.\nEpisode 18500/50000 completed.\nEpisode 18600/50000 completed.\nEpisode 18700/50000 completed.\nEpisode 18800/50000 completed.\nEpisode 18900/50000 completed.\nEpisode 19000/50000 completed.\nEpisode 19100/50000 completed.\nEpisode 19200/50000 completed.\nEpisode 19300/50000 completed.\nEpisode 19400/50000 completed.\nEpisode 19500/50000 completed.\nEpisode 19600/50000 completed.\nEpisode 19700/50000 completed.\nEpisode 19800/50000 completed.\nEpisode 19900/50000 completed.\nEpisode 20000/50000 completed.\nEpisode 20100/50000 completed.\nEpisode 20200/50000 completed.\nEpisode 20300/50000 completed.\nEpisode 20400/50000 completed.\nEpisode 20500/50000 completed.\nEpisode 20600/50000 completed.\nEpisode 20700/50000 completed.\nEpisode 20800/50000 completed.\nEpisode 20900/50000 completed.\nEpisode 21000/50000 completed.\nEpisode 21100/50000 completed.\nEpisode 21200/50000 completed.\nEpisode 21300/50000 completed.\nEpisode 21400/50000 completed.\nEpisode 21500/50000 completed.\nEpisode 21600/50000 completed.\nEpisode 21700/50000 completed.\nEpisode 21800/50000 completed.\nEpisode 21900/50000 completed.\nEpisode 22000/50000 completed.\nEpisode 22100/50000 completed.\nEpisode 22200/50000 completed.\nEpisode 22300/50000 completed.\nEpisode 22400/50000 completed.\nEpisode 22500/50000 completed.\nEpisode 22600/50000 completed.\nEpisode 22700/50000 completed.\nEpisode 22800/50000 completed.\nEpisode 22900/50000 completed.\nEpisode 23000/50000 completed.\nEpisode 23100/50000 completed.\nEpisode 23200/50000 completed.\nEpisode 23300/50000 completed.\nEpisode 23400/50000 completed.\nEpisode 23500/50000 completed.\nEpisode 23600/50000 completed.\nEpisode 23700/50000 completed.\nEpisode 23800/50000 completed.\nEpisode 23900/50000 completed.\nEpisode 24000/50000 completed.\nEpisode 24100/50000 completed.\nEpisode 24200/50000 completed.\nEpisode 24300/50000 completed.\nEpisode 24400/50000 completed.\nEpisode 24500/50000 completed.\nEpisode 24600/50000 completed.\nEpisode 24700/50000 completed.\nEpisode 24800/50000 completed.\nEpisode 24900/50000 completed.\nEpisode 25000/50000 completed.\nEpisode 25100/50000 completed.\nEpisode 25200/50000 completed.\nEpisode 25300/50000 completed.\nEpisode 25400/50000 completed.\nEpisode 25500/50000 completed.\nEpisode 25600/50000 completed.\nEpisode 25700/50000 completed.\nEpisode 25800/50000 completed.\nEpisode 25900/50000 completed.\nEpisode 26000/50000 completed.\nEpisode 26100/50000 completed.\nEpisode 26200/50000 completed.\nEpisode 26300/50000 completed.\nEpisode 26400/50000 completed.\nEpisode 26500/50000 completed.\nEpisode 26600/50000 completed.\nEpisode 26700/50000 completed.\nEpisode 26800/50000 completed.\nEpisode 26900/50000 completed.\nEpisode 27000/50000 completed.\nEpisode 27100/50000 completed.\nEpisode 27200/50000 completed.\nEpisode 27300/50000 completed.\nEpisode 27400/50000 completed.\nEpisode 27500/50000 completed.\nEpisode 27600/50000 completed.\nEpisode 27700/50000 completed.\nEpisode 27800/50000 completed.\nEpisode 27900/50000 completed.\nEpisode 28000/50000 completed.\nEpisode 28100/50000 completed.\nEpisode 28200/50000 completed.\nEpisode 28300/50000 completed.\nEpisode 28400/50000 completed.\nEpisode 28500/50000 completed.\nEpisode 28600/50000 completed.\nEpisode 28700/50000 completed.\nEpisode 28800/50000 completed.\nEpisode 28900/50000 completed.\nEpisode 29000/50000 completed.\nEpisode 29100/50000 completed.\nEpisode 29200/50000 completed.\nEpisode 29300/50000 completed.\nEpisode 29400/50000 completed.\nEpisode 29500/50000 completed.\nEpisode 29600/50000 completed.\nEpisode 29700/50000 completed.\nEpisode 29800/50000 completed.\nEpisode 29900/50000 completed.\nEpisode 30000/50000 completed.\nEpisode 30100/50000 completed.\nEpisode 30200/50000 completed.\nEpisode 30300/50000 completed.\nEpisode 30400/50000 completed.\nEpisode 30500/50000 completed.\nEpisode 30600/50000 completed.\nEpisode 30700/50000 completed.\nEpisode 30800/50000 completed.\nEpisode 30900/50000 completed.\nEpisode 31000/50000 completed.\nEpisode 31100/50000 completed.\nEpisode 31200/50000 completed.\nEpisode 31300/50000 completed.\nEpisode 31400/50000 completed.\nEpisode 31500/50000 completed.\nEpisode 31600/50000 completed.\nEpisode 31700/50000 completed.\nEpisode 31800/50000 completed.\nEpisode 31900/50000 completed.\nEpisode 32000/50000 completed.\nEpisode 32100/50000 completed.\nEpisode 32200/50000 completed.\nEpisode 32300/50000 completed.\nEpisode 32400/50000 completed.\nEpisode 32500/50000 completed.\nEpisode 32600/50000 completed.\nEpisode 32700/50000 completed.\nEpisode 32800/50000 completed.\nEpisode 32900/50000 completed.\nEpisode 33000/50000 completed.\nEpisode 33100/50000 completed.\nEpisode 33200/50000 completed.\nEpisode 33300/50000 completed.\nEpisode 33400/50000 completed.\nEpisode 33500/50000 completed.\nEpisode 33600/50000 completed.\nEpisode 33700/50000 completed.\nEpisode 33800/50000 completed.\nEpisode 33900/50000 completed.\nEpisode 34000/50000 completed.\nEpisode 34100/50000 completed.\nEpisode 34200/50000 completed.\nEpisode 34300/50000 completed.\nEpisode 34400/50000 completed.\nEpisode 34500/50000 completed.\nEpisode 34600/50000 completed.\nEpisode 34700/50000 completed.\nEpisode 34800/50000 completed.\nEpisode 34900/50000 completed.\nEpisode 35000/50000 completed.\nEpisode 35100/50000 completed.\nEpisode 35200/50000 completed.\nEpisode 35300/50000 completed.\nEpisode 35400/50000 completed.\nEpisode 35500/50000 completed.\nEpisode 35600/50000 completed.\nEpisode 35700/50000 completed.\nEpisode 35800/50000 completed.\nEpisode 35900/50000 completed.\nEpisode 36000/50000 completed.\nEpisode 36100/50000 completed.\nEpisode 36200/50000 completed.\nEpisode 36300/50000 completed.\nEpisode 36400/50000 completed.\nEpisode 36500/50000 completed.\nEpisode 36600/50000 completed.\nEpisode 36700/50000 completed.\nEpisode 36800/50000 completed.\nEpisode 36900/50000 completed.\nEpisode 37000/50000 completed.\nEpisode 37100/50000 completed.\nEpisode 37200/50000 completed.\nEpisode 37300/50000 completed.\nEpisode 37400/50000 completed.\nEpisode 37500/50000 completed.\nEpisode 37600/50000 completed.\nEpisode 37700/50000 completed.\nEpisode 37800/50000 completed.\nEpisode 37900/50000 completed.\nEpisode 38000/50000 completed.\nEpisode 38100/50000 completed.\nEpisode 38200/50000 completed.\nEpisode 38300/50000 completed.\nEpisode 38400/50000 completed.\nEpisode 38500/50000 completed.\nEpisode 38600/50000 completed.\nEpisode 38700/50000 completed.\nEpisode 38800/50000 completed.\nEpisode 38900/50000 completed.\nEpisode 39000/50000 completed.\nEpisode 39100/50000 completed.\nEpisode 39200/50000 completed.\nEpisode 39300/50000 completed.\nEpisode 39400/50000 completed.\nEpisode 39500/50000 completed.\nEpisode 39600/50000 completed.\nEpisode 39700/50000 completed.\nEpisode 39800/50000 completed.\nEpisode 39900/50000 completed.\nEpisode 40000/50000 completed.\nEpisode 40100/50000 completed.\nEpisode 40200/50000 completed.\nEpisode 40300/50000 completed.\nEpisode 40400/50000 completed.\nEpisode 40500/50000 completed.\nEpisode 40600/50000 completed.\nEpisode 40700/50000 completed.\nEpisode 40800/50000 completed.\nEpisode 40900/50000 completed.\nEpisode 41000/50000 completed.\nEpisode 41100/50000 completed.\nEpisode 41200/50000 completed.\nEpisode 41300/50000 completed.\nEpisode 41400/50000 completed.\nEpisode 41500/50000 completed.\nEpisode 41600/50000 completed.\nEpisode 41700/50000 completed.\nEpisode 41800/50000 completed.\nEpisode 41900/50000 completed.\nEpisode 42000/50000 completed.\nEpisode 42100/50000 completed.\nEpisode 42200/50000 completed.\nEpisode 42300/50000 completed.\nEpisode 42400/50000 completed.\nEpisode 42500/50000 completed.\nEpisode 42600/50000 completed.\nEpisode 42700/50000 completed.\nEpisode 42800/50000 completed.\nEpisode 42900/50000 completed.\nEpisode 43000/50000 completed.\nEpisode 43100/50000 completed.\nEpisode 43200/50000 completed.\nEpisode 43300/50000 completed.\nEpisode 43400/50000 completed.\nEpisode 43500/50000 completed.\nEpisode 43600/50000 completed.\nEpisode 43700/50000 completed.\nEpisode 43800/50000 completed.\nEpisode 43900/50000 completed.\nEpisode 44000/50000 completed.\nEpisode 44100/50000 completed.\nEpisode 44200/50000 completed.\nEpisode 44300/50000 completed.\nEpisode 44400/50000 completed.\nEpisode 44500/50000 completed.\nEpisode 44600/50000 completed.\nEpisode 44700/50000 completed.\nEpisode 44800/50000 completed.\nEpisode 44900/50000 completed.\nEpisode 45000/50000 completed.\nEpisode 45100/50000 completed.\nEpisode 45200/50000 completed.\nEpisode 45300/50000 completed.\nEpisode 45400/50000 completed.\nEpisode 45500/50000 completed.\nEpisode 45600/50000 completed.\nEpisode 45700/50000 completed.\nEpisode 45800/50000 completed.\nEpisode 45900/50000 completed.\nEpisode 46000/50000 completed.\nEpisode 46100/50000 completed.\nEpisode 46200/50000 completed.\nEpisode 46300/50000 completed.\nEpisode 46400/50000 completed.\nEpisode 46500/50000 completed.\nEpisode 46600/50000 completed.\nEpisode 46700/50000 completed.\nEpisode 46800/50000 completed.\nEpisode 46900/50000 completed.\nEpisode 47000/50000 completed.\nEpisode 47100/50000 completed.\nEpisode 47200/50000 completed.\nEpisode 47300/50000 completed.\nEpisode 47400/50000 completed.\nEpisode 47500/50000 completed.\nEpisode 47600/50000 completed.\nEpisode 47700/50000 completed.\nEpisode 47800/50000 completed.\nEpisode 47900/50000 completed.\nEpisode 48000/50000 completed.\nEpisode 48100/50000 completed.\nEpisode 48200/50000 completed.\nEpisode 48300/50000 completed.\nEpisode 48400/50000 completed.\nEpisode 48500/50000 completed.\nEpisode 48600/50000 completed.\nEpisode 48700/50000 completed.\nEpisode 48800/50000 completed.\nEpisode 48900/50000 completed.\nEpisode 49000/50000 completed.\nEpisode 49100/50000 completed.\nEpisode 49200/50000 completed.\nEpisode 49300/50000 completed.\nEpisode 49400/50000 completed.\nEpisode 49500/50000 completed.\nEpisode 49600/50000 completed.\nEpisode 49700/50000 completed.\nEpisode 49800/50000 completed.\nEpisode 49900/50000 completed.\nEpisode 50000/50000 completed.\n\n\n\n\n\n\n\n\n\nWe can see that the policy is to move to the bottom right corner, as expected. We used a large amount of episodes to solve the environment due to its noise and nature. It is interesting to see that some arrows point in a different direction than the one expected from DP. It makes sense as it is a stochastic method and bases itself on the episodes. However, they lead towards the goal.\n\n\nCode\n# Print the final V, Q, and policy\n\nprint(\"State-Value Function V:\")\nprint(V_grid)\n\nprint(\"\\nAction-Value Function Q for a sample of States:\")\nsample_states = [0, env.observation_space.n // 2, env.observation_space.n - 1]\nfor s in sample_states:\n    state = env.index_to_state(s)\n    print(f\"State {state}:\")\n    for a in range(env.action_space.n):\n        print(f\"  Action {a}: Q = {Q[s, a]:.4f}\")\n\n\n\nState-Value Function V:\n[[58.25803313 59.56191653 55.48853265 53.87052704 58.63128279 70.91287603]\n [60.67715075 63.03554198 63.94042008 59.30406794 69.09515794 78.03798798]\n [61.59467685 66.12038698 68.5132383  71.42912142 77.84945497 91.7456878 ]\n [57.70221524 62.53665025 65.28309907 87.99560935 92.87975146 96.87437918]\n [48.06705244 58.91421393 87.99249069 94.1455398  97.01743387 99.48128026]\n [34.17391899 46.89419121 79.09874303 94.58663899 99.87349188 99.72059563]]\n\nAction-Value Function Q for a sample of States:\nState (0, 0):\n  Action 0: Q = 48.2422\n  Action 1: Q = 53.7663\n  Action 2: Q = 56.0690\n  Action 3: Q = 58.2580\nState (3, 0):\n  Action 0: Q = 56.4867\n  Action 1: Q = 57.7022\n  Action 2: Q = 47.3510\n  Action 3: Q = 52.4756\nState (5, 5):\n  Action 0: Q = 96.4308\n  Action 1: Q = 99.3141\n  Action 2: Q = 99.7206\n  Action 3: Q = 95.3593\n\n\n\n\nCode\n# Plot convergence plots\n\nV_history = np.array(solver.V_history)\nplt.figure(figsize=(12, 6))\nstates_to_plot = [0, env.observation_space.n // 2, env.observation_space.n - 1]\nfor s in states_to_plot:\n    plt.plot(V_history[:, s], label=f\"State {s}\")\nplt.xlabel('Episodes')\nplt.ylabel('V(s)')\nplt.title('Convergence of State-Value Function V')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nQ_history = np.array(solver.Q_history)\nplt.figure(figsize=(12, 6))\nactions = [0, 1, 2, 3]\nfor s in states_to_plot:\n    for a in actions:\n        plt.plot(Q_history[:, s, a], label=f\"Q({s},{a})\")\nplt.xlabel('Episodes')\nplt.ylabel('Q(s,a)')\nplt.title('Convergence of Action-Value Function Q')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot convergence of policy for sample states\n\npolicy_history = np.array(solver.policy_history)\nplt.figure(figsize=(12, 6))\nfor s in states_to_plot:\n    plt.plot(policy_history[:, s], label=f\"Policy for State {s}\")\nplt.xlabel('Episodes')\nplt.ylabel('Action')\nplt.title('Convergence of Policy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nAs with the boat game, the convergence is much more noisy than DP, but it still converges. It makes sense as we are using MC. The noise in the environment also makes it harder to solve."
  },
  {
    "objectID": "Tabula_RL.html#geosearch-solved-with-mc",
    "href": "Tabula_RL.html#geosearch-solved-with-mc",
    "title": "Demonstration of the Package created",
    "section": "Geosearch Solved with MC",
    "text": "Geosearch Solved with MC\nWe will solve the geosearch environment using Monte Carlo with On-Policy First-Visit Monte Carlo Control. We will also extend the number of maximum steps per iteration to let the agent move accordingly due to the large environment.\n\n\nCode\n# Create the environment\nenv = GeosearchEnv(render=False)\n\n# Solver\nsolver = MCSolver(env, gamma=0.99, epsilon=0.1, exploring_starts=False, episodes=50000, max_steps_per_episode=500)\n\n\n# Solve\npolicy = solver.run()\n\n# Get the final V and Q\nV = solver.V_history[-1]\nQ = solver.Q_history[-1]\n\nV_grid = V.reshape((env.grid_size, env.grid_size))\n\n# Plot the optimal policy\naction_symbols = {0: '↑', 1: '↓', 2: '←', 3: '→'}\nplotter = PolicyPlotter_MC_TD(grid_size=(env.grid_size, env.grid_size), \n                        policy=policy, \n                        walls=[],\n                        terminal_states=[],\n                        action_symbols=action_symbols)\n\nplotter.plot()\n\n\nEpisode 100/50000 completed.\nEpisode 200/50000 completed.\nEpisode 300/50000 completed.\nEpisode 400/50000 completed.\nEpisode 500/50000 completed.\nEpisode 600/50000 completed.\nEpisode 700/50000 completed.\nEpisode 800/50000 completed.\nEpisode 900/50000 completed.\nEpisode 1000/50000 completed.\nEpisode 1100/50000 completed.\nEpisode 1200/50000 completed.\nEpisode 1300/50000 completed.\nEpisode 1400/50000 completed.\nEpisode 1500/50000 completed.\nEpisode 1600/50000 completed.\nEpisode 1700/50000 completed.\nEpisode 1800/50000 completed.\nEpisode 1900/50000 completed.\nEpisode 2000/50000 completed.\nEpisode 2100/50000 completed.\nEpisode 2200/50000 completed.\nEpisode 2300/50000 completed.\nEpisode 2400/50000 completed.\nEpisode 2500/50000 completed.\nEpisode 2600/50000 completed.\nEpisode 2700/50000 completed.\nEpisode 2800/50000 completed.\nEpisode 2900/50000 completed.\nEpisode 3000/50000 completed.\nEpisode 3100/50000 completed.\nEpisode 3200/50000 completed.\nEpisode 3300/50000 completed.\nEpisode 3400/50000 completed.\nEpisode 3500/50000 completed.\nEpisode 3600/50000 completed.\nEpisode 3700/50000 completed.\nEpisode 3800/50000 completed.\nEpisode 3900/50000 completed.\nEpisode 4000/50000 completed.\nEpisode 4100/50000 completed.\nEpisode 4200/50000 completed.\nEpisode 4300/50000 completed.\nEpisode 4400/50000 completed.\nEpisode 4500/50000 completed.\nEpisode 4600/50000 completed.\nEpisode 4700/50000 completed.\nEpisode 4800/50000 completed.\nEpisode 4900/50000 completed.\nEpisode 5000/50000 completed.\nEpisode 5100/50000 completed.\nEpisode 5200/50000 completed.\nEpisode 5300/50000 completed.\nEpisode 5400/50000 completed.\nEpisode 5500/50000 completed.\nEpisode 5600/50000 completed.\nEpisode 5700/50000 completed.\nEpisode 5800/50000 completed.\nEpisode 5900/50000 completed.\nEpisode 6000/50000 completed.\nEpisode 6100/50000 completed.\nEpisode 6200/50000 completed.\nEpisode 6300/50000 completed.\nEpisode 6400/50000 completed.\nEpisode 6500/50000 completed.\nEpisode 6600/50000 completed.\nEpisode 6700/50000 completed.\nEpisode 6800/50000 completed.\nEpisode 6900/50000 completed.\nEpisode 7000/50000 completed.\nEpisode 7100/50000 completed.\nEpisode 7200/50000 completed.\nEpisode 7300/50000 completed.\nEpisode 7400/50000 completed.\nEpisode 7500/50000 completed.\nEpisode 7600/50000 completed.\nEpisode 7700/50000 completed.\nEpisode 7800/50000 completed.\nEpisode 7900/50000 completed.\nEpisode 8000/50000 completed.\nEpisode 8100/50000 completed.\nEpisode 8200/50000 completed.\nEpisode 8300/50000 completed.\nEpisode 8400/50000 completed.\nEpisode 8500/50000 completed.\nEpisode 8600/50000 completed.\nEpisode 8700/50000 completed.\nEpisode 8800/50000 completed.\nEpisode 8900/50000 completed.\nEpisode 9000/50000 completed.\nEpisode 9100/50000 completed.\nEpisode 9200/50000 completed.\nEpisode 9300/50000 completed.\nEpisode 9400/50000 completed.\nEpisode 9500/50000 completed.\nEpisode 9600/50000 completed.\nEpisode 9700/50000 completed.\nEpisode 9800/50000 completed.\nEpisode 9900/50000 completed.\nEpisode 10000/50000 completed.\nEpisode 10100/50000 completed.\nEpisode 10200/50000 completed.\nEpisode 10300/50000 completed.\nEpisode 10400/50000 completed.\nEpisode 10500/50000 completed.\nEpisode 10600/50000 completed.\nEpisode 10700/50000 completed.\nEpisode 10800/50000 completed.\nEpisode 10900/50000 completed.\nEpisode 11000/50000 completed.\nEpisode 11100/50000 completed.\nEpisode 11200/50000 completed.\nEpisode 11300/50000 completed.\nEpisode 11400/50000 completed.\nEpisode 11500/50000 completed.\nEpisode 11600/50000 completed.\nEpisode 11700/50000 completed.\nEpisode 11800/50000 completed.\nEpisode 11900/50000 completed.\nEpisode 12000/50000 completed.\nEpisode 12100/50000 completed.\nEpisode 12200/50000 completed.\nEpisode 12300/50000 completed.\nEpisode 12400/50000 completed.\nEpisode 12500/50000 completed.\nEpisode 12600/50000 completed.\nEpisode 12700/50000 completed.\nEpisode 12800/50000 completed.\nEpisode 12900/50000 completed.\nEpisode 13000/50000 completed.\nEpisode 13100/50000 completed.\nEpisode 13200/50000 completed.\nEpisode 13300/50000 completed.\nEpisode 13400/50000 completed.\nEpisode 13500/50000 completed.\nEpisode 13600/50000 completed.\nEpisode 13700/50000 completed.\nEpisode 13800/50000 completed.\nEpisode 13900/50000 completed.\nEpisode 14000/50000 completed.\nEpisode 14100/50000 completed.\nEpisode 14200/50000 completed.\nEpisode 14300/50000 completed.\nEpisode 14400/50000 completed.\nEpisode 14500/50000 completed.\nEpisode 14600/50000 completed.\nEpisode 14700/50000 completed.\nEpisode 14800/50000 completed.\nEpisode 14900/50000 completed.\nEpisode 15000/50000 completed.\nEpisode 15100/50000 completed.\nEpisode 15200/50000 completed.\nEpisode 15300/50000 completed.\nEpisode 15400/50000 completed.\nEpisode 15500/50000 completed.\nEpisode 15600/50000 completed.\nEpisode 15700/50000 completed.\nEpisode 15800/50000 completed.\nEpisode 15900/50000 completed.\nEpisode 16000/50000 completed.\nEpisode 16100/50000 completed.\nEpisode 16200/50000 completed.\nEpisode 16300/50000 completed.\nEpisode 16400/50000 completed.\nEpisode 16500/50000 completed.\nEpisode 16600/50000 completed.\nEpisode 16700/50000 completed.\nEpisode 16800/50000 completed.\nEpisode 16900/50000 completed.\nEpisode 17000/50000 completed.\nEpisode 17100/50000 completed.\nEpisode 17200/50000 completed.\nEpisode 17300/50000 completed.\nEpisode 17400/50000 completed.\nEpisode 17500/50000 completed.\nEpisode 17600/50000 completed.\nEpisode 17700/50000 completed.\nEpisode 17800/50000 completed.\nEpisode 17900/50000 completed.\nEpisode 18000/50000 completed.\nEpisode 18100/50000 completed.\nEpisode 18200/50000 completed.\nEpisode 18300/50000 completed.\nEpisode 18400/50000 completed.\nEpisode 18500/50000 completed.\nEpisode 18600/50000 completed.\nEpisode 18700/50000 completed.\nEpisode 18800/50000 completed.\nEpisode 18900/50000 completed.\nEpisode 19000/50000 completed.\nEpisode 19100/50000 completed.\nEpisode 19200/50000 completed.\nEpisode 19300/50000 completed.\nEpisode 19400/50000 completed.\nEpisode 19500/50000 completed.\nEpisode 19600/50000 completed.\nEpisode 19700/50000 completed.\nEpisode 19800/50000 completed.\nEpisode 19900/50000 completed.\nEpisode 20000/50000 completed.\nEpisode 20100/50000 completed.\nEpisode 20200/50000 completed.\nEpisode 20300/50000 completed.\nEpisode 20400/50000 completed.\nEpisode 20500/50000 completed.\nEpisode 20600/50000 completed.\nEpisode 20700/50000 completed.\nEpisode 20800/50000 completed.\nEpisode 20900/50000 completed.\nEpisode 21000/50000 completed.\nEpisode 21100/50000 completed.\nEpisode 21200/50000 completed.\nEpisode 21300/50000 completed.\nEpisode 21400/50000 completed.\nEpisode 21500/50000 completed.\nEpisode 21600/50000 completed.\nEpisode 21700/50000 completed.\nEpisode 21800/50000 completed.\nEpisode 21900/50000 completed.\nEpisode 22000/50000 completed.\nEpisode 22100/50000 completed.\nEpisode 22200/50000 completed.\nEpisode 22300/50000 completed.\nEpisode 22400/50000 completed.\nEpisode 22500/50000 completed.\nEpisode 22600/50000 completed.\nEpisode 22700/50000 completed.\nEpisode 22800/50000 completed.\nEpisode 22900/50000 completed.\nEpisode 23000/50000 completed.\nEpisode 23100/50000 completed.\nEpisode 23200/50000 completed.\nEpisode 23300/50000 completed.\nEpisode 23400/50000 completed.\nEpisode 23500/50000 completed.\nEpisode 23600/50000 completed.\nEpisode 23700/50000 completed.\nEpisode 23800/50000 completed.\nEpisode 23900/50000 completed.\nEpisode 24000/50000 completed.\nEpisode 24100/50000 completed.\nEpisode 24200/50000 completed.\nEpisode 24300/50000 completed.\nEpisode 24400/50000 completed.\nEpisode 24500/50000 completed.\nEpisode 24600/50000 completed.\nEpisode 24700/50000 completed.\nEpisode 24800/50000 completed.\nEpisode 24900/50000 completed.\nEpisode 25000/50000 completed.\nEpisode 25100/50000 completed.\nEpisode 25200/50000 completed.\nEpisode 25300/50000 completed.\nEpisode 25400/50000 completed.\nEpisode 25500/50000 completed.\nEpisode 25600/50000 completed.\nEpisode 25700/50000 completed.\nEpisode 25800/50000 completed.\nEpisode 25900/50000 completed.\nEpisode 26000/50000 completed.\nEpisode 26100/50000 completed.\nEpisode 26200/50000 completed.\nEpisode 26300/50000 completed.\nEpisode 26400/50000 completed.\nEpisode 26500/50000 completed.\nEpisode 26600/50000 completed.\nEpisode 26700/50000 completed.\nEpisode 26800/50000 completed.\nEpisode 26900/50000 completed.\nEpisode 27000/50000 completed.\nEpisode 27100/50000 completed.\nEpisode 27200/50000 completed.\nEpisode 27300/50000 completed.\nEpisode 27400/50000 completed.\nEpisode 27500/50000 completed.\nEpisode 27600/50000 completed.\nEpisode 27700/50000 completed.\nEpisode 27800/50000 completed.\nEpisode 27900/50000 completed.\nEpisode 28000/50000 completed.\nEpisode 28100/50000 completed.\nEpisode 28200/50000 completed.\nEpisode 28300/50000 completed.\nEpisode 28400/50000 completed.\nEpisode 28500/50000 completed.\nEpisode 28600/50000 completed.\nEpisode 28700/50000 completed.\nEpisode 28800/50000 completed.\nEpisode 28900/50000 completed.\nEpisode 29000/50000 completed.\nEpisode 29100/50000 completed.\nEpisode 29200/50000 completed.\nEpisode 29300/50000 completed.\nEpisode 29400/50000 completed.\nEpisode 29500/50000 completed.\nEpisode 29600/50000 completed.\nEpisode 29700/50000 completed.\nEpisode 29800/50000 completed.\nEpisode 29900/50000 completed.\nEpisode 30000/50000 completed.\nEpisode 30100/50000 completed.\nEpisode 30200/50000 completed.\nEpisode 30300/50000 completed.\nEpisode 30400/50000 completed.\nEpisode 30500/50000 completed.\nEpisode 30600/50000 completed.\nEpisode 30700/50000 completed.\nEpisode 30800/50000 completed.\nEpisode 30900/50000 completed.\nEpisode 31000/50000 completed.\nEpisode 31100/50000 completed.\nEpisode 31200/50000 completed.\nEpisode 31300/50000 completed.\nEpisode 31400/50000 completed.\nEpisode 31500/50000 completed.\nEpisode 31600/50000 completed.\nEpisode 31700/50000 completed.\nEpisode 31800/50000 completed.\nEpisode 31900/50000 completed.\nEpisode 32000/50000 completed.\nEpisode 32100/50000 completed.\nEpisode 32200/50000 completed.\nEpisode 32300/50000 completed.\nEpisode 32400/50000 completed.\nEpisode 32500/50000 completed.\nEpisode 32600/50000 completed.\nEpisode 32700/50000 completed.\nEpisode 32800/50000 completed.\nEpisode 32900/50000 completed.\nEpisode 33000/50000 completed.\nEpisode 33100/50000 completed.\nEpisode 33200/50000 completed.\nEpisode 33300/50000 completed.\nEpisode 33400/50000 completed.\nEpisode 33500/50000 completed.\nEpisode 33600/50000 completed.\nEpisode 33700/50000 completed.\nEpisode 33800/50000 completed.\nEpisode 33900/50000 completed.\nEpisode 34000/50000 completed.\nEpisode 34100/50000 completed.\nEpisode 34200/50000 completed.\nEpisode 34300/50000 completed.\nEpisode 34400/50000 completed.\nEpisode 34500/50000 completed.\nEpisode 34600/50000 completed.\nEpisode 34700/50000 completed.\nEpisode 34800/50000 completed.\nEpisode 34900/50000 completed.\nEpisode 35000/50000 completed.\nEpisode 35100/50000 completed.\nEpisode 35200/50000 completed.\nEpisode 35300/50000 completed.\nEpisode 35400/50000 completed.\nEpisode 35500/50000 completed.\nEpisode 35600/50000 completed.\nEpisode 35700/50000 completed.\nEpisode 35800/50000 completed.\nEpisode 35900/50000 completed.\nEpisode 36000/50000 completed.\nEpisode 36100/50000 completed.\nEpisode 36200/50000 completed.\nEpisode 36300/50000 completed.\nEpisode 36400/50000 completed.\nEpisode 36500/50000 completed.\nEpisode 36600/50000 completed.\nEpisode 36700/50000 completed.\nEpisode 36800/50000 completed.\nEpisode 36900/50000 completed.\nEpisode 37000/50000 completed.\nEpisode 37100/50000 completed.\nEpisode 37200/50000 completed.\nEpisode 37300/50000 completed.\nEpisode 37400/50000 completed.\nEpisode 37500/50000 completed.\nEpisode 37600/50000 completed.\nEpisode 37700/50000 completed.\nEpisode 37800/50000 completed.\nEpisode 37900/50000 completed.\nEpisode 38000/50000 completed.\nEpisode 38100/50000 completed.\nEpisode 38200/50000 completed.\nEpisode 38300/50000 completed.\nEpisode 38400/50000 completed.\nEpisode 38500/50000 completed.\nEpisode 38600/50000 completed.\nEpisode 38700/50000 completed.\nEpisode 38800/50000 completed.\nEpisode 38900/50000 completed.\nEpisode 39000/50000 completed.\nEpisode 39100/50000 completed.\nEpisode 39200/50000 completed.\nEpisode 39300/50000 completed.\nEpisode 39400/50000 completed.\nEpisode 39500/50000 completed.\nEpisode 39600/50000 completed.\nEpisode 39700/50000 completed.\nEpisode 39800/50000 completed.\nEpisode 39900/50000 completed.\nEpisode 40000/50000 completed.\nEpisode 40100/50000 completed.\nEpisode 40200/50000 completed.\nEpisode 40300/50000 completed.\nEpisode 40400/50000 completed.\nEpisode 40500/50000 completed.\nEpisode 40600/50000 completed.\nEpisode 40700/50000 completed.\nEpisode 40800/50000 completed.\nEpisode 40900/50000 completed.\nEpisode 41000/50000 completed.\nEpisode 41100/50000 completed.\nEpisode 41200/50000 completed.\nEpisode 41300/50000 completed.\nEpisode 41400/50000 completed.\nEpisode 41500/50000 completed.\nEpisode 41600/50000 completed.\nEpisode 41700/50000 completed.\nEpisode 41800/50000 completed.\nEpisode 41900/50000 completed.\nEpisode 42000/50000 completed.\nEpisode 42100/50000 completed.\nEpisode 42200/50000 completed.\nEpisode 42300/50000 completed.\nEpisode 42400/50000 completed.\nEpisode 42500/50000 completed.\nEpisode 42600/50000 completed.\nEpisode 42700/50000 completed.\nEpisode 42800/50000 completed.\nEpisode 42900/50000 completed.\nEpisode 43000/50000 completed.\nEpisode 43100/50000 completed.\nEpisode 43200/50000 completed.\nEpisode 43300/50000 completed.\nEpisode 43400/50000 completed.\nEpisode 43500/50000 completed.\nEpisode 43600/50000 completed.\nEpisode 43700/50000 completed.\nEpisode 43800/50000 completed.\nEpisode 43900/50000 completed.\nEpisode 44000/50000 completed.\nEpisode 44100/50000 completed.\nEpisode 44200/50000 completed.\nEpisode 44300/50000 completed.\nEpisode 44400/50000 completed.\nEpisode 44500/50000 completed.\nEpisode 44600/50000 completed.\nEpisode 44700/50000 completed.\nEpisode 44800/50000 completed.\nEpisode 44900/50000 completed.\nEpisode 45000/50000 completed.\nEpisode 45100/50000 completed.\nEpisode 45200/50000 completed.\nEpisode 45300/50000 completed.\nEpisode 45400/50000 completed.\nEpisode 45500/50000 completed.\nEpisode 45600/50000 completed.\nEpisode 45700/50000 completed.\nEpisode 45800/50000 completed.\nEpisode 45900/50000 completed.\nEpisode 46000/50000 completed.\nEpisode 46100/50000 completed.\nEpisode 46200/50000 completed.\nEpisode 46300/50000 completed.\nEpisode 46400/50000 completed.\nEpisode 46500/50000 completed.\nEpisode 46600/50000 completed.\nEpisode 46700/50000 completed.\nEpisode 46800/50000 completed.\nEpisode 46900/50000 completed.\nEpisode 47000/50000 completed.\nEpisode 47100/50000 completed.\nEpisode 47200/50000 completed.\nEpisode 47300/50000 completed.\nEpisode 47400/50000 completed.\nEpisode 47500/50000 completed.\nEpisode 47600/50000 completed.\nEpisode 47700/50000 completed.\nEpisode 47800/50000 completed.\nEpisode 47900/50000 completed.\nEpisode 48000/50000 completed.\nEpisode 48100/50000 completed.\nEpisode 48200/50000 completed.\nEpisode 48300/50000 completed.\nEpisode 48400/50000 completed.\nEpisode 48500/50000 completed.\nEpisode 48600/50000 completed.\nEpisode 48700/50000 completed.\nEpisode 48800/50000 completed.\nEpisode 48900/50000 completed.\nEpisode 49000/50000 completed.\nEpisode 49100/50000 completed.\nEpisode 49200/50000 completed.\nEpisode 49300/50000 completed.\nEpisode 49400/50000 completed.\nEpisode 49500/50000 completed.\nEpisode 49600/50000 completed.\nEpisode 49700/50000 completed.\nEpisode 49800/50000 completed.\nEpisode 49900/50000 completed.\nEpisode 50000/50000 completed.\n\n\n\n\n\n\n\n\n\nAs expected, the policy is to move towards the “heat” points (gold and water) as desired. However, we can see that the policy is not as clear as in the other environments and it seems much more noisy compared to tthe results of dynamic programming. This makes sense due to the nature of the environment and the inherent large space counting with the episodes of Monte Carlo. We may need more episodes to solve the environment more effectively but it is still converging.\n\n\nCode\n# Print Action-Value Function Q for a sample of states\n\nprint(\"\\nAction-Value Function Q for Sample States:\")\nsample_states = [(0, 0), (12, 12), (24, 24), (20, 20), (10, 10)]\nfor state in sample_states:\n    s_index = env.state_to_index(state)\n    print(f\"State {state}:\")\n    for a in range(env.action_space.n):\n        print(f\"  Action {a}: Q = {Q[s_index, a]:.4f}\")\n\n\n\nAction-Value Function Q for Sample States:\nState (0, 0):\n  Action 0: Q = 1.2823\n  Action 1: Q = 0.5216\n  Action 2: Q = 1.0751\n  Action 3: Q = 2.1665\nState (12, 12):\n  Action 0: Q = 1.1946\n  Action 1: Q = 0.8690\n  Action 2: Q = 1.4868\n  Action 3: Q = 0.7418\nState (24, 24):\n  Action 0: Q = 4.1296\n  Action 1: Q = 0.0000\n  Action 2: Q = 3.9297\n  Action 3: Q = 0.0000\nState (20, 20):\n  Action 0: Q = 2.6196\n  Action 1: Q = 2.6195\n  Action 2: Q = 2.6196\n  Action 3: Q = 2.6194\nState (10, 10):\n  Action 0: Q = 0.9104\n  Action 1: Q = 0.9205\n  Action 2: Q = 0.9211\n  Action 3: Q = 2.0670\n\n\n\n\nCode\n# Plot the State-Value Function as a heatmap\n\nplt.figure(figsize=(8, 6))\nplt.imshow(V_grid, cmap='hot', interpolation='nearest')\nplt.colorbar(label='V(s)')\nplt.title('State-Value Function V')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.gca().invert_yaxis()\nplt.show()\n\n# Plot convergence plots\n\nV_history = np.array(solver.V_history)\nplt.figure(figsize=(10, 6))\nstate_indices = [env.state_to_index(s) for s in sample_states]\nfor idx, s in enumerate(state_indices):\n    plt.plot(V_history[:, s], label=f\"State {sample_states[idx]}\")\nplt.xlabel('Episodes')\nplt.ylabel('V(s)')\nplt.title('Convergence of State-Value Function V')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nQ_history = np.array(solver.Q_history)\nplt.figure(figsize=(10, 6))\nactions = [0, 1, 2, 3]\nfor idx, s in enumerate(state_indices):\n    for a in actions:\n        plt.plot(Q_history[:, s, a], label=f\"Q({sample_states[idx]}, {a})\")\nplt.xlabel('Episodes')\nplt.ylabel('Q(s,a)')\nplt.title('Convergence of Action-Value Function Q')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n\npolicy_history = np.array(solver.policy_history)\nplt.figure(figsize=(10, 6))\nfor idx, s in enumerate(state_indices):\n    plt.plot(policy_history[:, s], label=f\"Policy for State {sample_states[idx]}\")\nplt.xlabel('Episodes')\nplt.ylabel('Action')\nplt.title('Convergence of Policy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, as with all others, the convergence is much more noisy than DP, but it still converges. It makes sense as we are using MC."
  },
  {
    "objectID": "Tabula_RL.html#boat-solved-with-td",
    "href": "Tabula_RL.html#boat-solved-with-td",
    "title": "Demonstration of the Package created",
    "section": "Boat Solved with TD",
    "text": "Boat Solved with TD\nWe will solve the boat environment using Temporal Difference with SARSA.\n\n\nCode\n# Create the environment\nenv = BoatEnv(render=False)\n\n# Solver\nsarsa_solver = TDSolver(env, gamma=0.99, alpha=0.1, epsilon=0.1, episodes=500, method=\"sarsa\")\n\n# Solve\npolicy = sarsa_solver.run()\n\n# Get the final V and Q for SARSA\nV_sarsa = sarsa_solver.V_history[-1]\nQ_sarsa = sarsa_solver.Q_history[-1]\n\n\n# Plot the optimal policy\nboat_action_symbols = {0: '←', 1: '→'}\nplotter = PolicyPlotter_MC_TD((1, 2), policy, action_symbols=boat_action_symbols)\nplotter.plot()\n\n\nEpisode 100/500 completed.\nEpisode 200/500 completed.\nEpisode 300/500 completed.\nEpisode 400/500 completed.\nEpisode 500/500 completed.\n\n\n\n\n\n\n\n\n\nWe can see that the policy is to move to the right, as expected. The policy is very clear and the environment is very simple. Less than 500 episodes were needed to solve the environment.\n\n\nCode\n# Print the final V, Q, and policy\n\nprint(\"State-Value Function V (SARSA):\")\nfor s in range(env.observation_space.n):\n    print(f\"State {s}: V = {V_sarsa[s]:.4f}\")\n\nprint(\"\\nAction-Value Function Q (SARSA):\")\nfor s in range(env.observation_space.n):\n    print(f\"State {s}:\")\n    for a in range(env.action_space.n):\n        print(f\"  Action {a}: Q = {Q_sarsa[s, a]:.4f}\")\n\nprint(\"\\nOptimal Policy (SARSA):\")\nfor s in range(env.observation_space.n):\n    action = policy.get(s, 0)\n    action_name = \"West\" if action == 0 else \"East\"\n    print(f\"State {s}: Take action '{action_name}'\")\n\n\nState-Value Function V (SARSA):\nState 0: V = 109.3610\nState 1: V = 111.0091\n\nAction-Value Function Q (SARSA):\nState 0:\n  Action 0: Q = 108.5292\n  Action 1: Q = 109.3610\nState 1:\n  Action 0: Q = 108.4314\n  Action 1: Q = 111.0091\n\nOptimal Policy (SARSA):\nState 0: Take action 'East'\nState 1: Take action 'East'\n\n\n\n\nCode\n# Plot convergence plots\n\nV_history_sarsa = np.array(sarsa_solver.V_history)\nplt.figure(figsize=(8, 5))\nfor s in range(env.observation_space.n):\n    plt.plot(V_history_sarsa[:, s], label=f\"State {s}\")\nplt.xlabel('Episodes')\nplt.ylabel('V(s)')\nplt.title('Convergence of State-Value Function V (SARSA)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nQ_history_sarsa = np.array(sarsa_solver.Q_history)\nplt.figure(figsize=(8, 5))\nfor s in range(env.observation_space.n):\n    for a in range(env.action_space.n):\n        plt.plot(Q_history_sarsa[:, s, a], label=f\"Q({s},{a})\")\nplt.xlabel('Episodes')\nplt.ylabel('Q(s,a)')\nplt.title('Convergence of Action-Value Function Q (SARSA)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\npolicy_history_sarsa = np.array(sarsa_solver.policy_history)\nplt.figure(figsize=(8, 5))\nfor s in range(env.observation_space.n):\n    plt.plot(policy_history_sarsa[:, s], label=f\"Policy for State {s}\")\nplt.xlabel('Episodes')\nplt.ylabel('Action')\nplt.title('Convergence of Policy (SARSA)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile the convergence is more noisy than DP, it is less noisy than MC. This is interesting as expected due to the nature of TD."
  },
  {
    "objectID": "Tabula_RL.html#gridworld-solved-with-td",
    "href": "Tabula_RL.html#gridworld-solved-with-td",
    "title": "Demonstration of the Package created",
    "section": "Gridworld Solved with TD",
    "text": "Gridworld Solved with TD\nWe will solve the gridworld environment using Temporal Difference with Q-learning.\n\n\nCode\n# Create the environment\nenv = GridWorldEnv(render=False)\n\n# Solver\nsolver = TDSolver(env, gamma=0.99, alpha=0.1, epsilon=0.1, episodes=50000, method=\"q_learning\")\n\n# Solve\npolicy = solver.run()\n\n# Get the final V and Q\nV = solver.V_history[-1]\nQ = solver.Q_history[-1]\nV_grid = V.reshape(env.grid_size)\n\n# terminal states for the plotter\nterminal_coords = list(env.terminal_states.keys())\n\n# Plot the optimal policy\nplotter = PolicyPlotter_MC_TD(env.grid_size, policy, walls=list(env.walls), terminal_states=terminal_coords)\nplotter.plot()\n\n\nEpisode 100/50000 completed.\nEpisode 200/50000 completed.\nEpisode 300/50000 completed.\nEpisode 400/50000 completed.\nEpisode 500/50000 completed.\nEpisode 600/50000 completed.\nEpisode 700/50000 completed.\nEpisode 800/50000 completed.\nEpisode 900/50000 completed.\nEpisode 1000/50000 completed.\nEpisode 1100/50000 completed.\nEpisode 1200/50000 completed.\nEpisode 1300/50000 completed.\nEpisode 1400/50000 completed.\nEpisode 1500/50000 completed.\nEpisode 1600/50000 completed.\nEpisode 1700/50000 completed.\nEpisode 1800/50000 completed.\nEpisode 1900/50000 completed.\nEpisode 2000/50000 completed.\nEpisode 2100/50000 completed.\nEpisode 2200/50000 completed.\nEpisode 2300/50000 completed.\nEpisode 2400/50000 completed.\nEpisode 2500/50000 completed.\nEpisode 2600/50000 completed.\nEpisode 2700/50000 completed.\nEpisode 2800/50000 completed.\nEpisode 2900/50000 completed.\nEpisode 3000/50000 completed.\nEpisode 3100/50000 completed.\nEpisode 3200/50000 completed.\nEpisode 3300/50000 completed.\nEpisode 3400/50000 completed.\nEpisode 3500/50000 completed.\nEpisode 3600/50000 completed.\nEpisode 3700/50000 completed.\nEpisode 3800/50000 completed.\nEpisode 3900/50000 completed.\nEpisode 4000/50000 completed.\nEpisode 4100/50000 completed.\nEpisode 4200/50000 completed.\nEpisode 4300/50000 completed.\nEpisode 4400/50000 completed.\nEpisode 4500/50000 completed.\nEpisode 4600/50000 completed.\nEpisode 4700/50000 completed.\nEpisode 4800/50000 completed.\nEpisode 4900/50000 completed.\nEpisode 5000/50000 completed.\nEpisode 5100/50000 completed.\nEpisode 5200/50000 completed.\nEpisode 5300/50000 completed.\nEpisode 5400/50000 completed.\nEpisode 5500/50000 completed.\nEpisode 5600/50000 completed.\nEpisode 5700/50000 completed.\nEpisode 5800/50000 completed.\nEpisode 5900/50000 completed.\nEpisode 6000/50000 completed.\nEpisode 6100/50000 completed.\nEpisode 6200/50000 completed.\nEpisode 6300/50000 completed.\nEpisode 6400/50000 completed.\nEpisode 6500/50000 completed.\nEpisode 6600/50000 completed.\nEpisode 6700/50000 completed.\nEpisode 6800/50000 completed.\nEpisode 6900/50000 completed.\nEpisode 7000/50000 completed.\nEpisode 7100/50000 completed.\nEpisode 7200/50000 completed.\nEpisode 7300/50000 completed.\nEpisode 7400/50000 completed.\nEpisode 7500/50000 completed.\nEpisode 7600/50000 completed.\nEpisode 7700/50000 completed.\nEpisode 7800/50000 completed.\nEpisode 7900/50000 completed.\nEpisode 8000/50000 completed.\nEpisode 8100/50000 completed.\nEpisode 8200/50000 completed.\nEpisode 8300/50000 completed.\nEpisode 8400/50000 completed.\nEpisode 8500/50000 completed.\nEpisode 8600/50000 completed.\nEpisode 8700/50000 completed.\nEpisode 8800/50000 completed.\nEpisode 8900/50000 completed.\nEpisode 9000/50000 completed.\nEpisode 9100/50000 completed.\nEpisode 9200/50000 completed.\nEpisode 9300/50000 completed.\nEpisode 9400/50000 completed.\nEpisode 9500/50000 completed.\nEpisode 9600/50000 completed.\nEpisode 9700/50000 completed.\nEpisode 9800/50000 completed.\nEpisode 9900/50000 completed.\nEpisode 10000/50000 completed.\nEpisode 10100/50000 completed.\nEpisode 10200/50000 completed.\nEpisode 10300/50000 completed.\nEpisode 10400/50000 completed.\nEpisode 10500/50000 completed.\nEpisode 10600/50000 completed.\nEpisode 10700/50000 completed.\nEpisode 10800/50000 completed.\nEpisode 10900/50000 completed.\nEpisode 11000/50000 completed.\nEpisode 11100/50000 completed.\nEpisode 11200/50000 completed.\nEpisode 11300/50000 completed.\nEpisode 11400/50000 completed.\nEpisode 11500/50000 completed.\nEpisode 11600/50000 completed.\nEpisode 11700/50000 completed.\nEpisode 11800/50000 completed.\nEpisode 11900/50000 completed.\nEpisode 12000/50000 completed.\nEpisode 12100/50000 completed.\nEpisode 12200/50000 completed.\nEpisode 12300/50000 completed.\nEpisode 12400/50000 completed.\nEpisode 12500/50000 completed.\nEpisode 12600/50000 completed.\nEpisode 12700/50000 completed.\nEpisode 12800/50000 completed.\nEpisode 12900/50000 completed.\nEpisode 13000/50000 completed.\nEpisode 13100/50000 completed.\nEpisode 13200/50000 completed.\nEpisode 13300/50000 completed.\nEpisode 13400/50000 completed.\nEpisode 13500/50000 completed.\nEpisode 13600/50000 completed.\nEpisode 13700/50000 completed.\nEpisode 13800/50000 completed.\nEpisode 13900/50000 completed.\nEpisode 14000/50000 completed.\nEpisode 14100/50000 completed.\nEpisode 14200/50000 completed.\nEpisode 14300/50000 completed.\nEpisode 14400/50000 completed.\nEpisode 14500/50000 completed.\nEpisode 14600/50000 completed.\nEpisode 14700/50000 completed.\nEpisode 14800/50000 completed.\nEpisode 14900/50000 completed.\nEpisode 15000/50000 completed.\nEpisode 15100/50000 completed.\nEpisode 15200/50000 completed.\nEpisode 15300/50000 completed.\nEpisode 15400/50000 completed.\nEpisode 15500/50000 completed.\nEpisode 15600/50000 completed.\nEpisode 15700/50000 completed.\nEpisode 15800/50000 completed.\nEpisode 15900/50000 completed.\nEpisode 16000/50000 completed.\nEpisode 16100/50000 completed.\nEpisode 16200/50000 completed.\nEpisode 16300/50000 completed.\nEpisode 16400/50000 completed.\nEpisode 16500/50000 completed.\nEpisode 16600/50000 completed.\nEpisode 16700/50000 completed.\nEpisode 16800/50000 completed.\nEpisode 16900/50000 completed.\nEpisode 17000/50000 completed.\nEpisode 17100/50000 completed.\nEpisode 17200/50000 completed.\nEpisode 17300/50000 completed.\nEpisode 17400/50000 completed.\nEpisode 17500/50000 completed.\nEpisode 17600/50000 completed.\nEpisode 17700/50000 completed.\nEpisode 17800/50000 completed.\nEpisode 17900/50000 completed.\nEpisode 18000/50000 completed.\nEpisode 18100/50000 completed.\nEpisode 18200/50000 completed.\nEpisode 18300/50000 completed.\nEpisode 18400/50000 completed.\nEpisode 18500/50000 completed.\nEpisode 18600/50000 completed.\nEpisode 18700/50000 completed.\nEpisode 18800/50000 completed.\nEpisode 18900/50000 completed.\nEpisode 19000/50000 completed.\nEpisode 19100/50000 completed.\nEpisode 19200/50000 completed.\nEpisode 19300/50000 completed.\nEpisode 19400/50000 completed.\nEpisode 19500/50000 completed.\nEpisode 19600/50000 completed.\nEpisode 19700/50000 completed.\nEpisode 19800/50000 completed.\nEpisode 19900/50000 completed.\nEpisode 20000/50000 completed.\nEpisode 20100/50000 completed.\nEpisode 20200/50000 completed.\nEpisode 20300/50000 completed.\nEpisode 20400/50000 completed.\nEpisode 20500/50000 completed.\nEpisode 20600/50000 completed.\nEpisode 20700/50000 completed.\nEpisode 20800/50000 completed.\nEpisode 20900/50000 completed.\nEpisode 21000/50000 completed.\nEpisode 21100/50000 completed.\nEpisode 21200/50000 completed.\nEpisode 21300/50000 completed.\nEpisode 21400/50000 completed.\nEpisode 21500/50000 completed.\nEpisode 21600/50000 completed.\nEpisode 21700/50000 completed.\nEpisode 21800/50000 completed.\nEpisode 21900/50000 completed.\nEpisode 22000/50000 completed.\nEpisode 22100/50000 completed.\nEpisode 22200/50000 completed.\nEpisode 22300/50000 completed.\nEpisode 22400/50000 completed.\nEpisode 22500/50000 completed.\nEpisode 22600/50000 completed.\nEpisode 22700/50000 completed.\nEpisode 22800/50000 completed.\nEpisode 22900/50000 completed.\nEpisode 23000/50000 completed.\nEpisode 23100/50000 completed.\nEpisode 23200/50000 completed.\nEpisode 23300/50000 completed.\nEpisode 23400/50000 completed.\nEpisode 23500/50000 completed.\nEpisode 23600/50000 completed.\nEpisode 23700/50000 completed.\nEpisode 23800/50000 completed.\nEpisode 23900/50000 completed.\nEpisode 24000/50000 completed.\nEpisode 24100/50000 completed.\nEpisode 24200/50000 completed.\nEpisode 24300/50000 completed.\nEpisode 24400/50000 completed.\nEpisode 24500/50000 completed.\nEpisode 24600/50000 completed.\nEpisode 24700/50000 completed.\nEpisode 24800/50000 completed.\nEpisode 24900/50000 completed.\nEpisode 25000/50000 completed.\nEpisode 25100/50000 completed.\nEpisode 25200/50000 completed.\nEpisode 25300/50000 completed.\nEpisode 25400/50000 completed.\nEpisode 25500/50000 completed.\nEpisode 25600/50000 completed.\nEpisode 25700/50000 completed.\nEpisode 25800/50000 completed.\nEpisode 25900/50000 completed.\nEpisode 26000/50000 completed.\nEpisode 26100/50000 completed.\nEpisode 26200/50000 completed.\nEpisode 26300/50000 completed.\nEpisode 26400/50000 completed.\nEpisode 26500/50000 completed.\nEpisode 26600/50000 completed.\nEpisode 26700/50000 completed.\nEpisode 26800/50000 completed.\nEpisode 26900/50000 completed.\nEpisode 27000/50000 completed.\nEpisode 27100/50000 completed.\nEpisode 27200/50000 completed.\nEpisode 27300/50000 completed.\nEpisode 27400/50000 completed.\nEpisode 27500/50000 completed.\nEpisode 27600/50000 completed.\nEpisode 27700/50000 completed.\nEpisode 27800/50000 completed.\nEpisode 27900/50000 completed.\nEpisode 28000/50000 completed.\nEpisode 28100/50000 completed.\nEpisode 28200/50000 completed.\nEpisode 28300/50000 completed.\nEpisode 28400/50000 completed.\nEpisode 28500/50000 completed.\nEpisode 28600/50000 completed.\nEpisode 28700/50000 completed.\nEpisode 28800/50000 completed.\nEpisode 28900/50000 completed.\nEpisode 29000/50000 completed.\nEpisode 29100/50000 completed.\nEpisode 29200/50000 completed.\nEpisode 29300/50000 completed.\nEpisode 29400/50000 completed.\nEpisode 29500/50000 completed.\nEpisode 29600/50000 completed.\nEpisode 29700/50000 completed.\nEpisode 29800/50000 completed.\nEpisode 29900/50000 completed.\nEpisode 30000/50000 completed.\nEpisode 30100/50000 completed.\nEpisode 30200/50000 completed.\nEpisode 30300/50000 completed.\nEpisode 30400/50000 completed.\nEpisode 30500/50000 completed.\nEpisode 30600/50000 completed.\nEpisode 30700/50000 completed.\nEpisode 30800/50000 completed.\nEpisode 30900/50000 completed.\nEpisode 31000/50000 completed.\nEpisode 31100/50000 completed.\nEpisode 31200/50000 completed.\nEpisode 31300/50000 completed.\nEpisode 31400/50000 completed.\nEpisode 31500/50000 completed.\nEpisode 31600/50000 completed.\nEpisode 31700/50000 completed.\nEpisode 31800/50000 completed.\nEpisode 31900/50000 completed.\nEpisode 32000/50000 completed.\nEpisode 32100/50000 completed.\nEpisode 32200/50000 completed.\nEpisode 32300/50000 completed.\nEpisode 32400/50000 completed.\nEpisode 32500/50000 completed.\nEpisode 32600/50000 completed.\nEpisode 32700/50000 completed.\nEpisode 32800/50000 completed.\nEpisode 32900/50000 completed.\nEpisode 33000/50000 completed.\nEpisode 33100/50000 completed.\nEpisode 33200/50000 completed.\nEpisode 33300/50000 completed.\nEpisode 33400/50000 completed.\nEpisode 33500/50000 completed.\nEpisode 33600/50000 completed.\nEpisode 33700/50000 completed.\nEpisode 33800/50000 completed.\nEpisode 33900/50000 completed.\nEpisode 34000/50000 completed.\nEpisode 34100/50000 completed.\nEpisode 34200/50000 completed.\nEpisode 34300/50000 completed.\nEpisode 34400/50000 completed.\nEpisode 34500/50000 completed.\nEpisode 34600/50000 completed.\nEpisode 34700/50000 completed.\nEpisode 34800/50000 completed.\nEpisode 34900/50000 completed.\nEpisode 35000/50000 completed.\nEpisode 35100/50000 completed.\nEpisode 35200/50000 completed.\nEpisode 35300/50000 completed.\nEpisode 35400/50000 completed.\nEpisode 35500/50000 completed.\nEpisode 35600/50000 completed.\nEpisode 35700/50000 completed.\nEpisode 35800/50000 completed.\nEpisode 35900/50000 completed.\nEpisode 36000/50000 completed.\nEpisode 36100/50000 completed.\nEpisode 36200/50000 completed.\nEpisode 36300/50000 completed.\nEpisode 36400/50000 completed.\nEpisode 36500/50000 completed.\nEpisode 36600/50000 completed.\nEpisode 36700/50000 completed.\nEpisode 36800/50000 completed.\nEpisode 36900/50000 completed.\nEpisode 37000/50000 completed.\nEpisode 37100/50000 completed.\nEpisode 37200/50000 completed.\nEpisode 37300/50000 completed.\nEpisode 37400/50000 completed.\nEpisode 37500/50000 completed.\nEpisode 37600/50000 completed.\nEpisode 37700/50000 completed.\nEpisode 37800/50000 completed.\nEpisode 37900/50000 completed.\nEpisode 38000/50000 completed.\nEpisode 38100/50000 completed.\nEpisode 38200/50000 completed.\nEpisode 38300/50000 completed.\nEpisode 38400/50000 completed.\nEpisode 38500/50000 completed.\nEpisode 38600/50000 completed.\nEpisode 38700/50000 completed.\nEpisode 38800/50000 completed.\nEpisode 38900/50000 completed.\nEpisode 39000/50000 completed.\nEpisode 39100/50000 completed.\nEpisode 39200/50000 completed.\nEpisode 39300/50000 completed.\nEpisode 39400/50000 completed.\nEpisode 39500/50000 completed.\nEpisode 39600/50000 completed.\nEpisode 39700/50000 completed.\nEpisode 39800/50000 completed.\nEpisode 39900/50000 completed.\nEpisode 40000/50000 completed.\nEpisode 40100/50000 completed.\nEpisode 40200/50000 completed.\nEpisode 40300/50000 completed.\nEpisode 40400/50000 completed.\nEpisode 40500/50000 completed.\nEpisode 40600/50000 completed.\nEpisode 40700/50000 completed.\nEpisode 40800/50000 completed.\nEpisode 40900/50000 completed.\nEpisode 41000/50000 completed.\nEpisode 41100/50000 completed.\nEpisode 41200/50000 completed.\nEpisode 41300/50000 completed.\nEpisode 41400/50000 completed.\nEpisode 41500/50000 completed.\nEpisode 41600/50000 completed.\nEpisode 41700/50000 completed.\nEpisode 41800/50000 completed.\nEpisode 41900/50000 completed.\nEpisode 42000/50000 completed.\nEpisode 42100/50000 completed.\nEpisode 42200/50000 completed.\nEpisode 42300/50000 completed.\nEpisode 42400/50000 completed.\nEpisode 42500/50000 completed.\nEpisode 42600/50000 completed.\nEpisode 42700/50000 completed.\nEpisode 42800/50000 completed.\nEpisode 42900/50000 completed.\nEpisode 43000/50000 completed.\nEpisode 43100/50000 completed.\nEpisode 43200/50000 completed.\nEpisode 43300/50000 completed.\nEpisode 43400/50000 completed.\nEpisode 43500/50000 completed.\nEpisode 43600/50000 completed.\nEpisode 43700/50000 completed.\nEpisode 43800/50000 completed.\nEpisode 43900/50000 completed.\nEpisode 44000/50000 completed.\nEpisode 44100/50000 completed.\nEpisode 44200/50000 completed.\nEpisode 44300/50000 completed.\nEpisode 44400/50000 completed.\nEpisode 44500/50000 completed.\nEpisode 44600/50000 completed.\nEpisode 44700/50000 completed.\nEpisode 44800/50000 completed.\nEpisode 44900/50000 completed.\nEpisode 45000/50000 completed.\nEpisode 45100/50000 completed.\nEpisode 45200/50000 completed.\nEpisode 45300/50000 completed.\nEpisode 45400/50000 completed.\nEpisode 45500/50000 completed.\nEpisode 45600/50000 completed.\nEpisode 45700/50000 completed.\nEpisode 45800/50000 completed.\nEpisode 45900/50000 completed.\nEpisode 46000/50000 completed.\nEpisode 46100/50000 completed.\nEpisode 46200/50000 completed.\nEpisode 46300/50000 completed.\nEpisode 46400/50000 completed.\nEpisode 46500/50000 completed.\nEpisode 46600/50000 completed.\nEpisode 46700/50000 completed.\nEpisode 46800/50000 completed.\nEpisode 46900/50000 completed.\nEpisode 47000/50000 completed.\nEpisode 47100/50000 completed.\nEpisode 47200/50000 completed.\nEpisode 47300/50000 completed.\nEpisode 47400/50000 completed.\nEpisode 47500/50000 completed.\nEpisode 47600/50000 completed.\nEpisode 47700/50000 completed.\nEpisode 47800/50000 completed.\nEpisode 47900/50000 completed.\nEpisode 48000/50000 completed.\nEpisode 48100/50000 completed.\nEpisode 48200/50000 completed.\nEpisode 48300/50000 completed.\nEpisode 48400/50000 completed.\nEpisode 48500/50000 completed.\nEpisode 48600/50000 completed.\nEpisode 48700/50000 completed.\nEpisode 48800/50000 completed.\nEpisode 48900/50000 completed.\nEpisode 49000/50000 completed.\nEpisode 49100/50000 completed.\nEpisode 49200/50000 completed.\nEpisode 49300/50000 completed.\nEpisode 49400/50000 completed.\nEpisode 49500/50000 completed.\nEpisode 49600/50000 completed.\nEpisode 49700/50000 completed.\nEpisode 49800/50000 completed.\nEpisode 49900/50000 completed.\nEpisode 50000/50000 completed.\n\n\n\n\n\n\n\n\n\nWe can see that the policy is to move to the bottom right corner, as expected. We used a large amount of episodes to solve the environment due to its noise and nature. The policy is exactly the same as the one obtained with DP, which proofs the efficiency of this method in solving this environment.\n\n\nCode\nprint(\"State-Value Function V:\")\nprint(V_grid)\n\nprint(\"\\nAction-Value Function Q for a sample of States:\")\nsample_states = [0, env.observation_space.n // 2, env.observation_space.n - 1]\nfor s in sample_states:\n    state = env.index_to_state(s)\n    print(f\"State {state}:\")\n    for a in range(env.action_space.n):\n        print(f\"  Action {a}: Q = {Q[s, a]:.4f}\")\n\n\nState-Value Function V:\n[[64.81452738 66.96032657  0.         54.38225575 43.10283681 53.26222023]\n [66.40410697 69.14234925  0.         55.64561482  0.         71.6242858 ]\n [67.09054687 71.51320316 73.24481957 74.22207765 76.47954548 90.89601126]\n [54.98161546 63.62405729  0.          0.          0.         96.94535007]\n [43.94600015 47.01328113  0.         92.94012434 96.08132684 98.61172063]\n [ 0.         46.08805311  0.         90.93485795 93.1222424   0.        ]]\n\nAction-Value Function Q for a sample of States:\nState (0, 0):\n  Action 0: Q = 56.2440\n  Action 1: Q = 64.8145\n  Action 2: Q = 58.7834\n  Action 3: Q = 54.5829\nState (3, 0):\n  Action 0: Q = 54.9816\n  Action 1: Q = 43.6196\n  Action 2: Q = 41.6558\n  Action 3: Q = 43.4601\nState (5, 5):\n  Action 0: Q = 0.0000\n  Action 1: Q = 0.0000\n  Action 2: Q = 0.0000\n  Action 3: Q = 0.0000\n\n\n\n\nCode\n# Plot a heatmap of the State-Value Function V\n\nplt.figure(figsize=(8, 6))\nplt.imshow(V_grid, cmap='hot', interpolation='nearest')\nplt.colorbar(label='V(s)')\nplt.title('State-Value Function V (SARSA)')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.gca().invert_yaxis()\nplt.show()\n\n# Plot convergence plots\n\nV_history = np.array(solver.V_history)\nplt.figure(figsize=(12, 6))\nstates_to_plot = [0, env.observation_space.n // 2, env.observation_space.n - 1]\nfor s in states_to_plot:\n    plt.plot(V_history[:, s], label=f\"State {s}\")\nplt.xlabel('Episodes')\nplt.ylabel('V(s)')\nplt.title('Convergence of State-Value Function V (SARSA)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nQ_history = np.array(solver.Q_history)\nplt.figure(figsize=(12, 6))\nactions = [0, 1, 2, 3]\nfor s in states_to_plot:\n    for a in actions:\n        plt.plot(Q_history[:, s, a], label=f\"Q({s},{a})\")\nplt.xlabel('Episodes')\nplt.ylabel('Q(s,a)')\nplt.title('Convergence of Action-Value Function Q (SARSA)')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n\npolicy_history = np.array(solver.policy_history)\nplt.figure(figsize=(12, 6))\nfor s in states_to_plot:\n    plt.plot(policy_history[:, s], label=f\"Policy for State {s}\")\nplt.xlabel('Episodes')\nplt.ylabel('Action')\nplt.title('Convergence of Policy (SARSA)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe convergence is more noisy than DP, but it is less noisy than MC in general, but not in the beggining of the process."
  },
  {
    "objectID": "Tabula_RL.html#geosearch-solved-with-td",
    "href": "Tabula_RL.html#geosearch-solved-with-td",
    "title": "Demonstration of the Package created",
    "section": "Geosearch Solved with TD",
    "text": "Geosearch Solved with TD\nWe will solve the geosearch environment using Temporal Difference with Q-learning.\n\n\nCode\n# Create the environment\nenv = GeosearchEnv(render=False)\n\n# Solver\nsolver = TDSolver(env, gamma=0.99, alpha=0.1, epsilon=0.1, episodes=50000, method=\"q_learning\", max_steps_per_episode=500)\n\npolicy = solver.run()\n\n# Get the final V and Q\nV = solver.V_history[-1]\nQ = solver.Q_history[-1]\nV_grid = V.reshape((env.grid_size, env.grid_size))\n\n# Plot the optimal policy\naction_symbols = {0: '↑', 1: '↓', 2: '←', 3: '→'}\nplotter = PolicyPlotter_MC_TD(grid_size=(env.grid_size, env.grid_size), \n                        policy=policy, \n                        walls=[],\n                        terminal_states=[],\n                        action_symbols=action_symbols)\n\nplotter.plot()\n\n\nEpisode 100/50000 completed.\nEpisode 200/50000 completed.\nEpisode 300/50000 completed.\nEpisode 400/50000 completed.\nEpisode 500/50000 completed.\nEpisode 600/50000 completed.\nEpisode 700/50000 completed.\nEpisode 800/50000 completed.\nEpisode 900/50000 completed.\nEpisode 1000/50000 completed.\nEpisode 1100/50000 completed.\nEpisode 1200/50000 completed.\nEpisode 1300/50000 completed.\nEpisode 1400/50000 completed.\nEpisode 1500/50000 completed.\nEpisode 1600/50000 completed.\nEpisode 1700/50000 completed.\nEpisode 1800/50000 completed.\nEpisode 1900/50000 completed.\nEpisode 2000/50000 completed.\nEpisode 2100/50000 completed.\nEpisode 2200/50000 completed.\nEpisode 2300/50000 completed.\nEpisode 2400/50000 completed.\nEpisode 2500/50000 completed.\nEpisode 2600/50000 completed.\nEpisode 2700/50000 completed.\nEpisode 2800/50000 completed.\nEpisode 2900/50000 completed.\nEpisode 3000/50000 completed.\nEpisode 3100/50000 completed.\nEpisode 3200/50000 completed.\nEpisode 3300/50000 completed.\nEpisode 3400/50000 completed.\nEpisode 3500/50000 completed.\nEpisode 3600/50000 completed.\nEpisode 3700/50000 completed.\nEpisode 3800/50000 completed.\nEpisode 3900/50000 completed.\nEpisode 4000/50000 completed.\nEpisode 4100/50000 completed.\nEpisode 4200/50000 completed.\nEpisode 4300/50000 completed.\nEpisode 4400/50000 completed.\nEpisode 4500/50000 completed.\nEpisode 4600/50000 completed.\nEpisode 4700/50000 completed.\nEpisode 4800/50000 completed.\nEpisode 4900/50000 completed.\nEpisode 5000/50000 completed.\nEpisode 5100/50000 completed.\nEpisode 5200/50000 completed.\nEpisode 5300/50000 completed.\nEpisode 5400/50000 completed.\nEpisode 5500/50000 completed.\nEpisode 5600/50000 completed.\nEpisode 5700/50000 completed.\nEpisode 5800/50000 completed.\nEpisode 5900/50000 completed.\nEpisode 6000/50000 completed.\nEpisode 6100/50000 completed.\nEpisode 6200/50000 completed.\nEpisode 6300/50000 completed.\nEpisode 6400/50000 completed.\nEpisode 6500/50000 completed.\nEpisode 6600/50000 completed.\nEpisode 6700/50000 completed.\nEpisode 6800/50000 completed.\nEpisode 6900/50000 completed.\nEpisode 7000/50000 completed.\nEpisode 7100/50000 completed.\nEpisode 7200/50000 completed.\nEpisode 7300/50000 completed.\nEpisode 7400/50000 completed.\nEpisode 7500/50000 completed.\nEpisode 7600/50000 completed.\nEpisode 7700/50000 completed.\nEpisode 7800/50000 completed.\nEpisode 7900/50000 completed.\nEpisode 8000/50000 completed.\nEpisode 8100/50000 completed.\nEpisode 8200/50000 completed.\nEpisode 8300/50000 completed.\nEpisode 8400/50000 completed.\nEpisode 8500/50000 completed.\nEpisode 8600/50000 completed.\nEpisode 8700/50000 completed.\nEpisode 8800/50000 completed.\nEpisode 8900/50000 completed.\nEpisode 9000/50000 completed.\nEpisode 9100/50000 completed.\nEpisode 9200/50000 completed.\nEpisode 9300/50000 completed.\nEpisode 9400/50000 completed.\nEpisode 9500/50000 completed.\nEpisode 9600/50000 completed.\nEpisode 9700/50000 completed.\nEpisode 9800/50000 completed.\nEpisode 9900/50000 completed.\nEpisode 10000/50000 completed.\nEpisode 10100/50000 completed.\nEpisode 10200/50000 completed.\nEpisode 10300/50000 completed.\nEpisode 10400/50000 completed.\nEpisode 10500/50000 completed.\nEpisode 10600/50000 completed.\nEpisode 10700/50000 completed.\nEpisode 10800/50000 completed.\nEpisode 10900/50000 completed.\nEpisode 11000/50000 completed.\nEpisode 11100/50000 completed.\nEpisode 11200/50000 completed.\nEpisode 11300/50000 completed.\nEpisode 11400/50000 completed.\nEpisode 11500/50000 completed.\nEpisode 11600/50000 completed.\nEpisode 11700/50000 completed.\nEpisode 11800/50000 completed.\nEpisode 11900/50000 completed.\nEpisode 12000/50000 completed.\nEpisode 12100/50000 completed.\nEpisode 12200/50000 completed.\nEpisode 12300/50000 completed.\nEpisode 12400/50000 completed.\nEpisode 12500/50000 completed.\nEpisode 12600/50000 completed.\nEpisode 12700/50000 completed.\nEpisode 12800/50000 completed.\nEpisode 12900/50000 completed.\nEpisode 13000/50000 completed.\nEpisode 13100/50000 completed.\nEpisode 13200/50000 completed.\nEpisode 13300/50000 completed.\nEpisode 13400/50000 completed.\nEpisode 13500/50000 completed.\nEpisode 13600/50000 completed.\nEpisode 13700/50000 completed.\nEpisode 13800/50000 completed.\nEpisode 13900/50000 completed.\nEpisode 14000/50000 completed.\nEpisode 14100/50000 completed.\nEpisode 14200/50000 completed.\nEpisode 14300/50000 completed.\nEpisode 14400/50000 completed.\nEpisode 14500/50000 completed.\nEpisode 14600/50000 completed.\nEpisode 14700/50000 completed.\nEpisode 14800/50000 completed.\nEpisode 14900/50000 completed.\nEpisode 15000/50000 completed.\nEpisode 15100/50000 completed.\nEpisode 15200/50000 completed.\nEpisode 15300/50000 completed.\nEpisode 15400/50000 completed.\nEpisode 15500/50000 completed.\nEpisode 15600/50000 completed.\nEpisode 15700/50000 completed.\nEpisode 15800/50000 completed.\nEpisode 15900/50000 completed.\nEpisode 16000/50000 completed.\nEpisode 16100/50000 completed.\nEpisode 16200/50000 completed.\nEpisode 16300/50000 completed.\nEpisode 16400/50000 completed.\nEpisode 16500/50000 completed.\nEpisode 16600/50000 completed.\nEpisode 16700/50000 completed.\nEpisode 16800/50000 completed.\nEpisode 16900/50000 completed.\nEpisode 17000/50000 completed.\nEpisode 17100/50000 completed.\nEpisode 17200/50000 completed.\nEpisode 17300/50000 completed.\nEpisode 17400/50000 completed.\nEpisode 17500/50000 completed.\nEpisode 17600/50000 completed.\nEpisode 17700/50000 completed.\nEpisode 17800/50000 completed.\nEpisode 17900/50000 completed.\nEpisode 18000/50000 completed.\nEpisode 18100/50000 completed.\nEpisode 18200/50000 completed.\nEpisode 18300/50000 completed.\nEpisode 18400/50000 completed.\nEpisode 18500/50000 completed.\nEpisode 18600/50000 completed.\nEpisode 18700/50000 completed.\nEpisode 18800/50000 completed.\nEpisode 18900/50000 completed.\nEpisode 19000/50000 completed.\nEpisode 19100/50000 completed.\nEpisode 19200/50000 completed.\nEpisode 19300/50000 completed.\nEpisode 19400/50000 completed.\nEpisode 19500/50000 completed.\nEpisode 19600/50000 completed.\nEpisode 19700/50000 completed.\nEpisode 19800/50000 completed.\nEpisode 19900/50000 completed.\nEpisode 20000/50000 completed.\nEpisode 20100/50000 completed.\nEpisode 20200/50000 completed.\nEpisode 20300/50000 completed.\nEpisode 20400/50000 completed.\nEpisode 20500/50000 completed.\nEpisode 20600/50000 completed.\nEpisode 20700/50000 completed.\nEpisode 20800/50000 completed.\nEpisode 20900/50000 completed.\nEpisode 21000/50000 completed.\nEpisode 21100/50000 completed.\nEpisode 21200/50000 completed.\nEpisode 21300/50000 completed.\nEpisode 21400/50000 completed.\nEpisode 21500/50000 completed.\nEpisode 21600/50000 completed.\nEpisode 21700/50000 completed.\nEpisode 21800/50000 completed.\nEpisode 21900/50000 completed.\nEpisode 22000/50000 completed.\nEpisode 22100/50000 completed.\nEpisode 22200/50000 completed.\nEpisode 22300/50000 completed.\nEpisode 22400/50000 completed.\nEpisode 22500/50000 completed.\nEpisode 22600/50000 completed.\nEpisode 22700/50000 completed.\nEpisode 22800/50000 completed.\nEpisode 22900/50000 completed.\nEpisode 23000/50000 completed.\nEpisode 23100/50000 completed.\nEpisode 23200/50000 completed.\nEpisode 23300/50000 completed.\nEpisode 23400/50000 completed.\nEpisode 23500/50000 completed.\nEpisode 23600/50000 completed.\nEpisode 23700/50000 completed.\nEpisode 23800/50000 completed.\nEpisode 23900/50000 completed.\nEpisode 24000/50000 completed.\nEpisode 24100/50000 completed.\nEpisode 24200/50000 completed.\nEpisode 24300/50000 completed.\nEpisode 24400/50000 completed.\nEpisode 24500/50000 completed.\nEpisode 24600/50000 completed.\nEpisode 24700/50000 completed.\nEpisode 24800/50000 completed.\nEpisode 24900/50000 completed.\nEpisode 25000/50000 completed.\nEpisode 25100/50000 completed.\nEpisode 25200/50000 completed.\nEpisode 25300/50000 completed.\nEpisode 25400/50000 completed.\nEpisode 25500/50000 completed.\nEpisode 25600/50000 completed.\nEpisode 25700/50000 completed.\nEpisode 25800/50000 completed.\nEpisode 25900/50000 completed.\nEpisode 26000/50000 completed.\nEpisode 26100/50000 completed.\nEpisode 26200/50000 completed.\nEpisode 26300/50000 completed.\nEpisode 26400/50000 completed.\nEpisode 26500/50000 completed.\nEpisode 26600/50000 completed.\nEpisode 26700/50000 completed.\nEpisode 26800/50000 completed.\nEpisode 26900/50000 completed.\nEpisode 27000/50000 completed.\nEpisode 27100/50000 completed.\nEpisode 27200/50000 completed.\nEpisode 27300/50000 completed.\nEpisode 27400/50000 completed.\nEpisode 27500/50000 completed.\nEpisode 27600/50000 completed.\nEpisode 27700/50000 completed.\nEpisode 27800/50000 completed.\nEpisode 27900/50000 completed.\nEpisode 28000/50000 completed.\nEpisode 28100/50000 completed.\nEpisode 28200/50000 completed.\nEpisode 28300/50000 completed.\nEpisode 28400/50000 completed.\nEpisode 28500/50000 completed.\nEpisode 28600/50000 completed.\nEpisode 28700/50000 completed.\nEpisode 28800/50000 completed.\nEpisode 28900/50000 completed.\nEpisode 29000/50000 completed.\nEpisode 29100/50000 completed.\nEpisode 29200/50000 completed.\nEpisode 29300/50000 completed.\nEpisode 29400/50000 completed.\nEpisode 29500/50000 completed.\nEpisode 29600/50000 completed.\nEpisode 29700/50000 completed.\nEpisode 29800/50000 completed.\nEpisode 29900/50000 completed.\nEpisode 30000/50000 completed.\nEpisode 30100/50000 completed.\nEpisode 30200/50000 completed.\nEpisode 30300/50000 completed.\nEpisode 30400/50000 completed.\nEpisode 30500/50000 completed.\nEpisode 30600/50000 completed.\nEpisode 30700/50000 completed.\nEpisode 30800/50000 completed.\nEpisode 30900/50000 completed.\nEpisode 31000/50000 completed.\nEpisode 31100/50000 completed.\nEpisode 31200/50000 completed.\nEpisode 31300/50000 completed.\nEpisode 31400/50000 completed.\nEpisode 31500/50000 completed.\nEpisode 31600/50000 completed.\nEpisode 31700/50000 completed.\nEpisode 31800/50000 completed.\nEpisode 31900/50000 completed.\nEpisode 32000/50000 completed.\nEpisode 32100/50000 completed.\nEpisode 32200/50000 completed.\nEpisode 32300/50000 completed.\nEpisode 32400/50000 completed.\nEpisode 32500/50000 completed.\nEpisode 32600/50000 completed.\nEpisode 32700/50000 completed.\nEpisode 32800/50000 completed.\nEpisode 32900/50000 completed.\nEpisode 33000/50000 completed.\nEpisode 33100/50000 completed.\nEpisode 33200/50000 completed.\nEpisode 33300/50000 completed.\nEpisode 33400/50000 completed.\nEpisode 33500/50000 completed.\nEpisode 33600/50000 completed.\nEpisode 33700/50000 completed.\nEpisode 33800/50000 completed.\nEpisode 33900/50000 completed.\nEpisode 34000/50000 completed.\nEpisode 34100/50000 completed.\nEpisode 34200/50000 completed.\nEpisode 34300/50000 completed.\nEpisode 34400/50000 completed.\nEpisode 34500/50000 completed.\nEpisode 34600/50000 completed.\nEpisode 34700/50000 completed.\nEpisode 34800/50000 completed.\nEpisode 34900/50000 completed.\nEpisode 35000/50000 completed.\nEpisode 35100/50000 completed.\nEpisode 35200/50000 completed.\nEpisode 35300/50000 completed.\nEpisode 35400/50000 completed.\nEpisode 35500/50000 completed.\nEpisode 35600/50000 completed.\nEpisode 35700/50000 completed.\nEpisode 35800/50000 completed.\nEpisode 35900/50000 completed.\nEpisode 36000/50000 completed.\nEpisode 36100/50000 completed.\nEpisode 36200/50000 completed.\nEpisode 36300/50000 completed.\nEpisode 36400/50000 completed.\nEpisode 36500/50000 completed.\nEpisode 36600/50000 completed.\nEpisode 36700/50000 completed.\nEpisode 36800/50000 completed.\nEpisode 36900/50000 completed.\nEpisode 37000/50000 completed.\nEpisode 37100/50000 completed.\nEpisode 37200/50000 completed.\nEpisode 37300/50000 completed.\nEpisode 37400/50000 completed.\nEpisode 37500/50000 completed.\nEpisode 37600/50000 completed.\nEpisode 37700/50000 completed.\nEpisode 37800/50000 completed.\nEpisode 37900/50000 completed.\nEpisode 38000/50000 completed.\nEpisode 38100/50000 completed.\nEpisode 38200/50000 completed.\nEpisode 38300/50000 completed.\nEpisode 38400/50000 completed.\nEpisode 38500/50000 completed.\nEpisode 38600/50000 completed.\nEpisode 38700/50000 completed.\nEpisode 38800/50000 completed.\nEpisode 38900/50000 completed.\nEpisode 39000/50000 completed.\nEpisode 39100/50000 completed.\nEpisode 39200/50000 completed.\nEpisode 39300/50000 completed.\nEpisode 39400/50000 completed.\nEpisode 39500/50000 completed.\nEpisode 39600/50000 completed.\nEpisode 39700/50000 completed.\nEpisode 39800/50000 completed.\nEpisode 39900/50000 completed.\nEpisode 40000/50000 completed.\nEpisode 40100/50000 completed.\nEpisode 40200/50000 completed.\nEpisode 40300/50000 completed.\nEpisode 40400/50000 completed.\nEpisode 40500/50000 completed.\nEpisode 40600/50000 completed.\nEpisode 40700/50000 completed.\nEpisode 40800/50000 completed.\nEpisode 40900/50000 completed.\nEpisode 41000/50000 completed.\nEpisode 41100/50000 completed.\nEpisode 41200/50000 completed.\nEpisode 41300/50000 completed.\nEpisode 41400/50000 completed.\nEpisode 41500/50000 completed.\nEpisode 41600/50000 completed.\nEpisode 41700/50000 completed.\nEpisode 41800/50000 completed.\nEpisode 41900/50000 completed.\nEpisode 42000/50000 completed.\nEpisode 42100/50000 completed.\nEpisode 42200/50000 completed.\nEpisode 42300/50000 completed.\nEpisode 42400/50000 completed.\nEpisode 42500/50000 completed.\nEpisode 42600/50000 completed.\nEpisode 42700/50000 completed.\nEpisode 42800/50000 completed.\nEpisode 42900/50000 completed.\nEpisode 43000/50000 completed.\nEpisode 43100/50000 completed.\nEpisode 43200/50000 completed.\nEpisode 43300/50000 completed.\nEpisode 43400/50000 completed.\nEpisode 43500/50000 completed.\nEpisode 43600/50000 completed.\nEpisode 43700/50000 completed.\nEpisode 43800/50000 completed.\nEpisode 43900/50000 completed.\nEpisode 44000/50000 completed.\nEpisode 44100/50000 completed.\nEpisode 44200/50000 completed.\nEpisode 44300/50000 completed.\nEpisode 44400/50000 completed.\nEpisode 44500/50000 completed.\nEpisode 44600/50000 completed.\nEpisode 44700/50000 completed.\nEpisode 44800/50000 completed.\nEpisode 44900/50000 completed.\nEpisode 45000/50000 completed.\nEpisode 45100/50000 completed.\nEpisode 45200/50000 completed.\nEpisode 45300/50000 completed.\nEpisode 45400/50000 completed.\nEpisode 45500/50000 completed.\nEpisode 45600/50000 completed.\nEpisode 45700/50000 completed.\nEpisode 45800/50000 completed.\nEpisode 45900/50000 completed.\nEpisode 46000/50000 completed.\nEpisode 46100/50000 completed.\nEpisode 46200/50000 completed.\nEpisode 46300/50000 completed.\nEpisode 46400/50000 completed.\nEpisode 46500/50000 completed.\nEpisode 46600/50000 completed.\nEpisode 46700/50000 completed.\nEpisode 46800/50000 completed.\nEpisode 46900/50000 completed.\nEpisode 47000/50000 completed.\nEpisode 47100/50000 completed.\nEpisode 47200/50000 completed.\nEpisode 47300/50000 completed.\nEpisode 47400/50000 completed.\nEpisode 47500/50000 completed.\nEpisode 47600/50000 completed.\nEpisode 47700/50000 completed.\nEpisode 47800/50000 completed.\nEpisode 47900/50000 completed.\nEpisode 48000/50000 completed.\nEpisode 48100/50000 completed.\nEpisode 48200/50000 completed.\nEpisode 48300/50000 completed.\nEpisode 48400/50000 completed.\nEpisode 48500/50000 completed.\nEpisode 48600/50000 completed.\nEpisode 48700/50000 completed.\nEpisode 48800/50000 completed.\nEpisode 48900/50000 completed.\nEpisode 49000/50000 completed.\nEpisode 49100/50000 completed.\nEpisode 49200/50000 completed.\nEpisode 49300/50000 completed.\nEpisode 49400/50000 completed.\nEpisode 49500/50000 completed.\nEpisode 49600/50000 completed.\nEpisode 49700/50000 completed.\nEpisode 49800/50000 completed.\nEpisode 49900/50000 completed.\nEpisode 50000/50000 completed.\n\n\n\n\n\n\n\n\n\nWe can see that the policy is to move towards the “heat” points (gold and water) as desired. While it seems better than Monte Carlo, it is still noisy and not as clear as the results of dynamic programming. This makes sense due to the nature of the environment and the inherent large space counting with the episodes of Temporal Difference.\n\n\nCode\nprint(\"\\nAction-Value Function Q for Sample States:\")\nsample_states = [(0, 0), (12, 12), (24, 24), (20, 20), (10, 10)]\nfor state in sample_states:\n    s_index = env.state_to_index(state)\n    print(f\"State {state}:\")\n    for a in range(env.action_space.n):\n        print(f\"  Action {a}: Q = {Q[s_index, a]:.4f}\")\n\n\n\nAction-Value Function Q for Sample States:\nState (0, 0):\n  Action 0: Q = 0.6672\n  Action 1: Q = 0.0515\n  Action 2: Q = 1.0632\n  Action 3: Q = 2.5602\nState (12, 12):\n  Action 0: Q = 3.1987\n  Action 1: Q = 1.3074\n  Action 2: Q = 1.3116\n  Action 3: Q = 2.2479\nState (24, 24):\n  Action 0: Q = 8.2741\n  Action 1: Q = 1.3916\n  Action 2: Q = 0.3559\n  Action 3: Q = 1.3845\nState (20, 20):\n  Action 0: Q = 9.7674\n  Action 1: Q = 9.7674\n  Action 2: Q = 9.7674\n  Action 3: Q = 9.7674\nState (10, 10):\n  Action 0: Q = 3.2558\n  Action 1: Q = 3.2558\n  Action 2: Q = 3.2558\n  Action 3: Q = 3.2558\n\n\n\n\nCode\n# Plot a heatmap of the State-Value Function V\n\nplt.figure(figsize=(8, 6))\nplt.imshow(V_grid, cmap='hot', interpolation='nearest')\nplt.colorbar(label='V(s)')\nplt.title('State-Value Function V (Q-learning)')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.gca().invert_yaxis()\nplt.show()\n\n# Plot convergence plots\n\nV_history = np.array(solver.V_history)\nplt.figure(figsize=(10, 6))\nsample_states = [(0, 0), (12, 12), (24, 24), (20, 20), (10, 10)]\nstate_indices = [env.state_to_index(s) for s in sample_states]\nfor idx, s in enumerate(state_indices):\n    plt.plot(V_history[:, s], label=f\"State {sample_states[idx]}\")\nplt.xlabel('Episodes')\nplt.ylabel('V(s)')\nplt.title('Convergence of State-Value Function V (Q-learning)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nQ_history = np.array(solver.Q_history)\nplt.figure(figsize=(10, 6))\nactions = [0, 1, 2, 3]\nfor idx, s in enumerate(state_indices):\n    for a in actions:\n        plt.plot(Q_history[:, s, a], label=f\"Q({sample_states[idx]}, {a})\")\nplt.xlabel('Episodes')\nplt.ylabel('Q(s,a)')\nplt.title('Convergence of Action-Value Function Q (Q-learning)')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n\npolicy_history = np.array(solver.policy_history)\nplt.figure(figsize=(10, 6))\nfor idx, s in enumerate(state_indices):\n    plt.plot(policy_history[:, s], label=f\"Policy for State {sample_states[idx]}\")\nplt.xlabel('Episodes')\nplt.ylabel('Action')\nplt.title('Convergence of Policy (Q-learning)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe convergence is more noisy than DP, but it is less noisy than MC in general. However, the results seem to be better than Monte Carlo."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Data Exploration",
    "section": "",
    "text": "The goals of this tab is to gain a deeper understanding of the data, how variables behave, and identify anomalies, paterns, or potential insights for future exploration."
  },
  {
    "objectID": "EDA.html#eda-goals",
    "href": "EDA.html#eda-goals",
    "title": "Data Exploration",
    "section": "",
    "text": "The goals of this tab is to gain a deeper understanding of the data, how variables behave, and identify anomalies, paterns, or potential insights for future exploration."
  },
  {
    "objectID": "EDA.html#eda",
    "href": "EDA.html#eda",
    "title": "Data Exploration",
    "section": "EDA",
    "text": "EDA\nExplratory Data Analysis (EDA) is a critical step in data science. The goal is to visualize the data and get different insights that can be used to guide the rest of the analysis. It will not only help you understand the data better, but also identify areas of study. Furthermore, visualizing the data allows you to understand its shape, identify outliers, and see potential relationships between variables.\nBy analyzing the data, I hope to answer some questions while also identifying new ones."
  },
  {
    "objectID": "EDA.html#census-blocks-data",
    "href": "EDA.html#census-blocks-data",
    "title": "Data Exploration",
    "section": "Census Blocks Data",
    "text": "Census Blocks Data\nThis data set contains the information about all the census blocks in the US. The data set contains 117 columns and 220740 rows. Each row is a census block. The information about what each variable means can be found inside EPA pdf inside the data folder. For the purpose of EDA, we will focus on the following variables: ‘Ac_Water’, ‘Ac_Land’, ‘TotPop’, ‘AutoOwn0’, ‘E_MedWageWk’. These variables are the ones that I believe will be the most useful for the analysis.\nThe first main two focusses are Total Population and Working Class. These plots display relevant information about the distribution of these factors among all census blocks:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndf &lt;- read.csv(\"../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv\")\n\ncolumns_to_plot &lt;- c('CBSA_POP', 'Ac_Water', 'Ac_Land', 'TotPop', 'P_WrkAge', 'E_HiWageWk', 'AutoOwn0', 'NatWalkInd', 'E_MedWageWk', 'Ac_Unpr')\ndf &lt;- df[,columns_to_plot]\n\nggplot(df, aes(x = TotPop, fill = factor(1))) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Total Population\",\n       x = \"TotPop\",\n       y = \"Density\",\n       fill = \"TotPop\") + \n  scale_fill_manual(values = c(\"lightblue1\")) +\n  theme_minimal()\n\nggplot(df, aes(x = E_MedWageWk, fill = factor(1))) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"# of workers earning more than $1250/month but less than\n$3333/month (work location)\",\n       x = \"E_MedWageWk\",\n       y = \"Density\",\n       fill = \"E_MedWageWk\") + \n  scale_fill_manual(values = c(\"turquoise\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, while some census blocks have really high values on these two parameters, most of them have low values. This is important to keep in mind when analyzing the data, as it will be important to understand how the data is distributed. The plots are impressingly skewed, which make sense as most of the US population is concentrated in small towns and cities.\nThe third point we will focus on is the number of people who own 0 cars. While the US is a car-centric country, since we are focussing on public transportation, this information could give us great insights, as these people will only be able to use public transportation or walk to get to their destination. The following plot shows the distribution of the number of people who own 0 cars:\n\n\nCode\nggplot(df, aes(x = \"\", y = AutoOwn0, color = factor(1))) +\n  geom_jitter(alpha = 0.7, width = 0.3) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, coef = 0) +\n  labs(title = \"Total number of people who own 0 cars\",\n       x = \"AutoOwn0\",\n       y = \"Count\",\n       fill = \"AutoOwn0\") +\n  scale_color_manual(values = c(\"turquoise\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"AutoOwn\"))\n\n\n\n\n\n\n\n\n\nThe boxplot is squezzed next to zero and each point represents a value for a census block. As expected, most of the US population owns at least one car. However, there is a significant amount of people who own 0 cars, which is great information for our analysis.\nThis following plot will aim to see how much water is on the census blocks. Since water determines not only what methods of transportation can be built but also exacerbates the costs of construction, it is a factor to keep in consideration. The following plot shows the distribution of the amount of water in the census blocks:\n\n\nCode\nggplot(df, aes(x = \"\", y = Ac_Water, color = factor(1))) +\n  geom_jitter(alpha = 0.7, width = 0.3) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, coef = 0) +\n  labs(title = \"Total water area\",\n       x = \"Ac_Water\",\n       y = \"Count\",\n       fill = \"Ac_Water\") +\n  scale_color_manual(values = c(\"blue\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"Ac_Water\"))\n\n\n\n\n\n\n\n\n\nThis plot shows how most census blocks have a similar number of acres of water, however, the range is big enough to consider it in our analysis and cause problems with public transportation. Water is a determinant factor for public transportation, as it is not possible to build roads, railways on water, or makes constructing tunnels much harder. This is why it is important to understand how much water is in each census block. Furthermore, the fact that we have a couple of very big outliers will be very useful when drawing general conclusions about different groupings.\nThis last plot for this data set will focus on the amount of land in each census block. This is important as it will determine how much space is available for public transportation. The following plot shows the distribution of the amount of land in the census blocks:\n\n\nCode\nggplot(df, aes(x = \"\", y = Ac_Land, color = factor(1))) +\n  geom_jitter(alpha = 0.7, width = 0.3) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, coef = 0) +\n  labs(title = \"Total land area\",\n       x = \"Ac_Water\",\n       y = \"Count\",\n       fill = \"Ac_Land\") +\n  scale_color_manual(values = c(\"burlywood1\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"Ac_Land\"))\n\n\n\n\n\n\n\n\n\nThis plot shows how most census blocks have a similar number of acres of land, however, the range is big enough to consider it in our analysis and couse problems with public transportation. Land is a determinant factor for public transportation, as it determines the spread and the number of stops needed to cover these areas. We can see that there are a couple of very big outliers, which is important to take note of."
  },
  {
    "objectID": "EDA.html#fuel-and-energy-data",
    "href": "EDA.html#fuel-and-energy-data",
    "title": "Data Exploration",
    "section": "2021 Fuel and Energy Data",
    "text": "2021 Fuel and Energy Data\nThis data set contains information about public transportation, agencies, cities, and energy consumption. It was downloaded here. We will aim to look for patterns in the data, outliers, counts, etc.\nThe following two plots focus on the average miles per gallon based on the population and the density of the Population of the urbanized area served by the transit agency. This will give us insights on the data we have about population and if there is a correlation between the population and the miles per gallon.\n\n\nCode\nfuel &lt;- read.csv(\"../../data/Clean_Data_project_Pub.Transport_5000/2021_Fuel_and_Energy_mpg_fossil_fuels.csv\")\n\nggplot(fuel, aes(x = Primary.UZA.Population, fill = \"\")) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Population of the urbanized area served by the transit agency\",\n       x = \"Total Population\",\n       y = \"Density\",\n       fill = \"E_MedWageWk\") +\n  scale_fill_manual(values = c(\"lightblue1\")) +\n  theme_minimal()\n\nggplot(fuel, aes(x = Primary.UZA.Population, y = avg_mpg_fossil_fuels)) +\n  geom_point() +\n  labs(title = \"Population vs. Average MPG\",\n       x = \"Primary UZA Population\",\n       y = \"Average MPG (Fossil Fuels)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom these plots we can see that the population density is skewed to the left in our data. However, we can see small peaks at certain points. This helps us understand the scatter plot better. We can see that the data points follow the structure of the density, but it seems that population seems to not be very correlated with the miles per gallon. However, there are a couple of very heavy outliers to keep in mind. This is important to keep in mind when analyzing the data, as it will be important to understand how the data is distributed.\nThe following plot will allow us to see the count of each organization type. Not only is a great way to see the most predominant ones, but understand how much data of each type we have.\n\n\nCode\nfuel %&gt;%\n  group_by(Organization.Type) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = factor(Organization.Type), y = count, fill = factor(Organization.Type))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Organization Type Counts\",\n       x = \"\",  \n       y = \"Count\") +\n  theme(axis.text.x = element_blank()) \n\n\n\n\n\n\n\n\n\nFrom this plot, the major two organization types are City, County, or Local Government and the Independent Public Agency or Authority of Transit Service. They account for much more than 50% of the data. This is insightful to understand what agencies are there but also which ones are being utilized currently, since they may or may not be the optimal ones.\nThe following plot will allow us to see the relationship of each mode of transportation and the average MPG. It is a great way to make inferences about their relationship and understand which modes of transportation are more efficient.\n\n\nCode\nggplot(fuel, aes(x = Mode, y = avg_mpg_fossil_fuels, fill = Mode)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", position = \"dodge\") +\n  labs(title = \"Mode vs. Average MPG\",\n       x = \"Mode\",\n       y = \"Average MPG (Fossil Fuels)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFrom this plot it seems that the method of transportation clearly matters when it comes to the average MPG. There are some modes of transportation that are clearly more efficient than others. However, this might be due to the lack of data and should be crosschecked.\nThis final plot will allow us to see the relationship of each type of service and the average MPG. It is a great way to make inferences about their relationship and understand which types of services of transportation are more efficient.\n\n\nCode\nggplot(fuel, aes(x = TOS, y = avg_mpg_fossil_fuels, fill = TOS)) +\n  geom_violin() +\n  labs(title = \"Violin Plot: TOS vs. Average MPG\",\n       x = \"TOS\",\n       y = \"Average MPG (Fossil Fuels)\")\n\n\n\n\n\n\n\n\n\nThis violin plots don’t show much information, as the data is very skewed. However, we can see that the data is very concentrated around the 0. This is important to keep in mind when analyzing the data, as it will be important to understand how the data is distributed."
  },
  {
    "objectID": "EDA.html#border-crossing-entry-data",
    "href": "EDA.html#border-crossing-entry-data",
    "title": "Data Exploration",
    "section": "Border Crossing Entry Data",
    "text": "Border Crossing Entry Data\nThis data set contains information of the vehicles and their type that crossed the border between other countries and the US. It was downloaded here. We will aim to understand the distribution of the data, the outliers, and the relationship between the different variables since this data set will be used to build a naive bayes model. While this data set is not very insightful for the analysis, it can be useful to understand if different mehtods of transportation are valued different or run though different borders.\nThe following plot displays the distribution of the values of different transportation methods. This will allow us to understand the distribution of the data and if there are any outliers. Furthermore, we will be able to see if they are sparse (which would mean a good sign of different methods of transportation being valued differently) or very squed.\n\n\nCode\ncrossing &lt;- read.csv('../../data/Clean_Data_project_Pub.Transport_5000/Clean_Border_Crossing_Entry_Data_20231103.csv')\n\nggplot(crossing, aes(x = \"\", y = Value, color = factor(1))) +\n  geom_jitter(alpha = 0.7, width = 0.3) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, coef = 0) +\n  labs(title = \"Total land area\",\n       x = \"vehicles\",\n       y = \"Value\",\n       fill = \"Value\") +\n  scale_color_manual(values = c(\"darkolivegreen\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"Value of vehicles\"))\n\n\n\n\n\n\n\n\n\nFrom this plot, we can see that the data seems sparse, which means that different methods of transportation are valued differently. This is a good sign as we might be able to use this to predict the method of transportation (even though we cannot determine this until we test it).\nThe following plot will display the count of each method of transportation. This allow us to see if we have similar number of data points for each method of transportation or if some are more popular than others.\n\n\nCode\ncrossing %&gt;%\n  group_by(Measure) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = Measure, y = count, fill = Measure)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Counts by Method of transportation\", x = \"Measure\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\nAll methods seem to have the same popularity, which is great for our analysis. This means that we will have enough data points for each method of transportation to make predictions.\nThis final plot will show us the count of crossings in each state. This will help us identify the most popular states for crossing into or out from the US and understand better the nature of our data set.\n\n\nCode\ncrossing %&gt;%\n  group_by(State) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = State, y = count, fill = State)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Counts by State\",\n       x = \"State\",\n       y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\nThis plot shows us that the most popular states is North Dakota followed by Washington. This is important to keep in mind as these states might have some specific types of transportation that are more popular than others."
  },
  {
    "objectID": "EDA.html#other-datasets-that-will-help-us-with-insights",
    "href": "EDA.html#other-datasets-that-will-help-us-with-insights",
    "title": "Data Exploration",
    "section": "Other datasets that will help us with insights",
    "text": "Other datasets that will help us with insights\nThese data sets will help us with insights on the data we are working with. While they have not been further analyzed with other time due to time constraints, they reveal relevant insights about the public transportation industry.\n\nVehicle Production by countries\nHere, we will be exploring the vehicle_production_countries data set. It will allow us understand which countries are the major producers and how this production changed over time. This will allow us to understand the industry better and see if there are any trends that we can use to make predictions.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(boot)\nlibrary(AICcmodavg)\nlibrary(readxl)\nlibrary(broom)\n\nvehicle_production_countries_comercial &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/vehicle_production_countries.xlsx\", sheet=\"Comercial-vehicles\", skip = 1)\n\nvehicle_production_countries_comercial[-1] &lt;- lapply(vehicle_production_countries_comercial[-1], function(x) ifelse(x %in% c(\"N\", \"U\", \"Z\"), NA, x))\n\nvehicle_production_countries_comercial &lt;- vehicle_production_countries_comercial %&gt;%\n  mutate(across(-1, as.numeric))\n\ncolnames(vehicle_production_countries_comercial)[colnames(vehicle_production_countries_comercial) == \"...1\"] &lt;- \"Country\"\ncolnames(vehicle_production_countries_comercial)\n\nvehicle_production_countries_comercial_transposed &lt;- as.data.frame(t(vehicle_production_countries_comercial))\n\ncolnames(vehicle_production_countries_comercial_transposed) &lt;- vehicle_production_countries_comercial_transposed[1, ]\n\nvehicle_production_countries_comercial_transposed &lt;- vehicle_production_countries_comercial_transposed[-1, ]\n\nvehicle_production_countries_comercial_transposed[] &lt;- lapply(vehicle_production_countries_comercial_transposed, as.numeric)\n\nvehicle_production_countries_comercial_transposed &lt;- vehicle_production_countries_comercial_transposed[, !colnames(vehicle_production_countries_comercial_transposed) %in% \"Total world\"]\n\ndf &lt;- vehicle_production_countries_comercial_transposed %&gt;% \n  rownames_to_column(var = \"Year\")\n\ndf &lt;- df %&gt;%\n  mutate(Year = ifelse(Year == \"(R) 2019\", 2019, ifelse(Year == \"(R) 2020\", 2020, Year)))\n\n\n\n\nCode\ndf_long &lt;- df %&gt;%\n  pivot_longer(-Year, names_to = \"Country\", values_to = \"Value\")\n\nggplot(df_long, aes(x = Year, y = Value, color = Country)) +\n  geom_point() +\n  labs(\n    title = \"Value per year and per country in comercial vehicle production\",\n    x = \"Year\",\n    y = \"Value of comercial vehicles produced\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", axis.text.x = element_text(angle = 90, hjust = 1))\n\n\nWarning message:\n\"Removed 188 rows containing missing values (geom_point).\"\n\n\n\n\n\n\n\n\n\nFrom this plot we can see how Japan (back in the 1960s) used to be one of the biggest producers but not anymore. The US has been in the lead since the 1990s until around 2008, which was expected due to the recession. However, it is impressive how China was able to excel during the recession in this market at an almost exponential rate. And it still has kept increasing since then.\nIn order to run an ANOVA test and look if countries, years, and interaction among the both have an statistical significance on comercial vehicle production, we are going to assume that all the missing values are not relevant to the contribution of the overall market and, therefore, close to 0. Thuis, for the sake of this process, we will make them 0.\n\n\nCode\nvehicle_production_countries_comercial &lt;- replace(vehicle_production_countries_comercial, is.na(vehicle_production_countries_comercial), 0)\n\n\n\n\nCode\ndata_long &lt;- vehicle_production_countries_comercial %&gt;%\n  pivot_longer(cols = -Country, names_to = \"Year\", values_to = \"Value\")\n\ndata_long &lt;- data_long[data_long$Country != \"Total world\", ]\n\nmodel &lt;- aov(Value ~ Year * Country, data = data_long)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nWarning message in anova.lm(object):\n\"ANOVA F-tests on an essentially perfect fit are unreliable\"\n\n\nAnalysis of Variance Table\n\nResponse: Value\n              Df     Sum Sq  Mean Sq F value Pr(&gt;F)\nYear          31   85531235  2759072     NaN    NaN\nCountry       32 2172668273 67895884     NaN    NaN\nYear:Country 992 1139993857  1149187     NaN    NaN\nResiduals      0          0      NaN               \n\n\nSeing this, we will simplify our model, taking away the interaction (in order to no overfit the data with our ANOVA model).\n\n\nCode\nmodel &lt;- aov(Value ~ Year + Country, data = data_long)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Value\n           Df     Sum Sq  Mean Sq F value    Pr(&gt;F)    \nYear       31   85531235  2759072  2.4009 3.244e-05 ***\nCountry    32 2172668273 67895884 59.0816 &lt; 2.2e-16 ***\nResiduals 992 1139993857  1149187                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth have a significant effect (which was expected). However, this helped us also understand that their interaction would overfit the model in this case.\n\n\nEnergy and usage of Buses\nIn this part, we will be exploring the energy_consumed_byMill_passenger_MILES file.\n\n\nCode\nenergy_consum &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/energy_consumed_byMill_passenger_MILES.xlsx\", sheet=\"Energy\")\n\ncolnames(energy_consum) &lt;- energy_consum[1, ]\n\nenergy_consum &lt;- energy_consum[-1, ]\n\ncolnames(energy_consum)[1] &lt;- \"Data_to_Explore\"\n\nenergy_consum &lt;- energy_consum[energy_consum$Data_to_Explore %in% c(\n  \"Vehicle-miles (millions)\",\n  \"Passenger-miles (millions)\",\n  \"Energy consumed, total (billion Btu)\",\n  \"Energy intensity (Btu/passenger-mile)\"\n), ]\n\nenergy_consum[, -1] &lt;- apply(energy_consum[, -1], 2, function(x) ifelse(x == \"N\", NA, x))\n\nenergy_consum &lt;- energy_consum %&gt;% select(-\"1960\", -\"1965\", -\"1970\", -\"1975\", -\"1980\", -\"1985\", -\"1990\", -\"1991\", -\"1992\", -\"1993\", -\"1994\", -\"1995\")\n\nenergy_consum &lt;- energy_consum %&gt;%\n  mutate_at(vars(-1), as.numeric)\n\nenergy_consum_long &lt;-as.data.frame(t(energy_consum), index=False)\n\ncolnames(energy_consum_long) &lt;- energy_consum_long[1, ]\n\nenergy_consum_long &lt;- energy_consum_long[-1, ]\n\ncolnames(energy_consum_long) &lt;- c(\"Vehicle_milesMill\", \"Passenger_miles_mill\",\"Energy_consumed_total_bill_Btu\", \"Energy_intensity_Btu_passenger_mile\")\n\nYears = c(1996:2021)\nenergy_consum_long['Years'] &lt;- Years\n\nenergy_consum_long_no_Covid &lt;- energy_consum_long[!(energy_consum_long$Years %in% c(2020, 2021)), ]\n\n\nFor the fitted model in the following plots, year 2020 and 2021 are going to be ignored and considered outliers due to COVID-19. They are heavy outliers.\n\n\nCode\nlm_model &lt;- lm(energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile ~ energy_consum_long_no_Covid$Year)\n\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile,\n     xlab = \"Year\",\n     ylab = \"Energy consumed, total (billion Btu)\",\n     col = \"blue3\",\n     pch = 19)\n\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\nabline(lm_model, col = \"red\")\n\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile, col = \"blue3\", pch = 19)\n\n\n\n\n\n\n\n\n\nFrom this plot we can see how the total energy consumed (in billion Btu) has been constantly decreasing. However, is it because our vehicles are more efficient or due to cuts in public transportation?\n\n\nCode\nlm_model &lt;- lm(energy_consum_long_no_Covid$Passenger_miles_mill ~ energy_consum_long_no_Covid$Year)\n\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Passenger_miles_mill,\n     xlab = \"Year\",\n     ylab = \"Passenger-miles (millions)\",\n     col = \"blue3\",\n     pch = 19)\n\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\nabline(lm_model, col = \"red\")\n\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Passenger_miles_mill, col = \"blue3\", pch = 19)\n\n\n\n\n\n\n\n\n\nWhile the fitted model shows an increase in the usage of public transportation, we can see that it has been constantly dropping since around 2014. This can bring up many questions such as: Is it due to an investment problem? Is it because there are not enough incentives to use public transportation? Do people own more cars?\n\n\nCode\nlm_model &lt;- lm(energy_consum_long_no_Covid$Vehicle_milesMill ~ energy_consum_long_no_Covid$Year)\n\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Vehicle_milesMill,\n     xlab = \"Year\",\n     ylab = \"Vehicle miles (millions)\",\n     col = \"blue3\",\n     pch = 19)\n\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\nabline(lm_model, col = \"red\")\n\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Vehicle_milesMill, col = \"blue3\", pch = 19)\n\n\n\n\n\n\n\n\n\nThe vehicle miles have been constantly increasing, which means that there have been more and more routes added over time. However, why has this not been enough to increase the demand of public transportation?\n\n\nDC Metro Scorecard\nThis part will focus on the DC_Metro_Scorecard data, which counts the reliability and efficiency of DC Metro from 2014 to 2016.\n\n\nCode\nDC_metro &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/DC_Metro_Scorecard.xlsx\", sheet=\"Sheet1\")\n\nDC_metro &lt;- DC_metro%&gt;% select (-\"Crimes Target\", -\"Employee Injury Rate Target\", -\"Customer Injury Rate Target\", -\"Elevator Reliability\", -\"Elevator Reliability Target\", -\"Escalator Reliability Target\",\n-\"Rail Fleet Reliability Target\", -\"Bus On-Time Performance Target\", -\"Bus Fleet Reliability Target\", -\"Escalator Reliability\", -\"Rail On-Time Performance Target\")\n\ncolnames(DC_metro) &lt;- c('Year','Month','Bus_on_time','Bus_fleet_reliability','Rail_fleet_reliability', 'Rail_on_time', 'Customer_injury_rate_per_1_Mill', 'Employee_injury_rate_per_200k_h', 'Crimes_per_1_Mill_passengers', 'Crimes_per_1_Mill_passengers')\n\n\nFirstly, we are going to see wherther the year and the month have a significant effect on the values seen in our data. This will help us understand if it has become better over the years or if there are months that have effects on the outcomes of public transportation due to weather or other circumstances.\n\n\nCode\nBus_on_time &lt;- DC_metro %&gt;% select('Year', 'Month', 'Bus_on_time')\n\nmodel &lt;- aov(Bus_on_time ~ Year + Month, data = Bus_on_time)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Bus_on_time\n          Df    Sum Sq    Mean Sq F value    Pr(&gt;F)    \nYear       2 0.0012562 0.00062809  7.6584 0.0031782 ** \nMonth     12 0.0056355 0.00046963  5.7263 0.0002625 ***\nResiduals 21 0.0017223 0.00008201                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the buses being on time.\n\n\nCode\nBus_fleet_reliability &lt;- DC_metro %&gt;% select('Year', 'Month', 'Bus_fleet_reliability')\n\nmodel &lt;- aov(Bus_fleet_reliability ~ Year + Month, data = Bus_fleet_reliability)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Bus_fleet_reliability\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nYear       2 5604536 2802268  9.3323 0.001259 **\nMonth     12 9091418  757618  2.5231 0.030574 * \nResiduals 21 6305777  300275                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the bus fleet reliability (miles travelled before a breakdown).\n\n\nCode\nRail_fleet_reliability &lt;- DC_metro %&gt;% select('Year', 'Month', 'Rail_fleet_reliability')\n\nmodel &lt;- aov(Rail_fleet_reliability ~ Year + Month, data = Rail_fleet_reliability)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Rail_fleet_reliability\n          Df     Sum Sq   Mean Sq F value  Pr(&gt;F)  \nYear       2  536503502 268251751  2.8728 0.07892 .\nMonth     12 2480920803 206743400  2.2141 0.05343 .\nResiduals 21 1960928969  93377570                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case, year and month don’t seem to have a significant effect on rail transportation fleet reliability (miles travelled before a breakdown).\n\n\nCode\nRail_on_time &lt;- DC_metro %&gt;% select('Year', 'Month', 'Rail_on_time')\n\nmodel &lt;- aov(Rail_on_time ~ Year + Month, data = Rail_on_time)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Rail_on_time\n          Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    \nYear       2 0.060978 0.0304888 52.2126 7.084e-09 ***\nMonth     12 0.020835 0.0017363  2.9734   0.01397 *  \nResiduals 21 0.012263 0.0005839                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the rail transportation being on time.\n\n\nCode\nCustomer_injury_rate_per_1_Mill &lt;- DC_metro %&gt;% select('Year', 'Month', 'Customer_injury_rate_per_1_Mill')\n\nmodel &lt;- aov(Customer_injury_rate_per_1_Mill ~ Year + Month, data = Customer_injury_rate_per_1_Mill)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Customer_injury_rate_per_1_Mill\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nYear       2  0.1121 0.05606  0.1770 0.83903  \nMonth     12 11.8039 0.98366  3.1056 0.01119 *\nResiduals 21  6.6515 0.31674                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOnly the month seems to have a significant effect on the customer injury rate (which is expected due to weather). However, if year does not, have we taken any necessary actions to improve the security on public transportation?\n\n\nCode\nEmployee_injury_rate_per_200k_h &lt;- DC_metro %&gt;% select('Year', 'Month', 'Employee_injury_rate_per_200k_h')\n\nmodel &lt;- aov(Employee_injury_rate_per_200k_h ~ Year + Month, data = Employee_injury_rate_per_200k_h)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Employee_injury_rate_per_200k_h\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nYear       2 11.822  5.9109  9.0165 0.00149 **\nMonth     12 12.759  1.0632  1.6218 0.16025   \nResiduals 21 13.767  0.6556                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case as year has a significant effect on employee injury rates and months do not, we can assume that some actions have been taken during the years.\n\n\nCode\nCrimes_per_1_Mill_passengers &lt;- DC_metro %&gt;% select('Year', 'Month', 'Crimes_per_1_Mill_passengers')\n\nmodel &lt;- aov(Crimes_per_1_Mill_passengers ~ Year + Month, data = Crimes_per_1_Mill_passengers)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Crimes_per_1_Mill_passengers\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nYear       1  0.2313 0.23133  0.6465 0.44209  \nMonth     12 14.6777 1.22314  3.4182 0.03656 *\nResiduals  9  3.2205 0.35783                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOnly months seem to have a significant effect on crime rates produced on public transportation. We could use this information to decide what months of the year we should increase security and question if this effect is due to holidays or other important events.\n\n\nCode\n\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Year\", str_replace, \"CY \", \"\")\nhead(DC_metro)\n\nDC_metro$Year &lt;- as.numeric(DC_metro$Year)\n\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jan\", \"01\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Feb\", \"02\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Mar\", \"03\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Apr\", \"04\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"May\", \"05\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jun\", \"06\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jul\", \"07\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Aug\", \"08\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Sep\", \"09\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Oct\", \"10\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Nov\", \"11\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Dec\", \"12\")\n\nDC_metro &lt;- DC_metro[order(DC_metro$Year, DC_metro$Month), ]\n\nDC_metro &lt;- subset(DC_metro, DC_metro$Month!='YTD')\nhead(DC_metro)\n\nlibrary(zoo)\n\nDC_metro$Date &lt;- as.yearmon(paste(DC_metro$Month, DC_metro$Year, sep = \" \"), format = \"%m %Y\")\n\n\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Bus_on_time)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  \n  ylab(\"Bus on Time\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nWhile the spread of the points is very wide, it is concerning to see a trend in which the Buses seem to be more and more delayed over time. This means that we are not taking the necessary steps to improve it.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Bus_fleet_reliability)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") + \n  ylab(\"Fleet Reliability\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFleet reliability has improved over time (even though the spread of the data is too wide). That is probably due to vehicle improvements.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Rail_fleet_reliability)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") + \n  ylab(\"Rail Fleet reliability\")  \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nThere seems to not be a clear improvement on the fleet reliability of rail transportation over this period of time.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Rail_on_time)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") + \n  ylab(\"Rail on time performance\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nRail transportation seems to clearly have been getting worse over time. This problem is concerning and should be tackled as soon as possible. Is it due to safety reasons, investment problems, or poor planification?\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Customer_injury_rate_per_1_Mill)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  \n  ylab(\"Customer injuries per 1 Million passangers\")  \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nCustomer injuries seem to have been stable overtime (with a couple of outliers that could be interesting to look for).\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Employee_injury_rate_per_200k_h)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  \n  ylab(\"Employee injuries per 200k h\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nThe employee injuries have been increasing over time. This means that if the trend keeps following this pattern, we should probably invest more in safety and take some more precautions for the employees.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Crimes_per_1_Mill_passengers)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") + \n  ylab(\"Crimes per 1 Million passangers\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nThe crime rates have been increasing over time. This means that if the trend keeps following this pattern, we should probably invest in more security on public transportation."
  },
  {
    "objectID": "EDA.html#text-data",
    "href": "EDA.html#text-data",
    "title": "Data Exploration",
    "section": "Text Data",
    "text": "Text Data\nIn order to see what what are the most important concerns regarding public transportation to the users, we are going top explore what people mention the most in their reddits about public transportation through a word cloud.\n\n\nCode\nimport re\nimport spacy.lang.en.stop_words as stopwords\nimport spacy\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndf=pd.read_json(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/sentiment_results.json\")\nprint(df.shape)\nprint(df.columns)\n\ndf = df[df.text != '']\nprint(df.shape)\nprint(df.columns)\n\ntexts = []\ny = []\n\nparser = spacy.load('en_core_web_sm')\nstop_words = stopwords.STOP_WORDS\n\n\nfor i in range(df.shape[0]):\n\n    keep = \"abcdefghijklmnopqrstuvwxyz \"\n    replace = \".,!;\"\n    tmp = \"\"\n    text_value = df[\"text\"].iloc[i]  \n\n\n    text_value = re.sub('[^a-zA-Z ]+', '', text_value.replace(\"&lt;br /&gt;\", \"\").lower())\n    text_value = parser(text_value)\n    tokens = [token.lower_ for token in text_value]\n    tokens = [token.lemma_ for token in text_value if token not in stop_words]\n\n\n    tmp = \" \".join(tokens)\n    texts.append(tmp)\nwordcloud_text = \" \".join(texts)\n\n\n\n\nCode\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n\n    def plot_cloud(wordcloud):\n        plt.figure(figsize=(40, 30))\n        plt.imshow(wordcloud) \n        plt.axis(\"off\");\n\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\nworldcloud_text = \" \".join(texts)\n\nwords_to_remove = [\"one\", \"go\", \"even\", \"give\", \"will\", \"need\", \"say\", \"well\", \"still\", \"make\", \"think\", \"look\", \"etc\", \"actually\", \"yet\", \"put\"]\n\nfor word in words_to_remove:\n    wordcloud_text = wordcloud_text.replace(word, \"\")\n\n\n\ngenerate_word_cloud(wordcloud_text)\n\n\n\n\n\n\n\n\n\nFrom this world cloud, we can see what topics are the most talked about by people regarding public transportation. This can help us see what matters to them the most and focus on these topics."
  },
  {
    "objectID": "Luna_Rover.html",
    "href": "Luna_Rover.html",
    "title": "Autonomous Lunar Rover Simulation through Reinforcement Learning",
    "section": "",
    "text": "Note: This project is the first step towards our final project on the Lunar Rover. It has served as the final project for the DSAN 6650 course. Me and Billy McGloin are currently working with Dr. Hickman to further develop this project and publish a paper on it."
  },
  {
    "objectID": "Luna_Rover.html#introduction",
    "href": "Luna_Rover.html#introduction",
    "title": "Autonomous Lunar Rover Simulation through Reinforcement Learning",
    "section": "Introduction",
    "text": "Introduction\nAdvancing the frontiers of scientific discoveries within space requires innovative technologies and tools. Reinforcement learning (RL), a type of machine learning where an agent learns to make decisions based on its objective and environment, has the capacity to address the unique challenges of traversing and operating in extraterrestrial environments. Through the guidance of Dr. Hickman, our team began to explore the field of RL in space exploration through an ongoing project at the National Aeronautics and Space Administration (NASA). In the coming years, NASA plans to explore the Moon through autonomous robotics within the Cooperative Autonomous Distributed Robotic Exploration (CADRE) program. Within this mission, three small rovers will traverse the moon’s landscape autonomously to map its surface, collect distributed measurements, and conduct various scientific experiments Aeronautics and Administration (2024). Thus, the utilization of RL simulations can act as the technological backbone for autonomous traversal in space exploration. With an understanding of what autonomous space exploration can look like in the future, we were also provided the technical foundation to explore such a topic via DSAN 6650: Geosearch. Through this assignment, a gridworld approach to simulation autonomous traversal via a robotic agent was implemented. Even while using a simple environment, Geosearch acted as a stepping stone to the objective and plan of our final project and its findings.\n\nStatement of Purpose\nThus, with both the background of autonomous space exploration through NASA’s upcoming mission and a foundation from Geosearch, this project combines the two to simulate a lunar rover that explores the lunar surface with its objective being to find and gather resources. The rover will have to navigate the lunar surface by avoiding obstacles and managing its energy consumption. The goal is to develop a simulation space that recreates the moon’s surface and train various RL algorithms such that the rover learns the environment and gathers various resources while controlling its battery and safety\nPlease note that for this iteration of the project, we have made assumptions on the moon’s geography and the inner workings of the rover to simplify the problem. These assumptions are detailed in the following sections. The objective is to simplify the problem to check its feasibility and expand on it in future studies."
  },
  {
    "objectID": "Luna_Rover.html#environment",
    "href": "Luna_Rover.html#environment",
    "title": "Autonomous Lunar Rover Simulation through Reinforcement Learning",
    "section": "Environment",
    "text": "Environment\nThe environment created is an attempt at simulating the intricacies and nuance of the moon’s surface. The implementation of this environment is a 35 by 35 grid-world, with each cell representing \\(1km^{2}\\) of the lunar surface. The environment also includes integral components necessary to the rover’s exploration and resource-gathering tasks. Primarily, time steps structure progression through the environment while surface elevation and lunar dust emulate the uneven nature of the lunar space. Additionally, sunlight levels are simulated based on the lunar cycle in order to accurately depict the rover’s energy intake time and locations. Lastly, the rover will navigate the environment in the hopes to find a resource. For the purpose of this simulation, resources are broken down into water and gold, all of which will be discussed in greater detail to follow.\n\nTime Steps\nThe simulated rover learns the environment and makes decisions at every time step. In order to depict the true nature of the moon’s cycle, each time step is equal to approximately 24 hours on Earth but proportional to 1 hour of Earth’s day-night cycle Oceanic and Administration (2024). Within each time step, the rover can move a maximum of 1 kilometer. After movement, the rover collects and updates all necessary environment information such as sunlight levels, overall elevation, and resources to calculate battery expenditure and reward. The rover continues this process, with every 30 time steps defined as one month. Thus, a single day-night cycle on the moon is one month long. The rover completes an episode which is defined as the length of a lunar year or earlier if the rover ends up in a terminal state. While this is a simplified version of the lunar cycle, it provides a baseline foundation for the rover to navigate the environment while properly receiving sunlight as energy.\n\n\nSunlight\nSunlight plays a crucial role in this environment as it is the singular energy intake resource for the rover. To reproduce the sunlight’s intensity throughout a lunar day-night cycle, Equation 1 calculates the intensity of the sunlight on a dimensionless scale between 0 and 1 through a sinusoidal equation:\n\\[\nsunlight = 0.5 * (1 + \\sin(2 \\times \\pi \\times (time\\_fraction - 0.25))) \\times height\\_factor\n\\tag{1}\\]\nThis diurnal cycle of sunlight will produce a value of 0 at nightfall and 1 at the highest peak of sunlight intensity. The sunlight will also be slightly less intense at lower points of elevation. The sunlight value is used to calculate total energy intake at each time step, which will be discussed later on. The purpose of adding sunlight to the environment is to allow the rover to plan its actions with the intent to prioritize recharging during peak sunlight and avoiding high-energy tasks during low sunlight time steps.\n\n\nSurface Elevation\nThe lunar surface consists of several points of inconsistent elevation and rough terrain. Therefore, to effectively simulate the lunar surface, we designed a terrain generation algorithm. Primarily, the base terrain is created based on random Gaussian noise, where every cell has an elevation based on the Gaussian value. Then, a Gaussian filter is applied to smooth the terrain and blend neighboring elevation values. A random walk algorithm is used to create mountain ranges in the environment as well as cliff points on the edges of the mountains. Lastly, craters were added based on set radius and crater depth. The terrain was again smoothed out using a Gaussian filter and normalized such that all surface elevation is between the range -50 to 50 meters. This surface aims to recreate the lunar surface as closely as possible with the restrictions of our current grid size.\n\n\nLunar Dust\nAnother contributing factor to the overall elevation of the lunar surface is lunar dust. However, unlike on Earth, dust behaves in erratic ways, changing by even the smallest amounts of activity Aeronautics and Administration (06-08-2021). Thus, to simulate the randomness of the lunar dust, we implemented an algorithm to calculate and map the dust levels onto the terrain. Primarily, we generated a base noise level for each cell in the grid through Python’s noise package. Similar to the surface elevation, we used a Gaussian filter to smooth out the edges of the dust patches to avoid extreme differences in height. The values are then normalized on a scale of 0 to 1 meters and inverted such that lower areas (bottoms of cliffs, bottoms of craters) receive higher dust accumulation (this is also dependent on a dust_height_correlation parameter). The final dust map for the environment is calculated in Equation 2, where the logic for lower areas is implemented through a height_influence variable.\n\\[\n\\begin{aligned}\n\\text{final\\_dust} = &\\ (\\text{dust\\_height\\_correlation} \\times \\text{height\\_influence}) \\\\\n&+ ((1 - \\text{dust\\_height\\_correlation}) \\times \\text{dust\\_map})\n\\end{aligned}\n\\tag{2}\\]\nLastly, the final dust map is normalized a second time to be a range of 0 to 0.5 meters for a more accurate height of lunar dust on the moon’s surface. Both surface elevation and lunar dust provide detailed depictions for the simulation model as well as contribute to a more realistic depiction of the rover’s energy usage as it traverses for resources.\n\n\nResources\nTaking a closer look at NASA’s plans for the CADRE project, their trio of rovers are built with sensors and ground-penetrating radar, able to detect resources below the surface that scientists may be unable to view through satellite imagery Aeronautics and Administration (2024). Thus, our rover’s mission will also be to traverse the environment for resources, with the added ability to gather said resources. For the purposes of this simulation, the two resource groups a rover could land on are water and gold, however, further iterations of this experiment would include more realistic resources and probability maps of the moon.\n\nWater\nIn both resource scenarios, the rover does not initially know where any of the resources are located. Thus, the rover is provided a probability distribution for both water and gold. Specifically for water, the probability distribution is of Gaussian nature, where the center of these water pools Figure 1 are randomized per episode. For every water resource, the Gaussian distribution is shaped through a covariance matrix, controlling both its spread and orientation. Additionally, to better simulate the unknown, we added a noise scale to the Gaussian values as means of providing randomness to the resource pools. To finalize the probability map, values under 0.15 are zeroed out and re-normalized to target potential resource regions of the map. We also clear out resource probabilities in the landing zone (center of the grid).\nThe ground truth of each episode is calculated with the probability map in mind. Using the probability map, the ground truth of water pools is calculated by converting probability values into binary values and adding additional noise and thresholding. Thus, with every gathering step, the rover detects whether or not it has hit a resource and updates the probability map through its immediate neighbors.\n\n\nGold\nThe calculation for the gold resource probabilities are slightly different to the water pools. Rather than pools of gold, the gold can be found in vein-like shapes across the environment Figure 1. These veins are calculated by randomizing a starting location and direction of growth and mapping the resource in the specified direction with conditions to ensure a minimum resource amount and length. After calculating this probability map, the rest of the process mirrors that of the water calculation. The landing area is again cleared to avoid the rover immediately landing on a resource. Lastly, the ground truth is calculated utilizing the probability map, adding noise and thresholding, and converting to binary values. It is important to note that for both water and gold, there is also a component of the implementation that ensures neither are overlapping each other within the environment.\nThus, our environment is calculated to address a robust simulation of the lunar space, its randomness and hurdles for the rover, and the unknown nature of available resources underground. Next, we seek to simulate a lunar rover’s abilities to traverse the environment and learn to balance exploration and gather while prioritizing its conservation of energy.\n\n\n\n\n\n\n\nFigure 1: Environment Visualization using PyGame"
  },
  {
    "objectID": "Luna_Rover.html#rover-agent",
    "href": "Luna_Rover.html#rover-agent",
    "title": "Autonomous Lunar Rover Simulation through Reinforcement Learning",
    "section": "Rover (Agent)",
    "text": "Rover (Agent)\nThe lunar rovers in the CADRE mission will be fully autonomous robots that are able to traverse the landscape and complete a variety of tasks. Thus, for the purposes of this simulation, the singular rover acts as the RL agent that will learn the environment over the course of an extensive episodic training process. The rover takes in information regarding the environment, as stated previously, and calculates the energy consumed and generated. This energy is in watt-hour (Wh) units for measuring total battery usage.\n\nBattery\nWithin our simulation, the rover contains two batteries, the same size as the Apollo Lunar Roving Vehicle (LRV) at a capacity of 29,300 Wh each, making it a total of 58,600 Wh. With the battery capacity grounded in reality, this iteration of the simulation will have a logical input and output of energy that the rover will observe.\n\nInput\nAs previously denoted, the main source of energy coming into the rover is through sunlight. The rover, which we are modeling after Apollo LRV, contains three solar panels. These panels convert sunlight into Wh units, where the maximum amount of energy produced in a day with maximum sunlight is 6,532.8 Wh. Taking the product of the daily output with the sunlight intensity weight produces the correct generated energy for that time step (Equation 3, Equation 4).\n\\[\n\\text{Daily Output} = 272.2 \\times 24 \\times \\text{num\\_solar\\_panels}\n\\tag{3}\\]\n\\[\n\\begin{align}\n\\text{Energy Generated} &= \\text{Daily Output} \\times \\text{Sunlight Intensity}\n\\end{align}\n\\tag{4}\\]\n\n\nOutput\nEnergy consumed by the rover is based on a multitude of factors. First, for every time step the rover uses a base consumption of 1,200 Wh for its systems and connectivity. For any action that is not staying still, the rover consumes 13,890 Wh per kilometer Wikipedia (2024). To this value, we multiply the calculated dust and height factor (Equation 5, Equation 6, Equation 7), which increases energy consumption to account for greater energy exertion in large elevation deltas. Lastly, if the rover chooses to gather a resource, it will automatically consume an additional 20,000 Wh. Thus, after calculating the energy consumed, the formula for the battery percentage at the next time step is the current battery level plus the energy generated minus the energy consumed (Equation 8).\n\\[\n\\begin{align}\n\\text{Dust Factor} &= 1 + (\\text{Dust Level} \\times 0.5)\n\\end{align}\n\\tag{5}\\]\n\\[\n\\begin{align}\n\\text{Height Factor} &= 0.5 + \\frac{100}{\\text{Height Difference}}\n\\end{align}\n\\tag{6}\\]\n\\[\n\\begin{align}\n\\text{Movement Energy} &= 13,890 \\times \\text{Dust Factor} \\times \\text{Height Factor}\n\\end{align}\n\\tag{7}\\]\n\\[\n\\begin{align}\n\\text{Next Battery Level} &= \\text{Current Battery Level} + \\nonumber \\\\\n& \\text{Energy Generated} - \\text{Energy Consumed}\n\\end{align}\n\\tag{8}\\]\n\n\n\nTerminal States\nThere are many instances where a rover can terminate its run prematurely. To further enhance the rover’s ability to learn to avoid certain areas in the environment and account for random failures, we have defined the following terminal states.\n\nCrash\nThe rover has a probability of crashing when the change in height from one cell to the next is higher than 25 meters. This is to simulate when the rover has either fallen down a cliff and crashed or it has flipped over. If the rover crashes, it will be considered a terminal state where it will be inoperable for the remainder of the episode.\n\n\nStuck\nAs mentioned previously, the rover operates in a challenging environment where deep moon dust poses a significant hazard. Using a sigmoid probability function, the rover’s chance of getting stuck increases dramatically as dust depth approaches 0.25 meters, reaching a maximum 50% probability at 0.5 meters depth. Once stuck, the rover continues consuming energy while unable to move. If the rover remains stuck for 5 consecutive days, the mission is considered a failure and reaches a terminal state.\n\n\nRandom Death\nTo emulate cases of random or unexpected component failures, the rover has a sigmoidal probability of failing over the year/episode. The rover’s probability of failing increases each day, reaching a maximum probability of 5% at the end of the episode (day 365)."
  },
  {
    "objectID": "Luna_Rover.html#action-and-rewards",
    "href": "Luna_Rover.html#action-and-rewards",
    "title": "Autonomous Lunar Rover Simulation through Reinforcement Learning",
    "section": "Action and Rewards",
    "text": "Action and Rewards\n\nAction Space\nAs we previously mentioned, the actions of the rover are significant to its energy consuming and traversal of the environment. When building off of the Geosearch assignment, we maintained the four cardinal directions for the rover: up, down, left, and right. In addition to the basic movements, we also implemented two new action steps. Staying still allows the rover to remain in the same cell as the previous action step to avoid consuming any energy. This is especially helpful during the dark time steps with no opportunities to absorb sunlight. Additionally, as we have previously mentioned, gathering is another new action that allows the rover to mine the area for potential resources.\n\n\nReward Space\nAn integral part of any RL problem is creating a coherent reward structure that incentives the agent to learn and complete its objectives effectively. Due to the intricate nature of this environment, there are several components to this reward structure. Our rover first starts off with a baseline of -1 reward for each step. For positive rewards, the rover receives a unique reward for gathering either water or gold times a delay factor in order to prevent over-mining at the same location. Additionally, the rover also receives a positive reward for making it through a month in order to incentivize longevity. However, for terminal states, as previously discussed, the rover receives hefty amounts of negative reward to avoid premature termination of an episode. Another negative reward implemented is for very low or very high battery levels to avoid the rover from either dying prematurely or staying too long in a high intensity sun spot. For more details on the amounts of each reward please refer to the table below.\n\n\n\n\n\n\n\n\n\nScenario\nReward/Penalty\nExplanation\n\n\n\n\nBase Time Penalty\n(-1)\nPenalizes every time step to encourage efficiency.\n\n\nMonthly Survival Bonus\n(+100)\nReward for surviving another month.\n\n\nStuck State Daily Penalty\n(-30)\nPenalty for being unable to move.\n\n\nTerminal Stuck Penalty\n(-100,000)\nEnding penalty if stuck for 5 consecutive days.\n\n\nCrash Penalty\n(-100,000)\nPenalty if the agent crashes due to height changes.\n\n\nGathering Water (Base)\n(+200 )\nReward for collecting water, reduced by decay for repeated gathers.\n\n\nGathering Gold (Base)\n(+300 )\nReward for collecting gold, reduced by decay for repeated gathers.\n\n\nLow Battery Penalty\n(-20)\nPenalty if the battery level drops below (20%).\n\n\nOvercharged Battery Penalty\n(-15)\nPenalty if the battery exceeds (95%)."
  },
  {
    "objectID": "Luna_Rover.html#solvers",
    "href": "Luna_Rover.html#solvers",
    "title": "Autonomous Lunar Rover Simulation through Reinforcement Learning",
    "section": "Solvers",
    "text": "Solvers\nThe rover’s ability to navigate the environment and optimize its actions depends on reinforcement learning (RL) solvers. We implemented and evaluated three solvers—Proximal Policy Optimization (PPO), Rainbow DQN, and Soft Actor-Critic (SAC)—to test their effectiveness in this complex lunar environment.\n\nPPO\nProximal Policy Optimization (PPO) is an on-policy RL algorithm designed for stability and efficiency in learning. Its clipped objective function ensures controlled updates to the policy, making it less prone to erratic behavior during training. However, in our environment, PPO struggled to generalize and optimize the rover’s actions effectively. The rover frequently resorted to repeating the same actions, leading to inefficient exploration and minimal resource gathering. This outcome highlights the limitations of PPO in handling the highly stochastic and dynamic challenges of our lunar simulation.\n\n\nRainbow DQN\nRainbow DQN, an advanced Deep Q-Network (DQN) variant, integrates multiple extensions, such as prioritized experience replay and double Q-learning, to improve stability and performance. As an off-policy method, Rainbow DQN excels in environments with discrete action spaces and rich reward structures. Despite its theoretical advantages, Rainbow DQN also underperformed in our environment, as the rover consistently favored a single action. This failure to explore and adapt suggests that the solver struggled with the complexity of the state space and reward dynamics, emphasizing the need for more training time or environment-specific customization.\n\n\nSAC\nSoft Actor-Critic (SAC) is an off-policy method that optimizes a stochastic policy while maximizing a trade-off between expected reward and entropy. This approach encourages exploration and allows for a more robust understanding of the environment. Among the solvers tested, SAC demonstrated the most promising results, enabling the rover to navigate effectively and gather with strategic pauses between actions. SAC’s ability to balance exploration and exploitation proved particularly advantageous in our environment, where careful planning is critical for success.\n\n\nYour browser does not support the video tag. \n\n\nResults and Insights\nThe performance differences among these solvers illustrate the complexity of our lunar simulation. SAC’s relative success in navigating the environment and gathering underscores its suitability for problems requiring strategic decision-making under uncertainty. Conversely, the suboptimal performance of PPO and Rainbow DQN suggests that further training or solver customization may be necessary to address the intricate challenges posed by the environment. Future work could explore tailored solvers or additional modifications to improve performance, such as incorporating continuous action spaces, advanced noise modeling, or hierarchical RL architectures."
  },
  {
    "objectID": "Luna_Rover.html#concluding-remarks",
    "href": "Luna_Rover.html#concluding-remarks",
    "title": "Autonomous Lunar Rover Simulation through Reinforcement Learning",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nFrom emulating a lunar environment and rover to solving an autonomous traversal and resource acquisition problem, we can conclude that RL is essential in the continuation of scientific discoveries in space. In terms of our simulation, we can see that the SAC solver was the best in its performance and ability to navigate through the environment’s obstacles and obtain resources. Going forward, we hope to build off this implementation for a more sophisticated simulation of lunar traversal. Examples include converting the action space to be continuous, expanding on the terrain, adding noise to sensor input, and using more robust solvers like dreamer V3. Thus, these advancements will bring us closer to developing intelligent systems capable of operating autonomously for space exploration."
  },
  {
    "objectID": "Luna_Rover.html#references",
    "href": "Luna_Rover.html#references",
    "title": "Autonomous Lunar Rover Simulation through Reinforcement Learning",
    "section": "References",
    "text": "References\n\n\nAeronautics, National, and Space Administration. 06-08-2021. “Dust: An Out-of-This World Problem.” https://www.nasa.gov/humans-in-space/dust-an-out-of-this-world-problem/.\n\n\n———. 2024. “Cooperative Autonomous Distributed Robotic Exploration.” https://www.nasa.gov/some-page.\n\n\nOceanic, National, and Atmospheric Administration. 2024. “Tides and Water Levels.” https://oceanservice.noaa.gov/education/tutorial_tides/tides05_lunarday.html.\n\n\nWikipedia. 2024. “Chandrayaan-2.” https://en.wikipedia.org/wiki/Chandrayaan-2."
  },
  {
    "objectID": "VoterFindings.html#overview",
    "href": "VoterFindings.html#overview",
    "title": "Findings",
    "section": "Overview",
    "text": "Overview\nThis tab contained several visualization detailing the results of our project, and a write-up discussing potential implications. It contains thoughts for how to expand upon this project in the future, and ends with a short conclusion."
  },
  {
    "objectID": "VoterFindings.html#our-results",
    "href": "VoterFindings.html#our-results",
    "title": "Findings",
    "section": "Our Results",
    "text": "Our Results\nAfter our data had been through our pipeline, we were able to confidently label 308,654 posts (including both original submissions and comments) across Reddit as partisan, leaning either Democrat or Republican. An additional 9,742,082 posts contained our keywords and were ran through sentiment analysis, but could not be categorized to our standards.\n\nNumber of Posts by Month\nWe can see the month-by-month breakdown of these strongly partisan posts below:\n\nFrom this we can glean that, overall, the number of Reddit posts with strongly partisan leanings identifiable by our process per party does not seem drastically different month to month; that is, in a given month, we will tend to see comparable numbers of partisan posts from both parties, regardless of any increase in the number of identifiably partisan posts made in a period (try saying that sentence aloud). This suggests that, when intentionally excluding the highly partisan subreddits mentioned at the beginning of this process, overall partisan content on Reddit is not strongly skewed one direction or the other. Political posts seem to also rise and fall together in number regardless of party. There are a few interesting things to highlight from this graph, however:\n\nBoth parties peaked in aggregate number of posts in June, suggesting either a high rate of partisan posting or a stronger positive/negative sentiment of posts that made them identifiable to us. Either there were more posts on political subjects, or tempers were high.\nNovember saw a very high rate of positive posts concerning Republicans (or the contra, negative posts concerning Democrats), especially when compared to the declining number of Democratic leaning posts since September, when Republican posting was at a significant low after the height of the summer months.\nBoth March and September saw somewhat abberant behavior, at least when compared to previous months, in that we saw a spike in pro-Democrat / anti-Republican postings unaccompanied by a similar spike in pro-Republican / anti-Democrat posts.\n\n\n\nThe Reddit Thermometer\nNow that we have taken a look at the partisanship of postings by monthly totals, let’s build our thermometer with our approval ratings and compare it to the polls!\nHere are our results after aggregating the Reddit data as described in the “Reddit Results Extraction” section:\n\nSo that it is fresh in your head, here is that aggregation trend line FiveThirtyEight produced for the 2022 election again:\n\nWe’ve added a yellow bounding box for the data concerning only 2022, which is the data we used in our Reddit thermometer creation. Let’s compare this to our findings for the same period.\nThis is quite remarkable to us. While both the Reddit data and FiveThirtyEight’s aggregated polling appears to show a trend towards Republican popularity increasing right before the election in November, our findings disagree significantly from what the polling suggested during the rest of the year. Reddit as a whole seems to fluctuate a bit on positivity towards each party month by month, defying the smooth trends we see in FiveThirtyEight’s graph. Additionally, while an inversion in popularity occurs on both graphs in the early fall, the Democratic popularity soars on Reddit in September, which is not captured at all in this graph of FiveThirtyEight’s. Perhaps there are some key events that may add context to these differences.\nHere are our results again, this time with the overlay of three key dates that we suspected may have been large factors in the election:\n\nWe suspected at the outset of this project that three events would have generated high amounts of chatter around the election in the lead-up to the midterms: Russia’s invasion of Ukraine, the leak of an upcoming Supreme Court decision that would repeal of Roe v Wade, and the filings in New York suing former President Trump.\nWhile we cannot be certain without a closer analysis of the posts made during these specific timeframes, we suspect that these events did have an effect on the popularity of both parties on Reddit. Democratic supporters are more abundant on Reddit immediately following the Russian invasion of Ukraine, but then fall off in relation to Republicans as the months go on, perhaps indicating a frustration of Democratic President Joe Biden’s handling of the crisis, or a lack of fervor following the actual event and initial outpouring of sentiment.\nWe see what could be a smaller and possibly delayed reaction in terms of swing in approval around the leak of the Dobbs decision, although we cannot be certain the small spike in June’s approval ratio for Democrats is due to the timing of this information the month prior. This was also when the January 6th hearings were being televised over the summer, which galvanized folks of both parties, making it hard to separate out the two events. Given the high volume of identifiably partisan posts in June, we expect that both events created a flurry of activity over the summer months from both sides, with Democrats being slightly more prolific in terms of the amount of strongly partisan posts.\nIt does seem likely to us that the massive peak in September is related to the release of the news surrounding the suits against Trump. While again we cannot be certain without more of an in-depth analysis, this seems to be a significant turning point - Democratically partisan posts were high, second only to the activity peak already discussed in June, whereas Republican partisan posts were quite low, the lowest they’d been since February. We have several theories for this!\nThis September spike may have been because pro-Republican postings took place more on the removed highly partisan subreddits, whereas pro-Democratic postings felt they had more free reign to discuss partisan topics in more diverse subreddits. We could also be seeing this result due to degree of feeling: if, for instance, Democratic posts were more strongly positive whereas posts containing Republican keywords were more subdued, this could have resulted in our labelling logic missing the quieter pro-Republican posts. This could also indicate that posters who would normally be posting positively about right-leaning keywords would be critiquing former President Trump’s behavior and moving across party lines. And finally, a fourth theory admits there could also be a flaw in the labelling logic itself: if a post was expressing strong disgust over the indictments as being politically motivated, but did not mention keywords other than those on the right, this could have been erroneously sorted into our pro-Democratic category when labelled.\n\n\nComparison of the Reddit Thermometer to Unweighted Polls\nWe’ve been frequently comparing our results to the weighted polling aggregates that FiveThirtyEight created above - but what happens if we take a look at the polling data directly ourselves? Here we can see how our Reddit thermometer compares to the aggregated polling data across the same time period without any weighting from FiveThirtyEight:\n\nNow, this is quite different! The graph above has in the heavy non-dashed lines the straight averages month by month of every poll in the FiveThirtyEight dataset. These are not weighted for accuracy, for reach, even for sample size; this is just a straight average of folks who preferred Republicans vs folks who preferred Democrats. This is overlaid atop our Reddit thermometer in dashed and lighter lines.\nEven when simply comparing this to FiveThirtyEight’s graph from above, there are some interesting differences. During this same period, FiveThirtyEight’s weighting shows Republicans as ahead more often than not, whereas when taken as a simple average, the Democrats seem to be leading most of the months right before the election, with the gap narrowing in November. This suggests that part of why the Red Wave was predicted yet failed to manifest is an over-emphasizing of polls that skewed towards preferring Republicans. But that is a question for a different group of people to debate.\nIn terms of how this un-weighted average aligns with our data, we see some similarities but also some differences. The massive spike we see in September seems to be less out of the norm (if still surprising in magnitude) when viewed against the slow gentle rise in Democratic popularity since the start of the summer. This suggests that our earlier findings - that the combination of Dobbs and the January 6th hearings may have pushed more folks to view the Democrats favorably and the Republicans negatively - may have been reflected in several polls, especially when viewed in this unweighted manner."
  },
  {
    "objectID": "VoterFindings.html#future-avenues-of-exploration",
    "href": "VoterFindings.html#future-avenues-of-exploration",
    "title": "Findings",
    "section": "Future Avenues of Exploration",
    "text": "Future Avenues of Exploration\nNow that we have finished our results, we’d like to end with a brief look at possible ways to expand or refine this project.\nTo start with, we’d like to re-attempt this project with more sophisticated labelling logic. As you can see from the tab discussing the labelling section of the project, we were very tentative in labelling our data. If we could not be strongly certain that a post (either a submission or a comment) was able to be grouped into one camp or another, it was put into the “No Party” bucket, which then became a catch-all for any posts were not easily sorted by the binary “this or that” system. Given more resources, we think this could be expanded upon to include more refined labelling logic that has more branches (perhaps the random forest to our current decision-tree-esque process).\nWe would also suggest a re-run of this process with the additional step of aggregating posts by user before applying a label. This would have a dual effect of reducing the number of rows to work with (although perhaps also widening the data quite a bit), and of being able to potentially “tie” a political but non preferential posting to a Democratic- or Republican-leaning author. This would also make the direct comparison to polls a bit more justifiable to a layperson as, after all, people are the ones that cast a ballot, not posts.\nWhile we have great confidence in our keywords, and in the choice to initially lemmatize many of them in order to cast the widest possible net at the outset of our project, in another iteration of this same project we would have sorted more of the keywords into left- or right-leaning camps, and perhaps into more than two simple categories. You can see an early hint of this in our labelling logic, where a select few generic keywords are pulled out to be considered in a partisan manner, but this process could use an expansion given more time and expertise. Perhaps a weighting system prioritizing some keywords over others would be superior to our binary “is the word present” analysis.\nAnd lastly, given more resources to run more queries, we would have liked to add a geographical component to further segregate the data by the political race being discussed. This could have allowed for some fine-tuning in the poll comparison, allowing us to look at multiple races at once instead of the overall trendline, perhaps to see if there were races that skewed the result or that drew more attention than other. This would likely have to involve the re-inclusion of our previous excluded subreddits, like r/Democrats and r/Republicans, with some sort of weighting system again, this time to limit the skew of the heavily partisan nature of these subreddits."
  },
  {
    "objectID": "VoterFindings.html#in-conclusion",
    "href": "VoterFindings.html#in-conclusion",
    "title": "Findings",
    "section": "In Conclusion",
    "text": "In Conclusion\nOverall, we are quite please with this project and with our findings. We are content with the performance of the sentiment analysis model from Jon Snow Labs. We have confidence in our labelling logic as successfully identifying if a post is strongly partisan in favor of one party or another. We are happy to see that our model for labelling future Reddit posts based on our pipeline is fairly accurate and much more easily deployable than the many jobs we ran to get from A to B. And we are delighted that, after all that, we were able to produce a metric that could be succesfully compared to real-life polling to better understand the results of the election in 2022.\nIn general, it seems that Reddit was more accurate than the aggregated polling in the year before the election - or, if not more accurate, at least less wrong. The back-and-forth observed each month between numbers of posts speaking positively on one party or another indicates to us a high amount of dialogue concerning the ideologies of both parties, and no clear winner emerges on this Reddit stage across multiple months. Reddit, at least, did not seem poised to flood the elections with a Red Wave. That could be a significant part of why the Red Wave did not materialize as predicted: the people having the conversations with one another, convincing one another, were not being reflected in the polls. Or at least, in the weighted averages of the polls.\nIn conclusion: you might be able to trust the polls, but you probably can’t trust Nate Silver."
  },
  {
    "objectID": "ANN.html",
    "href": "ANN.html",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "ANN training in Keras or Pytorch & Hyper-parameter tuning\n\n\n\nClassification is one of the most common forms of supervised machine learning\nIn this homework we will explore “model tuning” for the case of a multi-class classification problem, as applied the MNIST data set\nYou can do this assignment in either Keras OR PyTorch (or both), it is your choice.\n\n\n\n\n\nYou need to upload TWO documents to Canvas when you are done\n\n\nA PDF (or HTML) of the completed form of the HW-2.ipynb document\n\n\nThe final uploaded version should NOT have any code-errors present\nAll outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\nIMPORTANT: THERE ARE MANY WAYS TO DO THIS, SO FEEL FREE TO DEVIATE SLIGHTLY FROM THE EXACT DETAILS, BUT THE OVERALL RESULT AND FLOW SHOULD MATCH WHAT IS OUTLINED BELOW.\n\n\n\nI printed the outputs as the runtimes were very long and it is common practice. Furthermore, even though it will be mentioned later, the gridsearch for the choice with l1 and l2 was done in a 5 by 5 grid instead of a 10 by 10 grid. Professor James said that it was okay to do so.\n\n\n\n\nNormalize the data as needed\nPartition data into training, validation, and test (i.e. leave one out CV)\n\nOne option to do this is to give these arrays global scope so they are seen inside the training function (so they don’t need to be passed to functions)\n\nOptional but recommended: Create a K-fold cross validation data set, rather than just doing leave one out\nDo any other preprocessing you feel is needed\n\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader, random_split\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom sklearn.model_selection import KFold\nimport copy\nimport pandas as pd\nimport seaborn as sns\n\n\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\n# You can also create DataLoader instances for both datasets to easily iterate during training and validation\nbatch_size = 64\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\n\n# explore data-set object\nprint(\"\\n TYPE:\\n\",type(training_data))\nprint(\"\\n SUMMARY:\\n\",training_data)\nprint(\"\\n ATTRIBUTES:\\n\",training_data.__dict__.keys())\nprint(\"\\n FIRST DATA POINT:\\n\",)\nimg, label = training_data[0]\nprint(img.shape,label)\nprint(len(training_data))\n# print(training_data.samples)\n\n\n TYPE:\n &lt;class 'torchvision.datasets.mnist.FashionMNIST'&gt;\n\n SUMMARY:\n Dataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor()\n\n ATTRIBUTES:\n dict_keys(['root', 'transform', 'target_transform', 'transforms', 'train', 'data', 'targets'])\n\n FIRST DATA POINT:\n\ntorch.Size([1, 28, 28]) 9\n60000\n\n\n\n# from demonstart data loader\n# for X, y in test_dataloader:\nfor X, y in train_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64\n\n\n\n\n\n\nCreate a General model function (or class) that takes hyper-parameters and evaluates the model\n\nThe function should work with a set of hyper parameters than can be easily be controlled and varied by the user (for later parameter tuning)\nThis should work for the training, test, and validation set\n\nFeel free to recycle code from the lab assignments and demo’s\n\nUse the deep learning best practices that we discussed in class.\nDocument what is going on in the code, as needed, with narrative markdown text between cells.\n\n\n# Get cpu or gpu device for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Define neural network\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, layer_sizes, activation_funcs, dropout_rate=0.0, weight_init='random'):\n        # Initialize neural network\n        super(NeuralNetwork, self).__init__()\n        # flatten the imput (as they are images)\n        layers = [nn.Flatten()]\n        # prev_size as size of previous imputs to track the next layer size\n        prev_size = input_size\n\n        # weight initialization\n        def init_weights(layer):\n            if weight_init == 'xavier':\n                if isinstance(layer, nn.Linear):\n                    init.xavier_uniform_(layer.weight)\n\n        # loop through the layers or the network\n        for i, size in enumerate(layer_sizes):\n            # fully connected layers\n            linear_layer = nn.Linear(prev_size, size)\n\n            layers.append(linear_layer)\n            \n            # activation functions\n            activation_func = activation_funcs[i]\n            if activation_func == 'relu':\n                layers.append(nn.ReLU())\n            # sigmoid activation function as default for classification\n            elif activation_func == 'sigmoid':\n                layers.append(nn.Sigmoid())\n            # add dropout layer if dropout rate is greater than 0\n            if dropout_rate &gt; 0:\n                layers.append(nn.Dropout(dropout_rate))\n            # set the size for the next layer\n            prev_size = size\n        \n        # Output layer with 10 classes\n        output_layer = nn.Linear(prev_size, 10) \n\n        # Initialize weights of output layer\n        init_weights(output_layer)\n        \n        layers.append(output_layer)\n\n        # Store layers in Sequential container\n        self.model = nn.Sequential(*layers)\n    # forward pass through the network\n    def forward(self, X):\n        return self.model(X)\n\n# define model\ndef model_fn():\n    return NeuralNetwork(input_size=28*28, layer_sizes=[512, 256], activation_funcs=['relu', 'relu'], dropout_rate=0.5)\n\nUsing cpu device\n\n\n\n\n\n\nYou can do this in either a function (or python class), or however you think is best.\nCreate a training function (or class) that takes hyper-parameter choices and trains the model\n\nIf you are doing “leave one out”, your training function only needs to do one training per hyper-parameter choice\nIf you are doing K-fold cross validation, you should train the model K times for each hyper-parameter choice, and report the average result cross the training runs at the end (this is technically a better practice but requires more computation).\nUse a dense feed forward ANN model, with the correct output layer activation, and correct loss function\nYou MUST use early stopping inside the function, otherwise it defeats the point\nHave at least the following hyper-parameters as inputs to this function\n\nL1 regularization constant, L2 regularization constant, dropout rate\nLearning rate\nWeight Initialization: Fully random vs Xavier Weight Initialization\nHidden layer activation function choice (use relu, sigmoid, or tanh)\nNumber and size of ANN hidden layers\nOptimizer choice, have at least three included (Adam, SGD, or RmsProp)\nYou can wrap all of the hyper-parameter arguments into a dictionary, or do it however you want\n\n\nVisualization\n\nInclude a boolean parameter as a function input that controls whether visualization is created or not\nIf true, Monitor training and validation throughout training by plotting\nReport a confusion matrix\n\nReturn the final training and validation error (averaged if using K-fold)\n\nagain, you must use early stopping to report the best training/validation loss without over-fitting\n\n\nDepending how you do this, it can be a lot of computation, start small and scale up and consider using Co-lab\n\n\n# model evaluation\ndef evaluate_model(model, dataloader, loss_fn, device):\n    # Set model to evaluation mode\n    model.eval()\n    # Initialize metrics\n    total_loss, correct, total = 0, 0, 0\n    # no gradients during evaluation (saving memory and computations)\n    with torch.no_grad():\n        # loop over batches of data\n        for X, y in dataloader:\n            # move data to device\n            X, y = X.to(device), y.to(device)\n            # predict on X\n            outputs = model(X)\n            # calculate loss\n            loss = loss_fn(outputs, y)\n            # extract and add loss to the total loss\n            total_loss += loss.item()\n            # calculate correct predictions\n            correct += (outputs.argmax(1) == y).sum().item()\n            # number of samples\n            total += y.size(0)\n    # return average loss and accuracy\n    return total_loss / len(dataloader), correct / total\n\n# train model\ndef training_model_kfold(k_folds, dataset, model_fn, loss_fn, optimizer_name, lr, l1_reg, l2_reg, num_epochs=100, device='cpu', plot=False, patience=4):\n    # Divide into k folds\n    kfold = KFold(n_splits=k_folds, shuffle=True)\n    # Initialize metrics to store\n    metrics_record = {'val_loss': [], 'val_accuracy': [], ['train_loss'] : []}\n\n    # Loop through the folds\n    for fold, (train_ids, test_ids) in enumerate(kfold.split(np.arange(len(dataset)))):\n        print(f'FOLD {fold}')\n        print('--------------------------------')\n\n        # Reset early stopping and best model tracking for each fold\n        early_stopping_wait_number = 0\n        best_val_loss = np.inf\n        best_model = None\n        \n        # Initialize plotting lists for each fold (plot=True only)\n        fold_performance = {'train_loss': [], 'val_loss': [], 'val_accuracy': []} if plot else None\n\n        # data loaders for training and testing data in this fold (better for batch processing)\n        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n        trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=train_subsampler)\n        testloader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=test_subsampler)\n\n        # Initialize neural network\n        model = model_fn().to(device)\n\n        # Choose optimizer (added more as probably will be used in the future)\n        optimizer = {\n            \"Adam\": torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg),\n            \"SGD\": torch.optim.SGD(model.parameters(), lr=lr, weight_decay=l2_reg),\n            \"RMSProp\": torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=l2_reg),\n            \"LBFGS\": torch.optim.LBFGS(model.parameters(), lr=lr)\n        }.get(optimizer_name, None)\n        # raise flag if optimizer is not recognized\n        if optimizer is None:\n            raise ValueError(f'Optimizer {optimizer_name} not recognized')\n        # loop through the epochs\n        for epoch in range(num_epochs):\n            # Training\n            model.train()\n            # initialize total loss\n            total_loss = 0.0\n            # loop through the batches\n            for batch, (X, y) in enumerate(trainloader):\n                # move data to device\n                X, y = X.to(device), y.to(device)\n                # zero the gradients\n                optimizer.zero_grad()\n                outputs = model(X)\n                loss = loss_fn(outputs, y)\n                l1_norm = sum(p.abs().sum() for p in model.parameters())\n                loss += l1_reg * l1_norm\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            average_train_loss_for_fold = total_loss / len(train_ids)\n            metrics_record['train_loss'].append(average_train_loss_for_fold)\n\n\n            # Validation\n            val_loss, correct = 0.0, 0\n            # set model to evaluation mode\n            model.eval()\n            with torch.no_grad():\n                for X, y in testloader:\n                    X, y = X.to(device), y.to(device)\n                    outputs = model(X)\n                    loss = loss_fn(outputs, y)\n                    val_loss += loss.item()\n                    correct += (outputs.argmax(1) == y).sum().item()\n\n            val_loss /= len(testloader)\n            val_accuracy = correct / len(test_ids)\n\n            # plotting data for this fold\n            if plot:\n                fold_performance['train_loss'].append(total_loss / len(train_ids))\n                fold_performance['val_loss'].append(val_loss)\n                fold_performance['val_accuracy'].append(val_accuracy)\n\n            # Set early stopping\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_model = copy.deepcopy(model.state_dict())\n                early_stopping_wait_number = 0\n            else:\n                early_stopping_wait_number += 1\n                if early_stopping_wait_number &gt;= patience:\n                    print(f\"Stopping early at epoch {epoch+1}\")\n                    break\n\n            print(f'Epoch {epoch+1}/{num_epochs} - Loss: {total_loss / len(train_ids):.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n\n        # Load best model fold\n        model.load_state_dict(best_model)\n\n        # save the best model of each fold to disk\n        torch.save(best_model, f'best_model_fold_{fold}.pth')\n\n        # Evaluate the model on test set \n        fold_val_loss, fold_val_accuracy = evaluate_model(model, testloader, loss_fn, device)\n        metrics_record['val_loss'].append(fold_val_loss)\n        metrics_record['val_accuracy'].append(fold_val_accuracy)\n\n        # Plot for each fold\n        if plot:\n            plt.figure(figsize=(12, 6))\n            plt.subplot(1, 3, 1)\n            plt.plot(fold_performance['train_loss'], label='Training Loss')\n            plt.title('Training Loss')\n            plt.legend()\n            plt.subplot(1, 3, 2)\n            plt.plot(fold_performance['val_loss'], label='Validation Loss')\n            plt.title('Validation Loss')\n            plt.legend()\n            plt.subplot(1, 3, 3)\n            plt.plot(fold_performance['val_accuracy'], label='Validation Accuracy')\n            plt.title('Validation Accuracy')\n            plt.legend()\n            plt.show()\n        \n        print('--------------------------------')\n        print(f'Finished FOLD {fold}')\n        print('--------------------------------')\n    \n    # print validation loss and accuracy per fold\n    print(\"Validation Loss and Accuracy per Fold:\")\n    for i, (loss, accuracy) in enumerate(zip(metrics_record['val_loss'], metrics_record['val_accuracy'])):\n        print(f\"Fold {i}: Validation Loss = {loss}, Validation Accuracy = {accuracy}\")\n\n    # avg performance across folds\n    average_val_loss = np.mean(metrics_record['val_loss'])\n    average_val_accuracy = np.mean(metrics_record['val_accuracy'])\n\n    average_val_loss = np.mean(metrics_record['val_loss'])\n    average_val_accuracy = np.mean(metrics_record['val_accuracy'])\n    average_train_loss = np.mean(metrics_record['train_loss'])\n\n    print(f\"Average Validation Loss across folds: {average_val_loss}\")\n    print(f\"Average Validation Accuracy across folds: {average_val_accuracy}\")\n    print('--------------------------------')\n    print('K-fold Cross-validation completed.')\n    # return avg validation loss, avg validation accuracy, and avg training loss\n    return average_val_loss, average_val_accuracy, average_train_loss\n\nHere, we are training the model once to see how it works and with the default parameters we picked.\n\n# train model\ntraining_model_kfold(k_folds=4, dataset=training_data, model_fn=model_fn, loss_fn=nn.CrossEntropyLoss(), optimizer_name=\"Adam\", lr=0.001, l1_reg=0, l2_reg=0, num_epochs=100, device='cpu', plot=True)\n\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0102, Val Loss: 0.4388, Val Accuracy: 0.8372\nEpoch 2/100 - Loss: 0.0074, Val Loss: 0.3866, Val Accuracy: 0.8532\nEpoch 3/100 - Loss: 0.0069, Val Loss: 0.3615, Val Accuracy: 0.8645\nEpoch 4/100 - Loss: 0.0065, Val Loss: 0.3604, Val Accuracy: 0.8628\nEpoch 5/100 - Loss: 0.0063, Val Loss: 0.3463, Val Accuracy: 0.8689\nEpoch 6/100 - Loss: 0.0061, Val Loss: 0.3351, Val Accuracy: 0.8772\nEpoch 7/100 - Loss: 0.0059, Val Loss: 0.3452, Val Accuracy: 0.8725\nEpoch 8/100 - Loss: 0.0058, Val Loss: 0.3350, Val Accuracy: 0.8766\nEpoch 9/100 - Loss: 0.0057, Val Loss: 0.3224, Val Accuracy: 0.8785\nEpoch 10/100 - Loss: 0.0056, Val Loss: 0.3237, Val Accuracy: 0.8793\nEpoch 11/100 - Loss: 0.0055, Val Loss: 0.3174, Val Accuracy: 0.8814\nEpoch 12/100 - Loss: 0.0054, Val Loss: 0.3174, Val Accuracy: 0.8799\nEpoch 13/100 - Loss: 0.0053, Val Loss: 0.3184, Val Accuracy: 0.8829\nEpoch 14/100 - Loss: 0.0053, Val Loss: 0.3096, Val Accuracy: 0.8849\nEpoch 15/100 - Loss: 0.0052, Val Loss: 0.3127, Val Accuracy: 0.8847\nEpoch 16/100 - Loss: 0.0052, Val Loss: 0.3216, Val Accuracy: 0.8812\nEpoch 17/100 - Loss: 0.0051, Val Loss: 0.3096, Val Accuracy: 0.8861\nEpoch 18/100 - Loss: 0.0050, Val Loss: 0.3079, Val Accuracy: 0.8875\nEpoch 19/100 - Loss: 0.0051, Val Loss: 0.3158, Val Accuracy: 0.8822\nEpoch 20/100 - Loss: 0.0050, Val Loss: 0.3032, Val Accuracy: 0.8879\nEpoch 21/100 - Loss: 0.0050, Val Loss: 0.3124, Val Accuracy: 0.8808\nEpoch 22/100 - Loss: 0.0048, Val Loss: 0.3084, Val Accuracy: 0.8871\nEpoch 23/100 - Loss: 0.0048, Val Loss: 0.3042, Val Accuracy: 0.8857\nEpoch 24/100 - Loss: 0.0048, Val Loss: 0.3045, Val Accuracy: 0.8864\nStopping early at epoch 25\n\n\n\n\n\n\n\n\n\nAverage Validation Loss across folds: 0.3041553478291694\nAverage Validation Accuracy across folds: 0.8878666666666667\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0102, Val Loss: 0.4408, Val Accuracy: 0.8400\nEpoch 2/100 - Loss: 0.0075, Val Loss: 0.4002, Val Accuracy: 0.8505\nEpoch 3/100 - Loss: 0.0068, Val Loss: 0.3707, Val Accuracy: 0.8677\nEpoch 4/100 - Loss: 0.0066, Val Loss: 0.3751, Val Accuracy: 0.8635\nEpoch 5/100 - Loss: 0.0063, Val Loss: 0.3623, Val Accuracy: 0.8673\nEpoch 6/100 - Loss: 0.0062, Val Loss: 0.3423, Val Accuracy: 0.8753\nEpoch 7/100 - Loss: 0.0059, Val Loss: 0.3427, Val Accuracy: 0.8754\nEpoch 8/100 - Loss: 0.0058, Val Loss: 0.3425, Val Accuracy: 0.8767\nEpoch 9/100 - Loss: 0.0058, Val Loss: 0.3278, Val Accuracy: 0.8814\nEpoch 10/100 - Loss: 0.0056, Val Loss: 0.3278, Val Accuracy: 0.8817\nEpoch 11/100 - Loss: 0.0056, Val Loss: 0.3329, Val Accuracy: 0.8795\nEpoch 12/100 - Loss: 0.0054, Val Loss: 0.3309, Val Accuracy: 0.8803\nEpoch 13/100 - Loss: 0.0054, Val Loss: 0.3216, Val Accuracy: 0.8834\nEpoch 14/100 - Loss: 0.0053, Val Loss: 0.3155, Val Accuracy: 0.8862\nEpoch 15/100 - Loss: 0.0052, Val Loss: 0.3307, Val Accuracy: 0.8825\nEpoch 16/100 - Loss: 0.0052, Val Loss: 0.3109, Val Accuracy: 0.8865\nEpoch 17/100 - Loss: 0.0051, Val Loss: 0.3207, Val Accuracy: 0.8860\nEpoch 18/100 - Loss: 0.0051, Val Loss: 0.3162, Val Accuracy: 0.8842\nEpoch 19/100 - Loss: 0.0050, Val Loss: 0.3171, Val Accuracy: 0.8847\nEpoch 20/100 - Loss: 0.0050, Val Loss: 0.3247, Val Accuracy: 0.8839\nEpoch 21/100 - Loss: 0.0049, Val Loss: 0.3091, Val Accuracy: 0.8883\nEpoch 22/100 - Loss: 0.0049, Val Loss: 0.3155, Val Accuracy: 0.8889\nEpoch 23/100 - Loss: 0.0048, Val Loss: 0.3176, Val Accuracy: 0.8882\nEpoch 24/100 - Loss: 0.0048, Val Loss: 0.3089, Val Accuracy: 0.8939\nEpoch 25/100 - Loss: 0.0049, Val Loss: 0.3017, Val Accuracy: 0.8932\nEpoch 26/100 - Loss: 0.0047, Val Loss: 0.3256, Val Accuracy: 0.8843\nEpoch 27/100 - Loss: 0.0047, Val Loss: 0.3252, Val Accuracy: 0.8799\nEpoch 28/100 - Loss: 0.0046, Val Loss: 0.3190, Val Accuracy: 0.8903\nEpoch 29/100 - Loss: 0.0047, Val Loss: 0.3098, Val Accuracy: 0.8893\nStopping early at epoch 30\n\n\n\n\n\n\n\n\n\nAverage Validation Loss across folds: 0.30303091040634095\nAverage Validation Accuracy across folds: 0.8905333333333334\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0101, Val Loss: 0.4239, Val Accuracy: 0.8476\nEpoch 2/100 - Loss: 0.0074, Val Loss: 0.3951, Val Accuracy: 0.8584\nEpoch 3/100 - Loss: 0.0069, Val Loss: 0.3622, Val Accuracy: 0.8667\nEpoch 4/100 - Loss: 0.0066, Val Loss: 0.3618, Val Accuracy: 0.8703\nEpoch 5/100 - Loss: 0.0063, Val Loss: 0.3505, Val Accuracy: 0.8724\nEpoch 6/100 - Loss: 0.0063, Val Loss: 0.3495, Val Accuracy: 0.8761\nEpoch 7/100 - Loss: 0.0060, Val Loss: 0.3389, Val Accuracy: 0.8799\nEpoch 8/100 - Loss: 0.0059, Val Loss: 0.3376, Val Accuracy: 0.8791\nEpoch 9/100 - Loss: 0.0058, Val Loss: 0.3334, Val Accuracy: 0.8809\nEpoch 10/100 - Loss: 0.0057, Val Loss: 0.3264, Val Accuracy: 0.8831\nEpoch 11/100 - Loss: 0.0055, Val Loss: 0.3250, Val Accuracy: 0.8833\nEpoch 12/100 - Loss: 0.0055, Val Loss: 0.3181, Val Accuracy: 0.8866\nEpoch 13/100 - Loss: 0.0054, Val Loss: 0.3309, Val Accuracy: 0.8794\nEpoch 14/100 - Loss: 0.0053, Val Loss: 0.3145, Val Accuracy: 0.8875\nEpoch 15/100 - Loss: 0.0053, Val Loss: 0.3254, Val Accuracy: 0.8825\nEpoch 16/100 - Loss: 0.0052, Val Loss: 0.3078, Val Accuracy: 0.8891\nEpoch 17/100 - Loss: 0.0052, Val Loss: 0.3187, Val Accuracy: 0.8844\nEpoch 18/100 - Loss: 0.0051, Val Loss: 0.3078, Val Accuracy: 0.8905\nEpoch 19/100 - Loss: 0.0050, Val Loss: 0.3125, Val Accuracy: 0.8881\nEpoch 20/100 - Loss: 0.0050, Val Loss: 0.3138, Val Accuracy: 0.8837\nEpoch 21/100 - Loss: 0.0050, Val Loss: 0.3023, Val Accuracy: 0.8906\nEpoch 22/100 - Loss: 0.0048, Val Loss: 0.3089, Val Accuracy: 0.8883\nEpoch 23/100 - Loss: 0.0049, Val Loss: 0.3162, Val Accuracy: 0.8813\nEpoch 24/100 - Loss: 0.0048, Val Loss: 0.3111, Val Accuracy: 0.8886\nEpoch 25/100 - Loss: 0.0048, Val Loss: 0.3056, Val Accuracy: 0.8881\nStopping early at epoch 26\n\n\n\n\n\n\n\n\n\nAverage Validation Loss across folds: 0.3031931030412092\nAverage Validation Accuracy across folds: 0.8905555555555557\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0102, Val Loss: 0.4590, Val Accuracy: 0.8276\nEpoch 2/100 - Loss: 0.0074, Val Loss: 0.3938, Val Accuracy: 0.8553\nEpoch 3/100 - Loss: 0.0068, Val Loss: 0.3764, Val Accuracy: 0.8630\nEpoch 4/100 - Loss: 0.0065, Val Loss: 0.3597, Val Accuracy: 0.8689\nEpoch 5/100 - Loss: 0.0063, Val Loss: 0.3629, Val Accuracy: 0.8644\nEpoch 6/100 - Loss: 0.0061, Val Loss: 0.3504, Val Accuracy: 0.8704\nEpoch 7/100 - Loss: 0.0059, Val Loss: 0.3478, Val Accuracy: 0.8666\nEpoch 8/100 - Loss: 0.0057, Val Loss: 0.3447, Val Accuracy: 0.8745\nEpoch 9/100 - Loss: 0.0057, Val Loss: 0.3512, Val Accuracy: 0.8686\nEpoch 10/100 - Loss: 0.0056, Val Loss: 0.3322, Val Accuracy: 0.8759\nEpoch 11/100 - Loss: 0.0055, Val Loss: 0.3250, Val Accuracy: 0.8819\nEpoch 12/100 - Loss: 0.0055, Val Loss: 0.3284, Val Accuracy: 0.8801\nEpoch 13/100 - Loss: 0.0053, Val Loss: 0.3178, Val Accuracy: 0.8854\nEpoch 14/100 - Loss: 0.0052, Val Loss: 0.3230, Val Accuracy: 0.8819\nEpoch 15/100 - Loss: 0.0052, Val Loss: 0.3225, Val Accuracy: 0.8807\nEpoch 16/100 - Loss: 0.0051, Val Loss: 0.3126, Val Accuracy: 0.8848\nEpoch 17/100 - Loss: 0.0051, Val Loss: 0.3088, Val Accuracy: 0.8860\nEpoch 18/100 - Loss: 0.0050, Val Loss: 0.3171, Val Accuracy: 0.8856\nEpoch 19/100 - Loss: 0.0050, Val Loss: 0.3192, Val Accuracy: 0.8806\nEpoch 20/100 - Loss: 0.0050, Val Loss: 0.3192, Val Accuracy: 0.8852\nEpoch 21/100 - Loss: 0.0050, Val Loss: 0.3105, Val Accuracy: 0.8886\nStopping early at epoch 22\n\n\n\n\n\n\n\n\n\nAverage Validation Loss across folds: 0.3047214560686274\nAverage Validation Accuracy across folds: 0.8894166666666667\n--------------------------------\nFinished FOLD 3\n--------------------------------\n\n\n\n# test model\ndef test(dataloader, model, loss_fn, device):\n    # size of the dataset\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    # model to evaluation mode\n    model.eval()\n    test_loss, correct = 0, 0\n    # no gradients during evaluation\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    # average loss\n    test_loss /= num_batches \n    #  accuracy \n    correct /= size\n\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n# load model\nmodel = NeuralNetwork(input_size=28*28, layer_sizes=[512, 256], activation_funcs=['relu', 'relu'], dropout_rate=0.5)\n# load the best model from disk\nmodel.load_state_dict(torch.load(\"best_model_fold_2.pth\"))\n# test the model with cross-entropy loss\nloss_fn = nn.CrossEntropyLoss()\n# optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\ntest(test_dataloader, model, loss_fn, device)\nprint(\"Done!\")\n\nTest Error: \n Accuracy: 88.3%, Avg loss: 0.331863 \n\nDone!\n\n\n\n\n\n\nKeep detailed records of hyper-parameter choices and associated training & validation errors\nThink critically and visualize the results of the search as needed\nDo each of these in a different sub-section of your notebook\nExplore hyper-parameter choice-0\n\nfor hidden activation=Relu, hidden layers = [32,32], optimizer=adam\nVary the learning rate via a grid search pattern\nPlot training and validation error as a function of the learning rate\nRepeat this exercise for both random and Xavier weight initialization\n\nExplore hyper-parameter choice-1\n\nfor hidden activation=relu, hidden layers = [64,64], optimizer=adam\nVary L1 and L2 in a 10x10 grid search (without dropout)\nPlot validation and training error as a function of L1 and L2 regularization in a 2D heatmap\nPlot the ratio (or difference) of validation to training error as a function of L1 and L2 regularization in a 2D heatmap\n\nExplore hyper-parameter choice-2\n\nfor hidden activation=sigmoid, hidden layers = [96,96,96], optimizer=rmsprop\nVary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization)\nPlot training and validation error as a function of dropout rate\n\nPlot the ratio (or difference) of validation to training error as a function of dropout rate\n\nExplore hyper-parameter choice-3:\n\nfor hidden activation=relu, hidden layers = [96,96,96], optimizer=adam\nVary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization)\nPlot training and validation as a function of dropout rate\n\nPlot the ratio (or difference) of validation to training error as a function of dropout rate\n\nOptional Systematically search for the best regularization parameters choice (3D search) using random search algorithm\n\n(https://en.wikipedia.org/wiki/Random_search)[https://en.wikipedia.org/wiki/Random_search]\nTry to see how deep you can get the ANN (max hidden layers) without suffering from the vanishing gradient effect\n\nFinal fit\n\nAt the very end, select a best fit model and report, training, validation, and test errors at the very end\nMake sure your “plotting variable=True” when for the final training\n\n\n\n\n\n\nfor hidden activation=Relu, hidden layers = [32,32], optimizer=adam\nVary the learning rate via a grid search pattern\nPlot training and validation error as a function of the learning rate\nRepeat this exercise for both random and Xavier weight initialization\n\nDue to the long runtimes, I will only do grid search of learning rate 0.001, 0.01, 0.1.\n\n# model with random weight\ndef model_fn_random_init():\n    return NeuralNetwork(input_size=28*28, layer_sizes=[32, 32], activation_funcs=['relu', 'relu'], dropout_rate=0.5, weight_init='random')\n# model with xavier weight\ndef model_fn_xavier_init():\n    return NeuralNetwork(input_size=28*28, layer_sizes=[32, 32], activation_funcs=['relu', 'relu'], dropout_rate=0.5, weight_init='xavier')\n\n\ndef mod_by_lr(learning_rates, k_folds, dataset, model_fn, loss_fn, optimizer_name, l1_reg, l2_reg, num_epochs=100, device='cpu', patience=5):\n    lr_performance = {'lr': [], 'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n    \n    for lr in learning_rates:\n        print(f\"Training with learning rate: {lr}\")\n        val_loss, val_accuracy, train_loss = training_model_kfold(k_folds=k_folds, dataset=dataset, model_fn=model_fn, loss_fn=loss_fn, optimizer_name=optimizer_name, lr=lr, l1_reg=l1_reg, l2_reg=l2_reg, num_epochs=num_epochs, device=device, plot=False, patience=patience)\n        \n        lr_performance['lr'].append(lr)\n        lr_performance['train_loss'].append(train_loss)\n        lr_performance['val_loss'].append(val_loss)\n        lr_performance['val_accuracy'].append(val_accuracy)\n    \n    # Plotting performance as a function of learning rate\n    plt.figure(figsize=(18, 6))\n    \n    plt.subplot(1, 3, 1)\n    plt.plot(lr_performance['lr'], lr_performance['train_loss'], marker='o', linestyle='-', label='Training Loss')\n    plt.title('Training Loss vs. Learning Rate')\n    plt.xlabel('Learning Rate')\n    plt.ylabel('Training Loss')\n    plt.xscale('log')\n    plt.legend()\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(lr_performance['lr'], lr_performance['val_loss'], marker='o', linestyle='-', label='Validation Loss')\n    plt.title('Validation Loss vs. Learning Rate')\n    plt.xlabel('Learning Rate')\n    plt.ylabel('Validation Loss')\n    plt.xscale('log')\n    plt.legend()\n    \n    plt.subplot(1, 3, 3)\n    plt.plot(lr_performance['lr'], lr_performance['val_accuracy'], marker='o', linestyle='-', label='Validation Accuracy')\n    plt.title('Validation Accuracy vs. Learning Rate')\n    plt.xlabel('Learning Rate')\n    plt.ylabel('Validation Accuracy')\n    plt.xscale('log')\n    plt.legend()\n    \n    plt.show()\n\n\nlearning_rates = [0.1, 0.01, 0.001]\n\nprint(\"Experiment with Random Initialization\")\nmod_by_lr(learning_rates, k_folds=4, dataset=training_data, model_fn=model_fn_random_init, loss_fn=nn.CrossEntropyLoss(), optimizer_name=\"Adam\", l1_reg=0, l2_reg=0, num_epochs=100, device='cpu', patience=5)\n\nprint(\"Experiment with Xavier Initialization\")\nmod_by_lr(learning_rates, k_folds=4, dataset=training_data, model_fn=model_fn_xavier_init, loss_fn=nn.CrossEntropyLoss(), optimizer_name=\"Adam\", l1_reg=0, l2_reg=0, num_epochs=100, device='cpu', patience=5)\n\nExperiment with Random Initialization\nTraining with learning rate: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0375, Val Loss: 2.3139, Val Accuracy: 0.0960\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3122, Val Accuracy: 0.0971\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3053, Val Accuracy: 0.0988\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3100, Val Accuracy: 0.1036\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3114, Val Accuracy: 0.0991\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3119, Val Accuracy: 0.0995\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3113, Val Accuracy: 0.1015\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0368, Val Loss: 2.3172, Val Accuracy: 0.1008\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3112, Val Accuracy: 0.1008\nEpoch 3/100 - Loss: 0.0363, Val Loss: 2.3100, Val Accuracy: 0.0991\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3174, Val Accuracy: 0.0991\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3115, Val Accuracy: 0.0973\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3116, Val Accuracy: 0.0987\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3074, Val Accuracy: 0.0985\nEpoch 8/100 - Loss: 0.0362, Val Loss: 2.3078, Val Accuracy: 0.0973\nEpoch 9/100 - Loss: 0.0362, Val Loss: 2.3096, Val Accuracy: 0.1015\nEpoch 10/100 - Loss: 0.0362, Val Loss: 2.3066, Val Accuracy: 0.0985\nEpoch 11/100 - Loss: 0.0362, Val Loss: 2.3146, Val Accuracy: 0.1015\nEpoch 12/100 - Loss: 0.0362, Val Loss: 2.3119, Val Accuracy: 0.0985\nEpoch 13/100 - Loss: 0.0362, Val Loss: 2.3114, Val Accuracy: 0.1015\nEpoch 14/100 - Loss: 0.0362, Val Loss: 2.3199, Val Accuracy: 0.0991\nStopping early at epoch 15\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0366, Val Loss: 2.3086, Val Accuracy: 0.0961\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3131, Val Accuracy: 0.0969\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3067, Val Accuracy: 0.1041\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3071, Val Accuracy: 0.0993\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3115, Val Accuracy: 0.1059\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3172, Val Accuracy: 0.0977\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3075, Val Accuracy: 0.1059\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0365, Val Loss: 2.3234, Val Accuracy: 0.1025\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3112, Val Accuracy: 0.1009\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3064, Val Accuracy: 0.1003\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3186, Val Accuracy: 0.0990\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3068, Val Accuracy: 0.0997\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3054, Val Accuracy: 0.0985\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3049, Val Accuracy: 0.0993\nEpoch 8/100 - Loss: 0.0362, Val Loss: 2.3061, Val Accuracy: 0.1002\nEpoch 9/100 - Loss: 0.0362, Val Loss: 2.3087, Val Accuracy: 0.0985\nEpoch 10/100 - Loss: 0.0362, Val Loss: 2.3087, Val Accuracy: 0.0993\nEpoch 11/100 - Loss: 0.0362, Val Loss: 2.3072, Val Accuracy: 0.1003\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.305333677251288, Validation Accuracy = 0.0988\nFold 1: Validation Loss = 2.3066133042599293, Validation Accuracy = 0.09853333333333333\nFold 2: Validation Loss = 2.3067821360648946, Validation Accuracy = 0.10413333333333333\nFold 3: Validation Loss = 2.3049005062022108, Validation Accuracy = 0.09926666666666667\nAverage Validation Loss across folds: 2.3059074059445805\nAverage Validation Accuracy across folds: 0.10018333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with learning rate: 0.01\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0197, Val Loss: 0.7694, Val Accuracy: 0.7041\nEpoch 2/100 - Loss: 0.0180, Val Loss: 0.7087, Val Accuracy: 0.7120\nEpoch 3/100 - Loss: 0.0177, Val Loss: 0.7038, Val Accuracy: 0.7156\nEpoch 4/100 - Loss: 0.0177, Val Loss: 0.6906, Val Accuracy: 0.7364\nEpoch 5/100 - Loss: 0.0179, Val Loss: 0.6841, Val Accuracy: 0.7288\nEpoch 6/100 - Loss: 0.0177, Val Loss: 0.6859, Val Accuracy: 0.7368\nEpoch 7/100 - Loss: 0.0174, Val Loss: 0.6665, Val Accuracy: 0.7345\nEpoch 8/100 - Loss: 0.0176, Val Loss: 0.6815, Val Accuracy: 0.7205\nEpoch 9/100 - Loss: 0.0178, Val Loss: 0.6759, Val Accuracy: 0.7392\nEpoch 10/100 - Loss: 0.0178, Val Loss: 0.7153, Val Accuracy: 0.7173\nEpoch 11/100 - Loss: 0.0189, Val Loss: 0.7043, Val Accuracy: 0.7271\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0196, Val Loss: 0.7754, Val Accuracy: 0.7030\nEpoch 2/100 - Loss: 0.0181, Val Loss: 0.7947, Val Accuracy: 0.7236\nEpoch 3/100 - Loss: 0.0179, Val Loss: 0.7320, Val Accuracy: 0.7219\nEpoch 4/100 - Loss: 0.0178, Val Loss: 0.7243, Val Accuracy: 0.7330\nEpoch 5/100 - Loss: 0.0185, Val Loss: 0.8466, Val Accuracy: 0.6280\nEpoch 6/100 - Loss: 0.0182, Val Loss: 0.8026, Val Accuracy: 0.6901\nEpoch 7/100 - Loss: 0.0180, Val Loss: 0.7515, Val Accuracy: 0.7240\nEpoch 8/100 - Loss: 0.0181, Val Loss: 0.7614, Val Accuracy: 0.7084\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0184, Val Loss: 0.7341, Val Accuracy: 0.7186\nEpoch 2/100 - Loss: 0.0166, Val Loss: 0.7014, Val Accuracy: 0.7171\nEpoch 3/100 - Loss: 0.0166, Val Loss: 0.6950, Val Accuracy: 0.7187\nEpoch 4/100 - Loss: 0.0164, Val Loss: 0.7151, Val Accuracy: 0.7229\nEpoch 5/100 - Loss: 0.0163, Val Loss: 0.6356, Val Accuracy: 0.7763\nEpoch 6/100 - Loss: 0.0162, Val Loss: 0.6856, Val Accuracy: 0.7225\nEpoch 7/100 - Loss: 0.0163, Val Loss: 0.6639, Val Accuracy: 0.7247\nEpoch 8/100 - Loss: 0.0162, Val Loss: 0.6852, Val Accuracy: 0.7105\nEpoch 9/100 - Loss: 0.0162, Val Loss: 0.6635, Val Accuracy: 0.7351\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0200, Val Loss: 0.7814, Val Accuracy: 0.7161\nEpoch 2/100 - Loss: 0.0183, Val Loss: 0.8294, Val Accuracy: 0.7033\nEpoch 3/100 - Loss: 0.0184, Val Loss: 0.8536, Val Accuracy: 0.6405\nEpoch 4/100 - Loss: 0.0189, Val Loss: 0.8221, Val Accuracy: 0.7103\nEpoch 5/100 - Loss: 0.0185, Val Loss: 0.7859, Val Accuracy: 0.7043\nEpoch 6/100 - Loss: 0.0184, Val Loss: 0.7678, Val Accuracy: 0.7309\nEpoch 7/100 - Loss: 0.0184, Val Loss: 0.8313, Val Accuracy: 0.6947\nEpoch 8/100 - Loss: 0.0183, Val Loss: 0.8306, Val Accuracy: 0.6425\nEpoch 9/100 - Loss: 0.0182, Val Loss: 0.8218, Val Accuracy: 0.6655\nEpoch 10/100 - Loss: 0.0181, Val Loss: 0.8310, Val Accuracy: 0.6519\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.6665161415617517, Validation Accuracy = 0.7345333333333334\nFold 1: Validation Loss = 0.7249018516946346, Validation Accuracy = 0.733\nFold 2: Validation Loss = 0.635443138442141, Validation Accuracy = 0.7762666666666667\nFold 3: Validation Loss = 0.7673437339194278, Validation Accuracy = 0.7309333333333333\nAverage Validation Loss across folds: 0.6985512164044887\nAverage Validation Accuracy across folds: 0.7436833333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with learning rate: 0.001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0213, Val Loss: 0.7794, Val Accuracy: 0.7019\nEpoch 2/100 - Loss: 0.0160, Val Loss: 0.6762, Val Accuracy: 0.7703\nEpoch 3/100 - Loss: 0.0146, Val Loss: 0.6215, Val Accuracy: 0.7928\nEpoch 4/100 - Loss: 0.0141, Val Loss: 0.5870, Val Accuracy: 0.8017\nEpoch 5/100 - Loss: 0.0136, Val Loss: 0.5717, Val Accuracy: 0.8181\nEpoch 6/100 - Loss: 0.0135, Val Loss: 0.5628, Val Accuracy: 0.8097\nEpoch 7/100 - Loss: 0.0132, Val Loss: 0.5438, Val Accuracy: 0.8243\nEpoch 8/100 - Loss: 0.0132, Val Loss: 0.5335, Val Accuracy: 0.8258\nEpoch 9/100 - Loss: 0.0130, Val Loss: 0.5272, Val Accuracy: 0.8285\nEpoch 10/100 - Loss: 0.0129, Val Loss: 0.5298, Val Accuracy: 0.8275\nEpoch 11/100 - Loss: 0.0128, Val Loss: 0.5225, Val Accuracy: 0.8272\nEpoch 12/100 - Loss: 0.0127, Val Loss: 0.5090, Val Accuracy: 0.8299\nEpoch 13/100 - Loss: 0.0127, Val Loss: 0.5103, Val Accuracy: 0.8273\nEpoch 14/100 - Loss: 0.0127, Val Loss: 0.5275, Val Accuracy: 0.8330\nEpoch 15/100 - Loss: 0.0125, Val Loss: 0.5145, Val Accuracy: 0.8334\nEpoch 16/100 - Loss: 0.0123, Val Loss: 0.5000, Val Accuracy: 0.8321\nEpoch 17/100 - Loss: 0.0124, Val Loss: 0.5097, Val Accuracy: 0.8330\nEpoch 18/100 - Loss: 0.0123, Val Loss: 0.5105, Val Accuracy: 0.8307\nEpoch 19/100 - Loss: 0.0123, Val Loss: 0.5074, Val Accuracy: 0.8313\nEpoch 20/100 - Loss: 0.0124, Val Loss: 0.4910, Val Accuracy: 0.8382\nEpoch 21/100 - Loss: 0.0122, Val Loss: 0.4925, Val Accuracy: 0.8323\nEpoch 22/100 - Loss: 0.0121, Val Loss: 0.4898, Val Accuracy: 0.8343\nEpoch 23/100 - Loss: 0.0121, Val Loss: 0.4786, Val Accuracy: 0.8345\nEpoch 24/100 - Loss: 0.0121, Val Loss: 0.4852, Val Accuracy: 0.8324\nEpoch 25/100 - Loss: 0.0121, Val Loss: 0.4854, Val Accuracy: 0.8381\nEpoch 26/100 - Loss: 0.0120, Val Loss: 0.4824, Val Accuracy: 0.8376\nEpoch 27/100 - Loss: 0.0120, Val Loss: 0.4797, Val Accuracy: 0.8373\nStopping early at epoch 28\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0212, Val Loss: 0.7428, Val Accuracy: 0.7428\nEpoch 2/100 - Loss: 0.0155, Val Loss: 0.6410, Val Accuracy: 0.7874\nEpoch 3/100 - Loss: 0.0144, Val Loss: 0.5818, Val Accuracy: 0.7929\nEpoch 4/100 - Loss: 0.0138, Val Loss: 0.5640, Val Accuracy: 0.8121\nEpoch 5/100 - Loss: 0.0135, Val Loss: 0.5499, Val Accuracy: 0.8007\nEpoch 6/100 - Loss: 0.0133, Val Loss: 0.5404, Val Accuracy: 0.8206\nEpoch 7/100 - Loss: 0.0130, Val Loss: 0.5248, Val Accuracy: 0.8154\nEpoch 8/100 - Loss: 0.0130, Val Loss: 0.5168, Val Accuracy: 0.8173\nEpoch 9/100 - Loss: 0.0129, Val Loss: 0.5306, Val Accuracy: 0.8192\nEpoch 10/100 - Loss: 0.0127, Val Loss: 0.5148, Val Accuracy: 0.8206\nEpoch 11/100 - Loss: 0.0127, Val Loss: 0.5056, Val Accuracy: 0.8115\nEpoch 12/100 - Loss: 0.0125, Val Loss: 0.5066, Val Accuracy: 0.8325\nEpoch 13/100 - Loss: 0.0124, Val Loss: 0.4951, Val Accuracy: 0.8326\nEpoch 14/100 - Loss: 0.0125, Val Loss: 0.4886, Val Accuracy: 0.8371\nEpoch 15/100 - Loss: 0.0124, Val Loss: 0.4863, Val Accuracy: 0.8361\nEpoch 16/100 - Loss: 0.0122, Val Loss: 0.5080, Val Accuracy: 0.8272\nEpoch 17/100 - Loss: 0.0122, Val Loss: 0.4951, Val Accuracy: 0.8366\nEpoch 18/100 - Loss: 0.0122, Val Loss: 0.4971, Val Accuracy: 0.8367\nEpoch 19/100 - Loss: 0.0123, Val Loss: 0.4961, Val Accuracy: 0.8303\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0199, Val Loss: 0.7251, Val Accuracy: 0.7438\nEpoch 2/100 - Loss: 0.0146, Val Loss: 0.6198, Val Accuracy: 0.7737\nEpoch 3/100 - Loss: 0.0133, Val Loss: 0.5596, Val Accuracy: 0.7913\nEpoch 4/100 - Loss: 0.0128, Val Loss: 0.5538, Val Accuracy: 0.8020\nEpoch 5/100 - Loss: 0.0124, Val Loss: 0.5378, Val Accuracy: 0.8129\nEpoch 6/100 - Loss: 0.0121, Val Loss: 0.5186, Val Accuracy: 0.8220\nEpoch 7/100 - Loss: 0.0119, Val Loss: 0.5089, Val Accuracy: 0.8207\nEpoch 8/100 - Loss: 0.0118, Val Loss: 0.5079, Val Accuracy: 0.8263\nEpoch 9/100 - Loss: 0.0116, Val Loss: 0.5013, Val Accuracy: 0.8253\nEpoch 10/100 - Loss: 0.0115, Val Loss: 0.4925, Val Accuracy: 0.8273\nEpoch 11/100 - Loss: 0.0115, Val Loss: 0.4844, Val Accuracy: 0.8345\nEpoch 12/100 - Loss: 0.0113, Val Loss: 0.4886, Val Accuracy: 0.8326\nEpoch 13/100 - Loss: 0.0112, Val Loss: 0.4974, Val Accuracy: 0.8256\nEpoch 14/100 - Loss: 0.0112, Val Loss: 0.4757, Val Accuracy: 0.8355\nEpoch 15/100 - Loss: 0.0112, Val Loss: 0.4862, Val Accuracy: 0.8361\nEpoch 16/100 - Loss: 0.0111, Val Loss: 0.4737, Val Accuracy: 0.8347\nEpoch 17/100 - Loss: 0.0111, Val Loss: 0.4921, Val Accuracy: 0.8271\nEpoch 18/100 - Loss: 0.0111, Val Loss: 0.4642, Val Accuracy: 0.8410\nEpoch 19/100 - Loss: 0.0109, Val Loss: 0.4817, Val Accuracy: 0.8333\nEpoch 20/100 - Loss: 0.0110, Val Loss: 0.4765, Val Accuracy: 0.8347\nEpoch 21/100 - Loss: 0.0108, Val Loss: 0.4763, Val Accuracy: 0.8377\nEpoch 22/100 - Loss: 0.0108, Val Loss: 0.4655, Val Accuracy: 0.8401\nStopping early at epoch 23\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0191, Val Loss: 0.7063, Val Accuracy: 0.7469\nEpoch 2/100 - Loss: 0.0141, Val Loss: 0.6135, Val Accuracy: 0.7872\nEpoch 3/100 - Loss: 0.0129, Val Loss: 0.5457, Val Accuracy: 0.8214\nEpoch 4/100 - Loss: 0.0121, Val Loss: 0.5253, Val Accuracy: 0.8118\nEpoch 5/100 - Loss: 0.0119, Val Loss: 0.4931, Val Accuracy: 0.8302\nEpoch 6/100 - Loss: 0.0115, Val Loss: 0.4812, Val Accuracy: 0.8321\nEpoch 7/100 - Loss: 0.0114, Val Loss: 0.4634, Val Accuracy: 0.8399\nEpoch 8/100 - Loss: 0.0112, Val Loss: 0.4687, Val Accuracy: 0.8327\nEpoch 9/100 - Loss: 0.0110, Val Loss: 0.4562, Val Accuracy: 0.8350\nEpoch 10/100 - Loss: 0.0109, Val Loss: 0.4555, Val Accuracy: 0.8447\nEpoch 11/100 - Loss: 0.0109, Val Loss: 0.4520, Val Accuracy: 0.8415\nEpoch 12/100 - Loss: 0.0107, Val Loss: 0.4518, Val Accuracy: 0.8437\nEpoch 13/100 - Loss: 0.0108, Val Loss: 0.4586, Val Accuracy: 0.8479\nEpoch 14/100 - Loss: 0.0107, Val Loss: 0.4420, Val Accuracy: 0.8450\nEpoch 15/100 - Loss: 0.0106, Val Loss: 0.4505, Val Accuracy: 0.8434\nEpoch 16/100 - Loss: 0.0105, Val Loss: 0.4423, Val Accuracy: 0.8504\nEpoch 17/100 - Loss: 0.0105, Val Loss: 0.4412, Val Accuracy: 0.8477\nEpoch 18/100 - Loss: 0.0104, Val Loss: 0.4388, Val Accuracy: 0.8482\nEpoch 19/100 - Loss: 0.0104, Val Loss: 0.4407, Val Accuracy: 0.8523\nEpoch 20/100 - Loss: 0.0103, Val Loss: 0.4444, Val Accuracy: 0.8513\nEpoch 21/100 - Loss: 0.0104, Val Loss: 0.4329, Val Accuracy: 0.8475\nEpoch 22/100 - Loss: 0.0103, Val Loss: 0.4463, Val Accuracy: 0.8519\nEpoch 23/100 - Loss: 0.0103, Val Loss: 0.4367, Val Accuracy: 0.8483\nEpoch 24/100 - Loss: 0.0102, Val Loss: 0.4417, Val Accuracy: 0.8549\nEpoch 25/100 - Loss: 0.0102, Val Loss: 0.4456, Val Accuracy: 0.8462\nStopping early at epoch 26\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.478112856504765, Validation Accuracy = 0.8344666666666667\nFold 1: Validation Loss = 0.4862562452224975, Validation Accuracy = 0.8361333333333333\nFold 2: Validation Loss = 0.46516420929989916, Validation Accuracy = 0.841\nFold 3: Validation Loss = 0.4339830860178521, Validation Accuracy = 0.8474666666666667\nAverage Validation Loss across folds: 0.4658790992612534\nAverage Validation Accuracy across folds: 0.8397666666666666\n--------------------------------\nK-fold Cross-validation completed.\n\n\n\n\n\n\n\n\n\nExperiment with Xavier Initialization\nTraining with learning rate: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0370, Val Loss: 2.3088, Val Accuracy: 0.0997\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3065, Val Accuracy: 0.1003\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3103, Val Accuracy: 0.0984\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3067, Val Accuracy: 0.1007\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3086, Val Accuracy: 0.1037\nEpoch 6/100 - Loss: 0.0361, Val Loss: 2.3135, Val Accuracy: 0.0984\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0371, Val Loss: 2.3130, Val Accuracy: 0.1010\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3127, Val Accuracy: 0.0974\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3070, Val Accuracy: 0.1010\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3063, Val Accuracy: 0.0974\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3164, Val Accuracy: 0.0949\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3096, Val Accuracy: 0.1015\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3084, Val Accuracy: 0.0986\nEpoch 8/100 - Loss: 0.0362, Val Loss: 2.3078, Val Accuracy: 0.0949\nEpoch 9/100 - Loss: 0.0362, Val Loss: 2.3061, Val Accuracy: 0.1035\nEpoch 10/100 - Loss: 0.0362, Val Loss: 2.3066, Val Accuracy: 0.1037\nEpoch 11/100 - Loss: 0.0362, Val Loss: 2.3202, Val Accuracy: 0.0986\nEpoch 12/100 - Loss: 0.0362, Val Loss: 2.3167, Val Accuracy: 0.0949\nEpoch 13/100 - Loss: 0.0362, Val Loss: 2.3158, Val Accuracy: 0.1035\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0368, Val Loss: 2.3155, Val Accuracy: 0.0977\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3132, Val Accuracy: 0.0997\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3088, Val Accuracy: 0.0975\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3116, Val Accuracy: 0.0997\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3138, Val Accuracy: 0.0997\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3159, Val Accuracy: 0.0977\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3096, Val Accuracy: 0.1001\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0376, Val Loss: 2.3100, Val Accuracy: 0.1005\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3127, Val Accuracy: 0.1008\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3076, Val Accuracy: 0.1034\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3041, Val Accuracy: 0.0969\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3182, Val Accuracy: 0.1008\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3133, Val Accuracy: 0.1008\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3103, Val Accuracy: 0.0969\nEpoch 8/100 - Loss: 0.0362, Val Loss: 2.3205, Val Accuracy: 0.0997\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.30643939566105, Validation Accuracy = 0.10033333333333333\nFold 1: Validation Loss = 2.306070342976996, Validation Accuracy = 0.10353333333333334\nFold 2: Validation Loss = 2.3087029294764743, Validation Accuracy = 0.09746666666666666\nFold 3: Validation Loss = 2.3041324899551716, Validation Accuracy = 0.09686666666666667\nAverage Validation Loss across folds: 2.3063362895174233\nAverage Validation Accuracy across folds: 0.09955\n--------------------------------\nK-fold Cross-validation completed.\nTraining with learning rate: 0.01\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0272, Val Loss: 1.2507, Val Accuracy: 0.5132\nEpoch 2/100 - Loss: 0.0250, Val Loss: 1.2151, Val Accuracy: 0.5541\nEpoch 3/100 - Loss: 0.0245, Val Loss: 1.2594, Val Accuracy: 0.5125\nEpoch 4/100 - Loss: 0.0245, Val Loss: 1.2815, Val Accuracy: 0.5239\nEpoch 5/100 - Loss: 0.0244, Val Loss: 1.1889, Val Accuracy: 0.5364\nEpoch 6/100 - Loss: 0.0245, Val Loss: 1.2305, Val Accuracy: 0.4761\nEpoch 7/100 - Loss: 0.0245, Val Loss: 1.2703, Val Accuracy: 0.4694\nEpoch 8/100 - Loss: 0.0244, Val Loss: 1.2046, Val Accuracy: 0.5203\nEpoch 9/100 - Loss: 0.0246, Val Loss: 1.2656, Val Accuracy: 0.4925\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0218, Val Loss: 0.9223, Val Accuracy: 0.6578\nEpoch 2/100 - Loss: 0.0196, Val Loss: 0.8823, Val Accuracy: 0.6870\nEpoch 3/100 - Loss: 0.0197, Val Loss: 0.8351, Val Accuracy: 0.7055\nEpoch 4/100 - Loss: 0.0194, Val Loss: 0.8782, Val Accuracy: 0.6890\nEpoch 5/100 - Loss: 0.0190, Val Loss: 0.7941, Val Accuracy: 0.6991\nEpoch 6/100 - Loss: 0.0192, Val Loss: 0.9430, Val Accuracy: 0.6506\nEpoch 7/100 - Loss: 0.0195, Val Loss: 0.8411, Val Accuracy: 0.7061\nEpoch 8/100 - Loss: 0.0198, Val Loss: 0.9245, Val Accuracy: 0.6631\nEpoch 9/100 - Loss: 0.0197, Val Loss: 0.9370, Val Accuracy: 0.6711\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0267, Val Loss: 1.1478, Val Accuracy: 0.5735\nEpoch 2/100 - Loss: 0.0241, Val Loss: 1.2049, Val Accuracy: 0.5738\nEpoch 3/100 - Loss: 0.0241, Val Loss: 1.1905, Val Accuracy: 0.5226\nEpoch 4/100 - Loss: 0.0239, Val Loss: 1.1609, Val Accuracy: 0.5009\nEpoch 5/100 - Loss: 0.0238, Val Loss: 1.1418, Val Accuracy: 0.5225\nEpoch 6/100 - Loss: 0.0238, Val Loss: 1.2251, Val Accuracy: 0.4369\nEpoch 7/100 - Loss: 0.0237, Val Loss: 1.1442, Val Accuracy: 0.5824\nEpoch 8/100 - Loss: 0.0239, Val Loss: 1.2387, Val Accuracy: 0.4830\nEpoch 9/100 - Loss: 0.0240, Val Loss: 1.1191, Val Accuracy: 0.5680\nEpoch 10/100 - Loss: 0.0242, Val Loss: 1.2084, Val Accuracy: 0.5136\nEpoch 11/100 - Loss: 0.0245, Val Loss: 1.1913, Val Accuracy: 0.4669\nEpoch 12/100 - Loss: 0.0255, Val Loss: 1.2048, Val Accuracy: 0.4927\nEpoch 13/100 - Loss: 0.0249, Val Loss: 1.1901, Val Accuracy: 0.4983\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0248, Val Loss: 1.1363, Val Accuracy: 0.5461\nEpoch 2/100 - Loss: 0.0234, Val Loss: 1.0632, Val Accuracy: 0.6339\nEpoch 3/100 - Loss: 0.0232, Val Loss: 1.1166, Val Accuracy: 0.5908\nEpoch 4/100 - Loss: 0.0231, Val Loss: 1.0342, Val Accuracy: 0.5953\nEpoch 5/100 - Loss: 0.0235, Val Loss: 1.0623, Val Accuracy: 0.5959\nEpoch 6/100 - Loss: 0.0233, Val Loss: 1.1010, Val Accuracy: 0.5499\nEpoch 7/100 - Loss: 0.0234, Val Loss: 1.1036, Val Accuracy: 0.5804\nEpoch 8/100 - Loss: 0.0234, Val Loss: 1.1087, Val Accuracy: 0.5259\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.1890143785070866, Validation Accuracy = 0.5364\nFold 1: Validation Loss = 0.7943653591135715, Validation Accuracy = 0.6990666666666666\nFold 2: Validation Loss = 1.1188485064405076, Validation Accuracy = 0.568\nFold 3: Validation Loss = 1.0350262459288253, Validation Accuracy = 0.5952666666666667\nAverage Validation Loss across folds: 1.0343136224974978\nAverage Validation Accuracy across folds: 0.5996833333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with learning rate: 0.001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0219, Val Loss: 0.7690, Val Accuracy: 0.7498\nEpoch 2/100 - Loss: 0.0162, Val Loss: 0.6561, Val Accuracy: 0.7862\nEpoch 3/100 - Loss: 0.0145, Val Loss: 0.5931, Val Accuracy: 0.7746\nEpoch 4/100 - Loss: 0.0137, Val Loss: 0.5575, Val Accuracy: 0.8181\nEpoch 5/100 - Loss: 0.0133, Val Loss: 0.5379, Val Accuracy: 0.8130\nEpoch 6/100 - Loss: 0.0130, Val Loss: 0.5280, Val Accuracy: 0.8316\nEpoch 7/100 - Loss: 0.0129, Val Loss: 0.5334, Val Accuracy: 0.8199\nEpoch 8/100 - Loss: 0.0126, Val Loss: 0.5245, Val Accuracy: 0.8323\nEpoch 9/100 - Loss: 0.0125, Val Loss: 0.5097, Val Accuracy: 0.8277\nEpoch 10/100 - Loss: 0.0124, Val Loss: 0.5209, Val Accuracy: 0.8250\nEpoch 11/100 - Loss: 0.0122, Val Loss: 0.5212, Val Accuracy: 0.8207\nEpoch 12/100 - Loss: 0.0122, Val Loss: 0.5024, Val Accuracy: 0.8366\nEpoch 13/100 - Loss: 0.0120, Val Loss: 0.5132, Val Accuracy: 0.8293\nEpoch 14/100 - Loss: 0.0119, Val Loss: 0.4909, Val Accuracy: 0.8427\nEpoch 15/100 - Loss: 0.0120, Val Loss: 0.4795, Val Accuracy: 0.8416\nEpoch 16/100 - Loss: 0.0118, Val Loss: 0.4961, Val Accuracy: 0.8362\nEpoch 17/100 - Loss: 0.0118, Val Loss: 0.4867, Val Accuracy: 0.8391\nEpoch 18/100 - Loss: 0.0118, Val Loss: 0.4960, Val Accuracy: 0.8406\nEpoch 19/100 - Loss: 0.0118, Val Loss: 0.4903, Val Accuracy: 0.8371\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0212, Val Loss: 0.7023, Val Accuracy: 0.7710\nEpoch 2/100 - Loss: 0.0148, Val Loss: 0.6000, Val Accuracy: 0.7988\nEpoch 3/100 - Loss: 0.0136, Val Loss: 0.5647, Val Accuracy: 0.8114\nEpoch 4/100 - Loss: 0.0128, Val Loss: 0.5440, Val Accuracy: 0.8191\nEpoch 5/100 - Loss: 0.0126, Val Loss: 0.5415, Val Accuracy: 0.8229\nEpoch 6/100 - Loss: 0.0123, Val Loss: 0.5195, Val Accuracy: 0.8282\nEpoch 7/100 - Loss: 0.0122, Val Loss: 0.5167, Val Accuracy: 0.8262\nEpoch 8/100 - Loss: 0.0118, Val Loss: 0.5181, Val Accuracy: 0.8315\nEpoch 9/100 - Loss: 0.0117, Val Loss: 0.5085, Val Accuracy: 0.8348\nEpoch 10/100 - Loss: 0.0117, Val Loss: 0.5120, Val Accuracy: 0.8226\nEpoch 11/100 - Loss: 0.0115, Val Loss: 0.5054, Val Accuracy: 0.8397\nEpoch 12/100 - Loss: 0.0115, Val Loss: 0.5103, Val Accuracy: 0.8407\nEpoch 13/100 - Loss: 0.0113, Val Loss: 0.5074, Val Accuracy: 0.8384\nEpoch 14/100 - Loss: 0.0112, Val Loss: 0.5168, Val Accuracy: 0.8376\nEpoch 15/100 - Loss: 0.0111, Val Loss: 0.4758, Val Accuracy: 0.8450\nEpoch 16/100 - Loss: 0.0110, Val Loss: 0.4839, Val Accuracy: 0.8459\nEpoch 17/100 - Loss: 0.0110, Val Loss: 0.4874, Val Accuracy: 0.8403\nEpoch 18/100 - Loss: 0.0109, Val Loss: 0.4852, Val Accuracy: 0.8481\nEpoch 19/100 - Loss: 0.0109, Val Loss: 0.4789, Val Accuracy: 0.8421\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0205, Val Loss: 0.6842, Val Accuracy: 0.7673\nEpoch 2/100 - Loss: 0.0147, Val Loss: 0.6239, Val Accuracy: 0.7797\nEpoch 3/100 - Loss: 0.0136, Val Loss: 0.5803, Val Accuracy: 0.7841\nEpoch 4/100 - Loss: 0.0130, Val Loss: 0.5655, Val Accuracy: 0.7883\nEpoch 5/100 - Loss: 0.0127, Val Loss: 0.5443, Val Accuracy: 0.8005\nEpoch 6/100 - Loss: 0.0124, Val Loss: 0.5437, Val Accuracy: 0.8035\nEpoch 7/100 - Loss: 0.0122, Val Loss: 0.5526, Val Accuracy: 0.8227\nEpoch 8/100 - Loss: 0.0121, Val Loss: 0.5198, Val Accuracy: 0.8071\nEpoch 9/100 - Loss: 0.0119, Val Loss: 0.5320, Val Accuracy: 0.8066\nEpoch 10/100 - Loss: 0.0118, Val Loss: 0.5368, Val Accuracy: 0.8047\nEpoch 11/100 - Loss: 0.0117, Val Loss: 0.5429, Val Accuracy: 0.7940\nEpoch 12/100 - Loss: 0.0116, Val Loss: 0.5239, Val Accuracy: 0.8072\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0219, Val Loss: 0.7404, Val Accuracy: 0.7503\nEpoch 2/100 - Loss: 0.0160, Val Loss: 0.6427, Val Accuracy: 0.7707\nEpoch 3/100 - Loss: 0.0145, Val Loss: 0.6098, Val Accuracy: 0.7887\nEpoch 4/100 - Loss: 0.0138, Val Loss: 0.5662, Val Accuracy: 0.8077\nEpoch 5/100 - Loss: 0.0134, Val Loss: 0.5703, Val Accuracy: 0.7955\nEpoch 6/100 - Loss: 0.0132, Val Loss: 0.5480, Val Accuracy: 0.8167\nEpoch 7/100 - Loss: 0.0130, Val Loss: 0.5384, Val Accuracy: 0.8149\nEpoch 8/100 - Loss: 0.0129, Val Loss: 0.5336, Val Accuracy: 0.8256\nEpoch 9/100 - Loss: 0.0127, Val Loss: 0.5299, Val Accuracy: 0.8171\nEpoch 10/100 - Loss: 0.0126, Val Loss: 0.5287, Val Accuracy: 0.8226\nEpoch 11/100 - Loss: 0.0125, Val Loss: 0.5314, Val Accuracy: 0.8158\nEpoch 12/100 - Loss: 0.0123, Val Loss: 0.5215, Val Accuracy: 0.8193\nEpoch 13/100 - Loss: 0.0123, Val Loss: 0.5284, Val Accuracy: 0.8198\nEpoch 14/100 - Loss: 0.0122, Val Loss: 0.5113, Val Accuracy: 0.8218\nEpoch 15/100 - Loss: 0.0123, Val Loss: 0.5142, Val Accuracy: 0.8226\nEpoch 16/100 - Loss: 0.0122, Val Loss: 0.5308, Val Accuracy: 0.8192\nEpoch 17/100 - Loss: 0.0122, Val Loss: 0.5192, Val Accuracy: 0.8275\nEpoch 18/100 - Loss: 0.0120, Val Loss: 0.5101, Val Accuracy: 0.8285\nEpoch 19/100 - Loss: 0.0120, Val Loss: 0.5422, Val Accuracy: 0.7837\nEpoch 20/100 - Loss: 0.0120, Val Loss: 0.5013, Val Accuracy: 0.8239\nEpoch 21/100 - Loss: 0.0121, Val Loss: 0.5119, Val Accuracy: 0.8227\nEpoch 22/100 - Loss: 0.0119, Val Loss: 0.5096, Val Accuracy: 0.8211\nEpoch 23/100 - Loss: 0.0119, Val Loss: 0.5044, Val Accuracy: 0.8197\nEpoch 24/100 - Loss: 0.0120, Val Loss: 0.4971, Val Accuracy: 0.8251\nEpoch 25/100 - Loss: 0.0118, Val Loss: 0.4978, Val Accuracy: 0.8190\nEpoch 26/100 - Loss: 0.0119, Val Loss: 0.4999, Val Accuracy: 0.8263\nEpoch 27/100 - Loss: 0.0118, Val Loss: 0.5073, Val Accuracy: 0.8205\nEpoch 28/100 - Loss: 0.0118, Val Loss: 0.5056, Val Accuracy: 0.8229\nStopping early at epoch 29\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.47926961602048673, Validation Accuracy = 0.8416\nFold 1: Validation Loss = 0.4755426121518967, Validation Accuracy = 0.845\nFold 2: Validation Loss = 0.5202207646471388, Validation Accuracy = 0.8070666666666667\nFold 3: Validation Loss = 0.49710745380279864, Validation Accuracy = 0.8250666666666666\nAverage Validation Loss across folds: 0.4930351116555802\nAverage Validation Accuracy across folds: 0.8296833333333333\n--------------------------------\nK-fold Cross-validation completed.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor hidden activation=relu, hidden layers = [64,64], optimizer=adam\nVary L1 and L2 in a 10x10 grid search (without dropout)\nPlot validation and training error as a function of L1 and L2 regularization in a 2D heatmap\nPlot the ratio (or difference) of validation to training error as a function of L1 and L2 regularization in a 2D heatmap\n\n\n\nI will do a 5 by 5 grid search instead of a 10 by 10 grid search as the runtimes are very long and the results are similar. Professor James said that it was okay to do so. The cell not ran shows the code for a 10 by 10 grid search and the second one shows the code for a 5 by 5 grid search.\n\n# 10 by 10 grid search (not run)\n# ranges for L1 and L2\nl1_range = np.logspace(-4, -1, 10)\nl2_range = np.logspace(-4, -1, 10)\n\n# store results\nresults1 = []\n# loop through the grid\nfor l1 in l1_range:\n    for l2 in l2_range:\n        print(f\"Training with L1: {l1}, L2: {l2}\")\n        # train model\n        avg_train_loss, avg_val_loss, avg_val_accuracy = training_model_kfold(\n            model_fn=model_fn,\n            k_folds=4,\n            num_epochs=100,\n            dataset=training_data,  \n            loss_fn=nn.CrossEntropyLoss(),\n            optimizer_name=\"Adam\",\n            lr=0.01,  \n            l1_reg=l1,\n            l2_reg=l2,\n            device=device,\n            patience=4,\n            plot=False)\n        \n        # store results\n        results1.append({\n            'l1': l1,\n            'l2': l2,\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n            'val_accuracy': avg_val_accuracy})\n\n\n# 5 by 5 grid search \n# Define ranges for L1 and L2 \nl1_range = np.logspace(-4, -1, 5) \nl2_range = np.logspace(-4, -1, 5) \n\nresults1 = []\n\nfor l1 in l1_range:\n    for l2 in l2_range:\n        print(f\"Training with L1: {l1}, L2: {l2}\")\n        \n        avg_train_loss, avg_val_loss, avg_val_accuracy = training_model_kfold(\n            model_fn=model_fn,\n            k_folds=2,\n            num_epochs=100,\n            dataset=training_data,\n            loss_fn=nn.CrossEntropyLoss(),\n            optimizer_name=\"Adam\",\n            lr=0.01,  \n            l1_reg=l1,\n            l2_reg=l2,\n            num_epochs=100,\n            device=device,\n            patience=4,\n            plot=False)\n        \n        results1.append({\n            'l1': l1,\n            'l2': l2,\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n            'val_accuracy': avg_val_accuracy})\n\nresults_choice_1 = pd.DataFrame(results1)\nresults_choice_1.to_csv('results1.csv', index=False)\nprint(results_choice_1)\n\nTraining with L1: 0.0001, L2: 0.0001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0380, Val Loss: 0.7429, Val Accuracy: 0.7243\nEpoch 2/100 - Loss: 0.0431, Val Loss: 0.7548, Val Accuracy: 0.6858\nEpoch 3/100 - Loss: 0.0430, Val Loss: 0.7459, Val Accuracy: 0.6887\nEpoch 4/100 - Loss: 0.0460, Val Loss: 0.6938, Val Accuracy: 0.7083\nEpoch 5/100 - Loss: 0.0443, Val Loss: 0.7786, Val Accuracy: 0.6976\nEpoch 6/100 - Loss: 0.0438, Val Loss: 0.9246, Val Accuracy: 0.6295\nEpoch 7/100 - Loss: 0.0459, Val Loss: 0.7116, Val Accuracy: 0.7470\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0377, Val Loss: 0.7237, Val Accuracy: 0.6817\nEpoch 2/100 - Loss: 0.0434, Val Loss: 0.6957, Val Accuracy: 0.7262\nEpoch 3/100 - Loss: 0.0443, Val Loss: 0.7128, Val Accuracy: 0.7239\nEpoch 4/100 - Loss: 0.0447, Val Loss: 0.7870, Val Accuracy: 0.7103\nEpoch 5/100 - Loss: 0.0448, Val Loss: 0.7433, Val Accuracy: 0.7018\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.6938231032667384, Validation Accuracy = 0.7082666666666667\nFold 1: Validation Loss = 0.6957771871516953, Validation Accuracy = 0.7262333333333333\nAverage Validation Loss across folds: 0.6948001452092168\nAverage Validation Accuracy across folds: 0.7172499999999999\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0001, L2: 0.0005623413251903491\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0373, Val Loss: 0.8293, Val Accuracy: 0.6851\nEpoch 2/100 - Loss: 0.0409, Val Loss: 0.8119, Val Accuracy: 0.6841\nEpoch 3/100 - Loss: 0.0417, Val Loss: 0.7265, Val Accuracy: 0.7159\nEpoch 4/100 - Loss: 0.0390, Val Loss: 0.7791, Val Accuracy: 0.6632\nEpoch 5/100 - Loss: 0.0413, Val Loss: 0.7154, Val Accuracy: 0.7070\nEpoch 6/100 - Loss: 0.0405, Val Loss: 0.7622, Val Accuracy: 0.6663\nEpoch 7/100 - Loss: 0.0400, Val Loss: 0.7384, Val Accuracy: 0.7042\nEpoch 8/100 - Loss: 0.0393, Val Loss: 0.7669, Val Accuracy: 0.6933\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0361, Val Loss: 0.8180, Val Accuracy: 0.6528\nEpoch 2/100 - Loss: 0.0394, Val Loss: 0.8629, Val Accuracy: 0.6975\nEpoch 3/100 - Loss: 0.0401, Val Loss: 0.7041, Val Accuracy: 0.7210\nEpoch 4/100 - Loss: 0.0399, Val Loss: 0.8094, Val Accuracy: 0.6811\nEpoch 5/100 - Loss: 0.0393, Val Loss: 0.8211, Val Accuracy: 0.6783\nEpoch 6/100 - Loss: 0.0402, Val Loss: 0.7743, Val Accuracy: 0.6977\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.7153623624524074, Validation Accuracy = 0.7070333333333333\nFold 1: Validation Loss = 0.7041597517568674, Validation Accuracy = 0.7210333333333333\nAverage Validation Loss across folds: 0.7097610571046373\nAverage Validation Accuracy across folds: 0.7140333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0001, L2: 0.0031622776601683794\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0319, Val Loss: 0.8192, Val Accuracy: 0.6723\nEpoch 2/100 - Loss: 0.0319, Val Loss: 0.7745, Val Accuracy: 0.7118\nEpoch 3/100 - Loss: 0.0313, Val Loss: 0.8971, Val Accuracy: 0.6288\nEpoch 4/100 - Loss: 0.0319, Val Loss: 0.8457, Val Accuracy: 0.6531\nEpoch 5/100 - Loss: 0.0304, Val Loss: 0.9356, Val Accuracy: 0.6112\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0318, Val Loss: 0.8225, Val Accuracy: 0.6679\nEpoch 2/100 - Loss: 0.0319, Val Loss: 0.8575, Val Accuracy: 0.6860\nEpoch 3/100 - Loss: 0.0317, Val Loss: 0.9248, Val Accuracy: 0.6869\nEpoch 4/100 - Loss: 0.0310, Val Loss: 0.7672, Val Accuracy: 0.7025\nEpoch 5/100 - Loss: 0.0301, Val Loss: 0.8730, Val Accuracy: 0.6903\nEpoch 6/100 - Loss: 0.0306, Val Loss: 0.8144, Val Accuracy: 0.7002\nEpoch 7/100 - Loss: 0.0315, Val Loss: 0.8718, Val Accuracy: 0.6770\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.7742582643464175, Validation Accuracy = 0.7118\nFold 1: Validation Loss = 0.7671754582604365, Validation Accuracy = 0.7025\nAverage Validation Loss across folds: 0.770716861303427\nAverage Validation Accuracy across folds: 0.70715\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0001, L2: 0.01778279410038923\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0255, Val Loss: 1.0673, Val Accuracy: 0.5599\nEpoch 2/100 - Loss: 0.0242, Val Loss: 0.9324, Val Accuracy: 0.6386\nEpoch 3/100 - Loss: 0.0241, Val Loss: 1.0444, Val Accuracy: 0.5678\nEpoch 4/100 - Loss: 0.0246, Val Loss: 1.0938, Val Accuracy: 0.6010\nEpoch 5/100 - Loss: 0.0236, Val Loss: 1.0484, Val Accuracy: 0.6161\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0257, Val Loss: 1.0057, Val Accuracy: 0.5963\nEpoch 2/100 - Loss: 0.0242, Val Loss: 1.0704, Val Accuracy: 0.5991\nEpoch 3/100 - Loss: 0.0241, Val Loss: 0.9964, Val Accuracy: 0.6420\nEpoch 4/100 - Loss: 0.0237, Val Loss: 0.9338, Val Accuracy: 0.6547\nEpoch 5/100 - Loss: 0.0240, Val Loss: 1.0051, Val Accuracy: 0.6275\nEpoch 6/100 - Loss: 0.0239, Val Loss: 0.9682, Val Accuracy: 0.6570\nEpoch 7/100 - Loss: 0.0235, Val Loss: 0.9504, Val Accuracy: 0.6461\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.9324265236793551, Validation Accuracy = 0.6385666666666666\nFold 1: Validation Loss = 0.9338125421294271, Validation Accuracy = 0.6547\nAverage Validation Loss across folds: 0.9331195329043911\nAverage Validation Accuracy across folds: 0.6466333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0001, L2: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0272, Val Loss: 1.5092, Val Accuracy: 0.3771\nEpoch 2/100 - Loss: 0.0260, Val Loss: 1.5169, Val Accuracy: 0.3841\nEpoch 3/100 - Loss: 0.0263, Val Loss: 1.4177, Val Accuracy: 0.4147\nEpoch 4/100 - Loss: 0.0262, Val Loss: 1.6608, Val Accuracy: 0.2949\nEpoch 5/100 - Loss: 0.0264, Val Loss: 1.6067, Val Accuracy: 0.2931\nEpoch 6/100 - Loss: 0.0262, Val Loss: 1.4282, Val Accuracy: 0.4366\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0268, Val Loss: 1.3539, Val Accuracy: 0.4802\nEpoch 2/100 - Loss: 0.0262, Val Loss: 1.4364, Val Accuracy: 0.4230\nEpoch 3/100 - Loss: 0.0261, Val Loss: 1.4150, Val Accuracy: 0.3777\nEpoch 4/100 - Loss: 0.0261, Val Loss: 1.4633, Val Accuracy: 0.3899\nStopping early at epoch 5\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.417715992500533, Validation Accuracy = 0.4147\nFold 1: Validation Loss = 1.354012314444666, Validation Accuracy = 0.4801666666666667\nAverage Validation Loss across folds: 1.3858641534725995\nAverage Validation Accuracy across folds: 0.44743333333333335\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0005623413251903491, L2: 0.0001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0597, Val Loss: 0.7166, Val Accuracy: 0.7466\nEpoch 2/100 - Loss: 0.0573, Val Loss: 0.8189, Val Accuracy: 0.6860\nEpoch 3/100 - Loss: 0.0552, Val Loss: 0.6643, Val Accuracy: 0.7468\nEpoch 4/100 - Loss: 0.0552, Val Loss: 0.9084, Val Accuracy: 0.6860\nEpoch 5/100 - Loss: 0.0542, Val Loss: 0.7774, Val Accuracy: 0.6888\nEpoch 6/100 - Loss: 0.0571, Val Loss: 0.7181, Val Accuracy: 0.7370\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0587, Val Loss: 0.8142, Val Accuracy: 0.6640\nEpoch 2/100 - Loss: 0.0577, Val Loss: 0.7875, Val Accuracy: 0.7033\nEpoch 3/100 - Loss: 0.0524, Val Loss: 0.8557, Val Accuracy: 0.6715\nEpoch 4/100 - Loss: 0.0533, Val Loss: 0.7621, Val Accuracy: 0.6897\nEpoch 5/100 - Loss: 0.0539, Val Loss: 0.7908, Val Accuracy: 0.6875\nEpoch 6/100 - Loss: 0.0550, Val Loss: 0.7250, Val Accuracy: 0.7192\nEpoch 7/100 - Loss: 0.0562, Val Loss: 0.7617, Val Accuracy: 0.6960\nEpoch 8/100 - Loss: 0.0572, Val Loss: 0.9011, Val Accuracy: 0.6383\nEpoch 9/100 - Loss: 0.0581, Val Loss: 0.7589, Val Accuracy: 0.7078\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.6643354269360174, Validation Accuracy = 0.7468\nFold 1: Validation Loss = 0.7251428825768835, Validation Accuracy = 0.7192\nAverage Validation Loss across folds: 0.6947391547564505\nAverage Validation Accuracy across folds: 0.733\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0005623413251903491, L2: 0.0005623413251903491\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0573, Val Loss: 0.8700, Val Accuracy: 0.6609\nEpoch 2/100 - Loss: 0.0578, Val Loss: 0.8070, Val Accuracy: 0.6868\nEpoch 3/100 - Loss: 0.0540, Val Loss: 0.7440, Val Accuracy: 0.7206\nEpoch 4/100 - Loss: 0.0515, Val Loss: 0.7448, Val Accuracy: 0.7279\nEpoch 5/100 - Loss: 0.0483, Val Loss: 0.8255, Val Accuracy: 0.6714\nEpoch 6/100 - Loss: 0.0518, Val Loss: 0.7378, Val Accuracy: 0.7190\nEpoch 7/100 - Loss: 0.0561, Val Loss: 0.7983, Val Accuracy: 0.7122\nEpoch 8/100 - Loss: 0.0553, Val Loss: 0.8474, Val Accuracy: 0.6588\nEpoch 9/100 - Loss: 0.0552, Val Loss: 0.8446, Val Accuracy: 0.6913\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0569, Val Loss: 0.8322, Val Accuracy: 0.7007\nEpoch 2/100 - Loss: 0.0569, Val Loss: 0.8508, Val Accuracy: 0.6539\nEpoch 3/100 - Loss: 0.0540, Val Loss: 0.7898, Val Accuracy: 0.6883\nEpoch 4/100 - Loss: 0.0533, Val Loss: 0.8368, Val Accuracy: 0.6910\nEpoch 5/100 - Loss: 0.0488, Val Loss: 0.8664, Val Accuracy: 0.6568\nEpoch 6/100 - Loss: 0.0487, Val Loss: 0.8323, Val Accuracy: 0.6581\nEpoch 7/100 - Loss: 0.0538, Val Loss: 0.7611, Val Accuracy: 0.7088\nEpoch 8/100 - Loss: 0.0530, Val Loss: 0.7929, Val Accuracy: 0.6826\nEpoch 9/100 - Loss: 0.0543, Val Loss: 0.8721, Val Accuracy: 0.6600\nEpoch 10/100 - Loss: 0.0525, Val Loss: 0.7877, Val Accuracy: 0.6895\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.7377686382356737, Validation Accuracy = 0.7190333333333333\nFold 1: Validation Loss = 0.7610122365102585, Validation Accuracy = 0.7087666666666667\nAverage Validation Loss across folds: 0.7493904373729661\nAverage Validation Accuracy across folds: 0.7139\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0005623413251903491, L2: 0.0031622776601683794\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0510, Val Loss: 0.8487, Val Accuracy: 0.6952\nEpoch 2/100 - Loss: 0.0450, Val Loss: 0.7704, Val Accuracy: 0.7096\nEpoch 3/100 - Loss: 0.0460, Val Loss: 0.8479, Val Accuracy: 0.6694\nEpoch 4/100 - Loss: 0.0459, Val Loss: 0.9400, Val Accuracy: 0.6460\nEpoch 5/100 - Loss: 0.0461, Val Loss: 0.8966, Val Accuracy: 0.6914\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0504, Val Loss: 0.8173, Val Accuracy: 0.6899\nEpoch 2/100 - Loss: 0.0467, Val Loss: 0.9908, Val Accuracy: 0.6157\nEpoch 3/100 - Loss: 0.0453, Val Loss: 0.7845, Val Accuracy: 0.7304\nEpoch 4/100 - Loss: 0.0449, Val Loss: 0.8695, Val Accuracy: 0.6960\nEpoch 5/100 - Loss: 0.0445, Val Loss: 0.9049, Val Accuracy: 0.6566\nEpoch 6/100 - Loss: 0.0500, Val Loss: 0.8826, Val Accuracy: 0.6867\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.7703188340038633, Validation Accuracy = 0.7096\nFold 1: Validation Loss = 0.7843411015167928, Validation Accuracy = 0.7304\nAverage Validation Loss across folds: 0.777329967760328\nAverage Validation Accuracy across folds: 0.72\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0005623413251903491, L2: 0.01778279410038923\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0399, Val Loss: 1.0123, Val Accuracy: 0.6276\nEpoch 2/100 - Loss: 0.0360, Val Loss: 0.9127, Val Accuracy: 0.6788\nEpoch 3/100 - Loss: 0.0361, Val Loss: 1.0770, Val Accuracy: 0.5827\nEpoch 4/100 - Loss: 0.0353, Val Loss: 1.0203, Val Accuracy: 0.6184\nEpoch 5/100 - Loss: 0.0338, Val Loss: 0.9278, Val Accuracy: 0.6785\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0408, Val Loss: 1.0140, Val Accuracy: 0.6322\nEpoch 2/100 - Loss: 0.0354, Val Loss: 0.9142, Val Accuracy: 0.6569\nEpoch 3/100 - Loss: 0.0365, Val Loss: 1.0175, Val Accuracy: 0.6321\nEpoch 4/100 - Loss: 0.0353, Val Loss: 0.9401, Val Accuracy: 0.6525\nEpoch 5/100 - Loss: 0.0354, Val Loss: 0.9571, Val Accuracy: 0.6831\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.9127078578670396, Validation Accuracy = 0.6787666666666666\nFold 1: Validation Loss = 0.9142541471066505, Validation Accuracy = 0.6569\nAverage Validation Loss across folds: 0.913481002486845\nAverage Validation Accuracy across folds: 0.6678333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0005623413251903491, L2: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0354, Val Loss: 1.5898, Val Accuracy: 0.4001\nEpoch 2/100 - Loss: 0.0336, Val Loss: 1.4679, Val Accuracy: 0.3868\nEpoch 3/100 - Loss: 0.0330, Val Loss: 1.4997, Val Accuracy: 0.4280\nEpoch 4/100 - Loss: 0.0335, Val Loss: 1.6485, Val Accuracy: 0.2848\nEpoch 5/100 - Loss: 0.0338, Val Loss: 1.5371, Val Accuracy: 0.4092\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0350, Val Loss: 1.5837, Val Accuracy: 0.3383\nEpoch 2/100 - Loss: 0.0340, Val Loss: 1.4663, Val Accuracy: 0.4791\nEpoch 3/100 - Loss: 0.0333, Val Loss: 1.4617, Val Accuracy: 0.4708\nEpoch 4/100 - Loss: 0.0333, Val Loss: 1.4582, Val Accuracy: 0.4978\nEpoch 5/100 - Loss: 0.0329, Val Loss: 1.4466, Val Accuracy: 0.4689\nEpoch 6/100 - Loss: 0.0330, Val Loss: 1.5447, Val Accuracy: 0.3295\nEpoch 7/100 - Loss: 0.0338, Val Loss: 1.5587, Val Accuracy: 0.3723\nEpoch 8/100 - Loss: 0.0330, Val Loss: 1.6625, Val Accuracy: 0.2749\nEpoch 9/100 - Loss: 0.0334, Val Loss: 1.4336, Val Accuracy: 0.3806\nEpoch 10/100 - Loss: 0.0335, Val Loss: 1.6100, Val Accuracy: 0.3103\nEpoch 11/100 - Loss: 0.0330, Val Loss: 1.4960, Val Accuracy: 0.3970\nEpoch 12/100 - Loss: 0.0328, Val Loss: 1.4443, Val Accuracy: 0.5305\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.4679549767264426, Validation Accuracy = 0.38676666666666665\nFold 1: Validation Loss = 1.4335429940396534, Validation Accuracy = 0.38056666666666666\nAverage Validation Loss across folds: 1.450748985383048\nAverage Validation Accuracy across folds: 0.38366666666666666\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0031622776601683794, L2: 0.0001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0702, Val Loss: 1.2142, Val Accuracy: 0.5484\nEpoch 2/100 - Loss: 0.0628, Val Loss: 1.0471, Val Accuracy: 0.5998\nEpoch 3/100 - Loss: 0.0623, Val Loss: 1.2113, Val Accuracy: 0.5436\nEpoch 4/100 - Loss: 0.0619, Val Loss: 1.1946, Val Accuracy: 0.6082\nEpoch 5/100 - Loss: 0.0617, Val Loss: 1.0882, Val Accuracy: 0.6289\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0703, Val Loss: 1.0566, Val Accuracy: 0.6086\nEpoch 2/100 - Loss: 0.0624, Val Loss: 1.0639, Val Accuracy: 0.6287\nEpoch 3/100 - Loss: 0.0621, Val Loss: 1.1023, Val Accuracy: 0.6211\nEpoch 4/100 - Loss: 0.0624, Val Loss: 1.1183, Val Accuracy: 0.6349\nEpoch 5/100 - Loss: 0.0617, Val Loss: 1.0396, Val Accuracy: 0.5908\nEpoch 6/100 - Loss: 0.0619, Val Loss: 1.1077, Val Accuracy: 0.6046\nEpoch 7/100 - Loss: 0.0619, Val Loss: 1.0303, Val Accuracy: 0.6632\nEpoch 8/100 - Loss: 0.0618, Val Loss: 1.0857, Val Accuracy: 0.6275\nEpoch 9/100 - Loss: 0.0615, Val Loss: 1.0177, Val Accuracy: 0.6452\nEpoch 10/100 - Loss: 0.0623, Val Loss: 1.0869, Val Accuracy: 0.6293\nEpoch 11/100 - Loss: 0.0616, Val Loss: 1.1041, Val Accuracy: 0.6326\nEpoch 12/100 - Loss: 0.0613, Val Loss: 1.2221, Val Accuracy: 0.5665\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.047046583979877, Validation Accuracy = 0.5998333333333333\nFold 1: Validation Loss = 1.0176388470094595, Validation Accuracy = 0.6452333333333333\nAverage Validation Loss across folds: 1.0323427154946683\nAverage Validation Accuracy across folds: 0.6225333333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0031622776601683794, L2: 0.0005623413251903491\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0697, Val Loss: 1.3432, Val Accuracy: 0.4925\nEpoch 2/100 - Loss: 0.0621, Val Loss: 1.3640, Val Accuracy: 0.4989\nEpoch 3/100 - Loss: 0.0619, Val Loss: 1.3231, Val Accuracy: 0.4818\nEpoch 4/100 - Loss: 0.0616, Val Loss: 1.2866, Val Accuracy: 0.5160\nEpoch 5/100 - Loss: 0.0616, Val Loss: 1.2246, Val Accuracy: 0.5135\nEpoch 6/100 - Loss: 0.0616, Val Loss: 1.2663, Val Accuracy: 0.5260\nEpoch 7/100 - Loss: 0.0618, Val Loss: 1.2851, Val Accuracy: 0.4676\nEpoch 8/100 - Loss: 0.0616, Val Loss: 1.2655, Val Accuracy: 0.4668\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0700, Val Loss: 1.1976, Val Accuracy: 0.5388\nEpoch 2/100 - Loss: 0.0621, Val Loss: 1.1500, Val Accuracy: 0.6100\nEpoch 3/100 - Loss: 0.0620, Val Loss: 1.1957, Val Accuracy: 0.5632\nEpoch 4/100 - Loss: 0.0616, Val Loss: 1.1593, Val Accuracy: 0.6115\nEpoch 5/100 - Loss: 0.0619, Val Loss: 1.1667, Val Accuracy: 0.5423\nEpoch 6/100 - Loss: 0.0615, Val Loss: 1.1338, Val Accuracy: 0.5216\nEpoch 7/100 - Loss: 0.0620, Val Loss: 1.1180, Val Accuracy: 0.5622\nEpoch 8/100 - Loss: 0.0620, Val Loss: 1.1217, Val Accuracy: 0.5223\nEpoch 9/100 - Loss: 0.0617, Val Loss: 1.0985, Val Accuracy: 0.5538\nEpoch 10/100 - Loss: 0.0617, Val Loss: 1.1757, Val Accuracy: 0.5917\nEpoch 11/100 - Loss: 0.0620, Val Loss: 1.2339, Val Accuracy: 0.5466\nEpoch 12/100 - Loss: 0.0618, Val Loss: 1.2514, Val Accuracy: 0.5252\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.2245664738897066, Validation Accuracy = 0.5134666666666666\nFold 1: Validation Loss = 1.098504531866452, Validation Accuracy = 0.5538333333333333\nAverage Validation Loss across folds: 1.1615355028780794\nAverage Validation Accuracy across folds: 0.53365\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0031622776601683794, L2: 0.0031622776601683794\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0698, Val Loss: 1.1210, Val Accuracy: 0.5800\nEpoch 2/100 - Loss: 0.0626, Val Loss: 1.1171, Val Accuracy: 0.6018\nEpoch 3/100 - Loss: 0.0624, Val Loss: 1.2142, Val Accuracy: 0.5945\nEpoch 4/100 - Loss: 0.0625, Val Loss: 1.1116, Val Accuracy: 0.5954\nEpoch 5/100 - Loss: 0.0625, Val Loss: 1.0580, Val Accuracy: 0.6436\nEpoch 6/100 - Loss: 0.0623, Val Loss: 1.0520, Val Accuracy: 0.6532\nEpoch 7/100 - Loss: 0.0631, Val Loss: 1.1405, Val Accuracy: 0.6034\nEpoch 8/100 - Loss: 0.0626, Val Loss: 1.0393, Val Accuracy: 0.6611\nEpoch 9/100 - Loss: 0.0627, Val Loss: 1.0927, Val Accuracy: 0.5875\nEpoch 10/100 - Loss: 0.0626, Val Loss: 1.1462, Val Accuracy: 0.5700\nEpoch 11/100 - Loss: 0.0626, Val Loss: 1.1107, Val Accuracy: 0.6031\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0695, Val Loss: 1.3060, Val Accuracy: 0.5113\nEpoch 2/100 - Loss: 0.0621, Val Loss: 1.3661, Val Accuracy: 0.5119\nEpoch 3/100 - Loss: 0.0619, Val Loss: 1.1826, Val Accuracy: 0.6169\nEpoch 4/100 - Loss: 0.0622, Val Loss: 1.2362, Val Accuracy: 0.5092\nEpoch 5/100 - Loss: 0.0618, Val Loss: 1.1930, Val Accuracy: 0.4931\nEpoch 6/100 - Loss: 0.0621, Val Loss: 1.1818, Val Accuracy: 0.5320\nEpoch 7/100 - Loss: 0.0621, Val Loss: 1.2270, Val Accuracy: 0.5668\nEpoch 8/100 - Loss: 0.0619, Val Loss: 1.2743, Val Accuracy: 0.4967\nEpoch 9/100 - Loss: 0.0623, Val Loss: 1.2049, Val Accuracy: 0.5150\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.0391970632681207, Validation Accuracy = 0.6611333333333334\nFold 1: Validation Loss = 1.1817621540413228, Validation Accuracy = 0.5319666666666667\nAverage Validation Loss across folds: 1.1104796086547217\nAverage Validation Accuracy across folds: 0.59655\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0031622776601683794, L2: 0.01778279410038923\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0717, Val Loss: 1.8977, Val Accuracy: 0.2290\nEpoch 2/100 - Loss: 0.0650, Val Loss: 1.8449, Val Accuracy: 0.2247\nEpoch 3/100 - Loss: 0.0647, Val Loss: 1.8484, Val Accuracy: 0.2828\nEpoch 4/100 - Loss: 0.0646, Val Loss: 1.8296, Val Accuracy: 0.2428\nEpoch 5/100 - Loss: 0.0646, Val Loss: 1.8246, Val Accuracy: 0.2562\nEpoch 6/100 - Loss: 0.0644, Val Loss: 1.8353, Val Accuracy: 0.2701\nEpoch 7/100 - Loss: 0.0643, Val Loss: 1.8368, Val Accuracy: 0.2284\nEpoch 8/100 - Loss: 0.0643, Val Loss: 1.8305, Val Accuracy: 0.2974\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0706, Val Loss: 1.5530, Val Accuracy: 0.4141\nEpoch 2/100 - Loss: 0.0636, Val Loss: 1.6296, Val Accuracy: 0.3513\nEpoch 3/100 - Loss: 0.0633, Val Loss: 1.6140, Val Accuracy: 0.4232\nEpoch 4/100 - Loss: 0.0635, Val Loss: 1.5355, Val Accuracy: 0.4204\nEpoch 5/100 - Loss: 0.0632, Val Loss: 1.5616, Val Accuracy: 0.3841\nEpoch 6/100 - Loss: 0.0632, Val Loss: 1.5469, Val Accuracy: 0.4184\nEpoch 7/100 - Loss: 0.0633, Val Loss: 1.6163, Val Accuracy: 0.4525\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.8245379136823643, Validation Accuracy = 0.2562\nFold 1: Validation Loss = 1.5354848561256424, Validation Accuracy = 0.4204\nAverage Validation Loss across folds: 1.6800113849040033\nAverage Validation Accuracy across folds: 0.3383\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0031622776601683794, L2: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0753, Val Loss: 2.3030, Val Accuracy: 0.1013\nEpoch 2/100 - Loss: 0.0701, Val Loss: 2.3027, Val Accuracy: 0.1016\nEpoch 3/100 - Loss: 0.0702, Val Loss: 2.3028, Val Accuracy: 0.0989\nEpoch 4/100 - Loss: 0.0702, Val Loss: 2.3029, Val Accuracy: 0.1016\nEpoch 5/100 - Loss: 0.0702, Val Loss: 2.3028, Val Accuracy: 0.0977\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0752, Val Loss: 2.3030, Val Accuracy: 0.0984\nEpoch 2/100 - Loss: 0.0700, Val Loss: 2.3028, Val Accuracy: 0.0990\nEpoch 3/100 - Loss: 0.0701, Val Loss: 2.3029, Val Accuracy: 0.0992\nEpoch 4/100 - Loss: 0.0701, Val Loss: 2.3034, Val Accuracy: 0.0987\nEpoch 5/100 - Loss: 0.0701, Val Loss: 2.3028, Val Accuracy: 0.0987\nEpoch 6/100 - Loss: 0.0701, Val Loss: 2.3031, Val Accuracy: 0.0992\nEpoch 7/100 - Loss: 0.0701, Val Loss: 2.3028, Val Accuracy: 0.1011\nEpoch 8/100 - Loss: 0.0701, Val Loss: 2.3028, Val Accuracy: 0.0990\nEpoch 9/100 - Loss: 0.0701, Val Loss: 2.3027, Val Accuracy: 0.0992\nEpoch 10/100 - Loss: 0.0701, Val Loss: 2.3028, Val Accuracy: 0.0992\nEpoch 11/100 - Loss: 0.0701, Val Loss: 2.3031, Val Accuracy: 0.0984\nEpoch 12/100 - Loss: 0.0701, Val Loss: 2.3030, Val Accuracy: 0.0990\nEpoch 13/100 - Loss: 0.0701, Val Loss: 2.3026, Val Accuracy: 0.1023\nEpoch 14/100 - Loss: 0.0701, Val Loss: 2.3031, Val Accuracy: 0.0992\nEpoch 15/100 - Loss: 0.0701, Val Loss: 2.3028, Val Accuracy: 0.0984\nEpoch 16/100 - Loss: 0.0701, Val Loss: 2.3027, Val Accuracy: 0.0990\nStopping early at epoch 17\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.302656940305665, Validation Accuracy = 0.10156666666666667\nFold 1: Validation Loss = 2.3025848621498546, Validation Accuracy = 0.10233333333333333\nAverage Validation Loss across folds: 2.30262090122776\nAverage Validation Accuracy across folds: 0.10195\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.01778279410038923, L2: 0.0001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.2639, Val Loss: 2.3026, Val Accuracy: 0.1012\nEpoch 2/100 - Loss: 0.2326, Val Loss: 2.3028, Val Accuracy: 0.0977\nEpoch 3/100 - Loss: 0.2318, Val Loss: 2.3026, Val Accuracy: 0.0977\nEpoch 4/100 - Loss: 0.2313, Val Loss: 2.3025, Val Accuracy: 0.1017\nEpoch 5/100 - Loss: 0.2309, Val Loss: 2.3028, Val Accuracy: 0.0977\nEpoch 6/100 - Loss: 0.2306, Val Loss: 2.3028, Val Accuracy: 0.0977\nEpoch 7/100 - Loss: 0.2303, Val Loss: 2.3026, Val Accuracy: 0.0984\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.2627, Val Loss: 2.3028, Val Accuracy: 0.0977\nEpoch 2/100 - Loss: 0.2313, Val Loss: 2.3029, Val Accuracy: 0.0988\nEpoch 3/100 - Loss: 0.2305, Val Loss: 2.3028, Val Accuracy: 0.0988\nEpoch 4/100 - Loss: 0.2301, Val Loss: 2.3026, Val Accuracy: 0.0988\nEpoch 5/100 - Loss: 0.2296, Val Loss: 2.3028, Val Accuracy: 0.0977\nEpoch 6/100 - Loss: 0.2293, Val Loss: 2.3027, Val Accuracy: 0.0997\nEpoch 7/100 - Loss: 0.2291, Val Loss: 2.3027, Val Accuracy: 0.0983\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.302532252472347, Validation Accuracy = 0.1017\nFold 1: Validation Loss = 2.3025761972358234, Validation Accuracy = 0.09876666666666667\nAverage Validation Loss across folds: 2.3025542248540853\nAverage Validation Accuracy across folds: 0.10023333333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.01778279410038923, L2: 0.0005623413251903491\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.2640, Val Loss: 2.3027, Val Accuracy: 0.0979\nEpoch 2/100 - Loss: 0.2330, Val Loss: 2.3028, Val Accuracy: 0.0997\nEpoch 3/100 - Loss: 0.2320, Val Loss: 2.3026, Val Accuracy: 0.0989\nEpoch 4/100 - Loss: 0.2312, Val Loss: 2.3027, Val Accuracy: 0.0997\nEpoch 5/100 - Loss: 0.2304, Val Loss: 2.3028, Val Accuracy: 0.0979\nEpoch 6/100 - Loss: 0.2297, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 7/100 - Loss: 0.2293, Val Loss: 2.3026, Val Accuracy: 0.0993\nEpoch 8/100 - Loss: 0.2289, Val Loss: 2.3026, Val Accuracy: 0.1004\nEpoch 9/100 - Loss: 0.2285, Val Loss: 2.3028, Val Accuracy: 0.0979\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.2640, Val Loss: 2.3026, Val Accuracy: 0.0993\nEpoch 2/100 - Loss: 0.2331, Val Loss: 2.3027, Val Accuracy: 0.0996\nEpoch 3/100 - Loss: 0.2320, Val Loss: 2.3028, Val Accuracy: 0.0989\nEpoch 4/100 - Loss: 0.2312, Val Loss: 2.3026, Val Accuracy: 0.1017\nStopping early at epoch 5\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3025770975328457, Validation Accuracy = 0.10113333333333334\nFold 1: Validation Loss = 2.302582888206693, Validation Accuracy = 0.09926666666666667\nAverage Validation Loss across folds: 2.302579992869769\nAverage Validation Accuracy across folds: 0.10020000000000001\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.01778279410038923, L2: 0.0031622776601683794\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.2647, Val Loss: 2.3026, Val Accuracy: 0.1008\nEpoch 2/100 - Loss: 0.2309, Val Loss: 2.3026, Val Accuracy: 0.0989\nEpoch 3/100 - Loss: 0.2293, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 4/100 - Loss: 0.2307, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 5/100 - Loss: 0.2326, Val Loss: 2.3027, Val Accuracy: 0.0990\nEpoch 6/100 - Loss: 0.2348, Val Loss: 2.3026, Val Accuracy: 0.0976\nEpoch 7/100 - Loss: 0.2359, Val Loss: 2.3027, Val Accuracy: 0.0976\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.2652, Val Loss: 2.3027, Val Accuracy: 0.1005\nEpoch 2/100 - Loss: 0.2314, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 3/100 - Loss: 0.2299, Val Loss: 2.3028, Val Accuracy: 0.0989\nEpoch 4/100 - Loss: 0.2312, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 5/100 - Loss: 0.2331, Val Loss: 2.3027, Val Accuracy: 0.0982\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.302550287897399, Validation Accuracy = 0.1011\nFold 1: Validation Loss = 2.3026224865333864, Validation Accuracy = 0.10113333333333334\nAverage Validation Loss across folds: 2.302586387215393\nAverage Validation Accuracy across folds: 0.10111666666666666\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.01778279410038923, L2: 0.01778279410038923\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.2681, Val Loss: 2.3026, Val Accuracy: 0.0991\nEpoch 2/100 - Loss: 0.2416, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 3/100 - Loss: 0.2420, Val Loss: 2.3027, Val Accuracy: 0.1014\nEpoch 4/100 - Loss: 0.2423, Val Loss: 2.3028, Val Accuracy: 0.0988\nEpoch 5/100 - Loss: 0.2426, Val Loss: 2.3027, Val Accuracy: 0.0998\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.2681, Val Loss: 2.3026, Val Accuracy: 0.1012\nEpoch 2/100 - Loss: 0.2417, Val Loss: 2.3026, Val Accuracy: 0.1002\nEpoch 3/100 - Loss: 0.2421, Val Loss: 2.3026, Val Accuracy: 0.1009\nEpoch 4/100 - Loss: 0.2424, Val Loss: 2.3026, Val Accuracy: 0.1009\nEpoch 5/100 - Loss: 0.2426, Val Loss: 2.3027, Val Accuracy: 0.0990\nEpoch 6/100 - Loss: 0.2428, Val Loss: 2.3027, Val Accuracy: 0.0990\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3026089129417437, Validation Accuracy = 0.0998\nFold 1: Validation Loss = 2.3026048130572225, Validation Accuracy = 0.1009\nAverage Validation Loss across folds: 2.3026068629994834\nAverage Validation Accuracy across folds: 0.10035\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.01778279410038923, L2: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.2619, Val Loss: 2.3026, Val Accuracy: 0.1005\nEpoch 2/100 - Loss: 0.2333, Val Loss: 2.3028, Val Accuracy: 0.1000\nEpoch 3/100 - Loss: 0.2334, Val Loss: 2.3025, Val Accuracy: 0.1018\nEpoch 4/100 - Loss: 0.2334, Val Loss: 2.3027, Val Accuracy: 0.0971\nEpoch 5/100 - Loss: 0.2333, Val Loss: 2.3026, Val Accuracy: 0.1018\nEpoch 6/100 - Loss: 0.2333, Val Loss: 2.3026, Val Accuracy: 0.1024\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.2616, Val Loss: 2.3027, Val Accuracy: 0.0976\nEpoch 2/100 - Loss: 0.2331, Val Loss: 2.3026, Val Accuracy: 0.0990\nEpoch 3/100 - Loss: 0.2332, Val Loss: 2.3028, Val Accuracy: 0.0995\nEpoch 4/100 - Loss: 0.2332, Val Loss: 2.3027, Val Accuracy: 0.1000\nEpoch 5/100 - Loss: 0.2332, Val Loss: 2.3026, Val Accuracy: 0.1019\nEpoch 6/100 - Loss: 0.2331, Val Loss: 2.3027, Val Accuracy: 0.0995\nEpoch 7/100 - Loss: 0.2331, Val Loss: 2.3027, Val Accuracy: 0.0982\nEpoch 8/100 - Loss: 0.2331, Val Loss: 2.3027, Val Accuracy: 0.0995\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3025161579473696, Validation Accuracy = 0.1018\nFold 1: Validation Loss = 2.302603925469079, Validation Accuracy = 0.1019\nAverage Validation Loss across folds: 2.302560041708224\nAverage Validation Accuracy across folds: 0.10185\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.1, L2: 0.0001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 1.3447, Val Loss: 2.3026, Val Accuracy: 0.0996\nEpoch 2/100 - Loss: 1.1775, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 3/100 - Loss: 1.1746, Val Loss: 2.3026, Val Accuracy: 0.1003\nEpoch 4/100 - Loss: 1.1726, Val Loss: 2.3026, Val Accuracy: 0.0969\nEpoch 5/100 - Loss: 1.1704, Val Loss: 2.3026, Val Accuracy: 0.1009\nEpoch 6/100 - Loss: 1.1689, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 7/100 - Loss: 1.1674, Val Loss: 2.3026, Val Accuracy: 0.0992\nEpoch 8/100 - Loss: 1.1664, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 9/100 - Loss: 1.1652, Val Loss: 2.3026, Val Accuracy: 0.0977\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 1.3472, Val Loss: 2.3026, Val Accuracy: 0.0976\nEpoch 2/100 - Loss: 1.1804, Val Loss: 2.3026, Val Accuracy: 0.0976\nEpoch 3/100 - Loss: 1.1776, Val Loss: 2.3026, Val Accuracy: 0.0991\nEpoch 4/100 - Loss: 1.1756, Val Loss: 2.3026, Val Accuracy: 0.0983\nEpoch 5/100 - Loss: 1.1734, Val Loss: 2.3026, Val Accuracy: 0.0991\nEpoch 6/100 - Loss: 1.1718, Val Loss: 2.3026, Val Accuracy: 0.1031\nEpoch 7/100 - Loss: 1.1704, Val Loss: 2.3026, Val Accuracy: 0.0976\nEpoch 8/100 - Loss: 1.1692, Val Loss: 2.3026, Val Accuracy: 0.0992\nEpoch 9/100 - Loss: 1.1681, Val Loss: 2.3026, Val Accuracy: 0.0983\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.302571329734981, Validation Accuracy = 0.1017\nFold 1: Validation Loss = 2.302554980015704, Validation Accuracy = 0.1031\nAverage Validation Loss across folds: 2.3025631548753425\nAverage Validation Accuracy across folds: 0.10239999999999999\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.1, L2: 0.0005623413251903491\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 1.3479, Val Loss: 2.3026, Val Accuracy: 0.0992\nEpoch 2/100 - Loss: 1.1824, Val Loss: 2.3026, Val Accuracy: 0.1000\nEpoch 3/100 - Loss: 1.1804, Val Loss: 2.3026, Val Accuracy: 0.0987\nEpoch 4/100 - Loss: 1.1789, Val Loss: 2.3026, Val Accuracy: 0.1000\nEpoch 5/100 - Loss: 1.1768, Val Loss: 2.3026, Val Accuracy: 0.0987\nEpoch 6/100 - Loss: 1.1752, Val Loss: 2.3026, Val Accuracy: 0.1004\nEpoch 7/100 - Loss: 1.1737, Val Loss: 2.3026, Val Accuracy: 0.1000\nEpoch 8/100 - Loss: 1.1724, Val Loss: 2.3026, Val Accuracy: 0.1007\nEpoch 9/100 - Loss: 1.1711, Val Loss: 2.3026, Val Accuracy: 0.1000\nEpoch 10/100 - Loss: 1.1694, Val Loss: 2.3026, Val Accuracy: 0.1007\nEpoch 11/100 - Loss: 1.1684, Val Loss: 2.3026, Val Accuracy: 0.0993\nEpoch 12/100 - Loss: 1.1675, Val Loss: 2.3026, Val Accuracy: 0.1004\nEpoch 13/100 - Loss: 1.1664, Val Loss: 2.3026, Val Accuracy: 0.1000\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 1.3469, Val Loss: 2.3026, Val Accuracy: 0.1008\nEpoch 2/100 - Loss: 1.1808, Val Loss: 2.3026, Val Accuracy: 0.0988\nEpoch 3/100 - Loss: 1.1786, Val Loss: 2.3026, Val Accuracy: 0.0988\nEpoch 4/100 - Loss: 1.1771, Val Loss: 2.3026, Val Accuracy: 0.0996\nEpoch 5/100 - Loss: 1.1750, Val Loss: 2.3026, Val Accuracy: 0.1000\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3025767330421822, Validation Accuracy = 0.10073333333333333\nFold 1: Validation Loss = 2.302581841503379, Validation Accuracy = 0.09883333333333333\nAverage Validation Loss across folds: 2.3025792872727804\nAverage Validation Accuracy across folds: 0.09978333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.1, L2: 0.0031622776601683794\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 1.3499, Val Loss: 2.3026, Val Accuracy: 0.1001\nEpoch 2/100 - Loss: 1.1794, Val Loss: 2.3026, Val Accuracy: 0.1001\nEpoch 3/100 - Loss: 1.1719, Val Loss: 2.3026, Val Accuracy: 0.1001\nEpoch 4/100 - Loss: 1.1663, Val Loss: 2.3026, Val Accuracy: 0.1004\nEpoch 5/100 - Loss: 1.1602, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 6/100 - Loss: 1.1550, Val Loss: 2.3026, Val Accuracy: 0.0997\nEpoch 7/100 - Loss: 1.1506, Val Loss: 2.3026, Val Accuracy: 0.1001\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 1.3511, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 2/100 - Loss: 1.1807, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 3/100 - Loss: 1.1734, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 4/100 - Loss: 1.1676, Val Loss: 2.3026, Val Accuracy: 0.0999\nEpoch 5/100 - Loss: 1.1613, Val Loss: 2.3026, Val Accuracy: 0.1003\nEpoch 6/100 - Loss: 1.1559, Val Loss: 2.3026, Val Accuracy: 0.0996\nEpoch 7/100 - Loss: 1.1516, Val Loss: 2.3026, Val Accuracy: 0.1003\nEpoch 8/100 - Loss: 1.1482, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 9/100 - Loss: 1.1445, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 10/100 - Loss: 1.1411, Val Loss: 2.3026, Val Accuracy: 0.1003\nEpoch 11/100 - Loss: 1.1386, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 12/100 - Loss: 1.1367, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 13/100 - Loss: 1.1362, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 14/100 - Loss: 1.1365, Val Loss: 2.3026, Val Accuracy: 0.0992\nStopping early at epoch 15\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.302580474027947, Validation Accuracy = 0.10036666666666667\nFold 1: Validation Loss = 2.3025680162759223, Validation Accuracy = 0.10173333333333333\nAverage Validation Loss across folds: 2.3025742451519347\nAverage Validation Accuracy across folds: 0.10105\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.1, L2: 0.01778279410038923\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 1.3508, Val Loss: 2.3026, Val Accuracy: 0.1016\nEpoch 2/100 - Loss: 1.1591, Val Loss: 2.3026, Val Accuracy: 0.0989\nEpoch 3/100 - Loss: 1.1487, Val Loss: 2.3026, Val Accuracy: 0.0984\nEpoch 4/100 - Loss: 1.1580, Val Loss: 2.3026, Val Accuracy: 0.0990\nEpoch 5/100 - Loss: 1.1679, Val Loss: 2.3026, Val Accuracy: 0.1022\nEpoch 6/100 - Loss: 1.1835, Val Loss: 2.3026, Val Accuracy: 0.0984\nEpoch 7/100 - Loss: 1.1895, Val Loss: 2.3026, Val Accuracy: 0.0989\nEpoch 8/100 - Loss: 1.1911, Val Loss: 2.3026, Val Accuracy: 0.0999\nEpoch 9/100 - Loss: 1.1917, Val Loss: 2.3026, Val Accuracy: 0.0999\nEpoch 10/100 - Loss: 1.1920, Val Loss: 2.3026, Val Accuracy: 0.0984\nEpoch 11/100 - Loss: 1.1926, Val Loss: 2.3026, Val Accuracy: 0.1013\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 1.3498, Val Loss: 2.3026, Val Accuracy: 0.0978\nEpoch 2/100 - Loss: 1.1583, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 3/100 - Loss: 1.1479, Val Loss: 2.3026, Val Accuracy: 0.0987\nEpoch 4/100 - Loss: 1.1572, Val Loss: 2.3026, Val Accuracy: 0.1191\nEpoch 5/100 - Loss: 1.1673, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 6/100 - Loss: 1.1829, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 7/100 - Loss: 1.1889, Val Loss: 2.3026, Val Accuracy: 0.0984\nEpoch 8/100 - Loss: 1.1904, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 9/100 - Loss: 1.1911, Val Loss: 2.3026, Val Accuracy: 0.0987\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3025647897455994, Validation Accuracy = 0.09993333333333333\nFold 1: Validation Loss = 2.3025678800367344, Validation Accuracy = 0.09976666666666667\nAverage Validation Loss across folds: 2.302566334891167\nAverage Validation Accuracy across folds: 0.09985\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.1, L2: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 1.3712, Val Loss: 2.3026, Val Accuracy: 0.1001\nEpoch 2/100 - Loss: 1.2216, Val Loss: 2.3026, Val Accuracy: 0.1005\nEpoch 3/100 - Loss: 1.2224, Val Loss: 2.3026, Val Accuracy: 0.1005\nEpoch 4/100 - Loss: 1.2226, Val Loss: 2.3026, Val Accuracy: 0.0984\nEpoch 5/100 - Loss: 1.2228, Val Loss: 2.3026, Val Accuracy: 0.0975\nEpoch 6/100 - Loss: 1.2226, Val Loss: 2.3026, Val Accuracy: 0.0993\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 1.3718, Val Loss: 2.3026, Val Accuracy: 0.1310\nEpoch 2/100 - Loss: 1.2222, Val Loss: 2.3026, Val Accuracy: 0.1025\nEpoch 3/100 - Loss: 1.2231, Val Loss: 2.3026, Val Accuracy: 0.1010\nEpoch 4/100 - Loss: 1.2233, Val Loss: 2.3026, Val Accuracy: 0.0986\nEpoch 5/100 - Loss: 1.2235, Val Loss: 2.3026, Val Accuracy: 0.0974\nEpoch 6/100 - Loss: 1.2233, Val Loss: 2.3026, Val Accuracy: 0.0980\nEpoch 7/100 - Loss: 1.2234, Val Loss: 2.3026, Val Accuracy: 0.1016\nEpoch 8/100 - Loss: 1.2235, Val Loss: 2.3026, Val Accuracy: 0.1007\nEpoch 9/100 - Loss: 1.2235, Val Loss: 2.3026, Val Accuracy: 0.1025\nEpoch 10/100 - Loss: 1.2235, Val Loss: 2.3026, Val Accuracy: 0.1016\nEpoch 11/100 - Loss: 1.2236, Val Loss: 2.3026, Val Accuracy: 0.1007\nEpoch 12/100 - Loss: 1.2236, Val Loss: 2.3026, Val Accuracy: 0.0995\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3025793963149668, Validation Accuracy = 0.1005\nFold 1: Validation Loss = 2.3025585674782043, Validation Accuracy = 0.1025\nAverage Validation Loss across folds: 2.3025689818965853\nAverage Validation Accuracy across folds: 0.1015\n--------------------------------\nK-fold Cross-validation completed.\n          l1        l2  train_loss  val_loss  val_accuracy\n0   0.000100  0.000100    0.694800  0.717250      0.043452\n1   0.000100  0.000562    0.709761  0.714033      0.039795\n2   0.000100  0.003162    0.770717  0.707150      0.031289\n3   0.000100  0.017783    0.933120  0.646633      0.024251\n4   0.000100  0.100000    1.385864  0.447433      0.026379\n5   0.000562  0.000100    0.694739  0.733000      0.056297\n6   0.000562  0.000562    0.749390  0.713900      0.053505\n7   0.000562  0.003162    0.777330  0.720000      0.046358\n8   0.000562  0.017783    0.913481  0.667833      0.036259\n9   0.000562  0.100000    1.450749  0.383667      0.033569\n10  0.003162  0.000100    1.032343  0.622533      0.062833\n11  0.003162  0.000562    1.161536  0.533650      0.062517\n12  0.003162  0.003162    1.110480  0.596550      0.062991\n13  0.003162  0.017783    1.680011  0.338300      0.064812\n14  0.003162  0.100000    2.302621  0.101950      0.070583\n15  0.017783  0.000100    2.302554  0.100233      0.234562\n16  0.017783  0.000562    2.302580  0.100200      0.235061\n17  0.017783  0.003162    2.302586  0.101117      0.237253\n18  0.017783  0.017783    2.302607  0.100350      0.246316\n19  0.017783  0.100000    2.302560  0.101850      0.236775\n20  0.100000  0.000100    2.302563  0.102400      1.188609\n21  0.100000  0.000562    2.302579  0.099783      1.191367\n22  0.100000  0.003162    2.302574  0.101050      1.171356\n23  0.100000  0.017783    2.302566  0.099850      1.192878\n24  0.100000  0.100000    2.302569  0.101500      1.237895\n\n\n\n# ordered them wrong, so fixing the column names\nresults_choice_1.rename(columns={'train_loss': 'avg_val_loss'}, inplace=True)\nresults_choice_1.rename(columns={'val_loss': 'average_val_accuracy'}, inplace=True)\nresults_choice_1.rename(columns={'val_accuracy': 'average_train_loss'}, inplace=True)\n\n# save results_df1 to csv\nresults_choice_1.to_csv('results/results_choice_1.csv')\n\n\n# Display results\nresults_choice_1\n\n\n\n\n\n\n\n\nl1\nl2\naverage_val_loss\naverage_val_accuracy\naverage_train_loss\n\n\n\n\n0\n0.000100\n0.000100\n0.694800\n0.717250\n0.043452\n\n\n1\n0.000100\n0.000562\n0.709761\n0.714033\n0.039795\n\n\n2\n0.000100\n0.003162\n0.770717\n0.707150\n0.031289\n\n\n3\n0.000100\n0.017783\n0.933120\n0.646633\n0.024251\n\n\n4\n0.000100\n0.100000\n1.385864\n0.447433\n0.026379\n\n\n5\n0.000562\n0.000100\n0.694739\n0.733000\n0.056297\n\n\n6\n0.000562\n0.000562\n0.749390\n0.713900\n0.053505\n\n\n7\n0.000562\n0.003162\n0.777330\n0.720000\n0.046358\n\n\n8\n0.000562\n0.017783\n0.913481\n0.667833\n0.036259\n\n\n9\n0.000562\n0.100000\n1.450749\n0.383667\n0.033569\n\n\n10\n0.003162\n0.000100\n1.032343\n0.622533\n0.062833\n\n\n11\n0.003162\n0.000562\n1.161536\n0.533650\n0.062517\n\n\n12\n0.003162\n0.003162\n1.110480\n0.596550\n0.062991\n\n\n13\n0.003162\n0.017783\n1.680011\n0.338300\n0.064812\n\n\n14\n0.003162\n0.100000\n2.302621\n0.101950\n0.070583\n\n\n15\n0.017783\n0.000100\n2.302554\n0.100233\n0.234562\n\n\n16\n0.017783\n0.000562\n2.302580\n0.100200\n0.235061\n\n\n17\n0.017783\n0.003162\n2.302586\n0.101117\n0.237253\n\n\n18\n0.017783\n0.017783\n2.302607\n0.100350\n0.246316\n\n\n19\n0.017783\n0.100000\n2.302560\n0.101850\n0.236775\n\n\n20\n0.100000\n0.000100\n2.302563\n0.102400\n1.188609\n\n\n21\n0.100000\n0.000562\n2.302579\n0.099783\n1.191367\n\n\n22\n0.100000\n0.003162\n2.302574\n0.101050\n1.171356\n\n\n23\n0.100000\n0.017783\n2.302566\n0.099850\n1.192878\n\n\n24\n0.100000\n0.100000\n2.302569\n0.101500\n1.237895\n\n\n\n\n\n\n\n\nresults_choice_1 = pd.read_csv('results/results_choice_1.csv')\n\n# pivot the df for heatmap\ntrain_loss_choice_1_data = results_choice_1.pivot(index='l1', columns='l2', values='average_train_loss')\nval_loss_choice_1_data = results_choice_1.pivot(index='l1', columns='l2', values='average_val_loss')\n# error ratio\nerror_ratio_choice_1_data = val_loss_choice_1_data / train_loss_choice_1_data\n\n# Plot heatmaps\nplt.figure(figsize=(14, 12))\n\n# Training Error\nplt.subplot(2, 1, 1)\nsns.heatmap(train_loss_choice_1_data, annot=True, cmap='mako', fmt=\".3f\")\nplt.title('Training Error by L1 and L2 Reg')\nplt.xlabel('L2 Reg')\nplt.ylabel('L1 Reg')\n\n# Validation Error\nplt.subplot(2, 1, 2)\nsns.heatmap(val_loss_choice_1_data, annot=True, cmap='mako', fmt=\".3f\")\nplt.title('Validation Error by L1 and L2 Reg')\nplt.xlabel('L2 Reg')\nplt.ylabel('L1 Reg')\n\n# Show heatmaps\nplt.tight_layout()\nplt.show()\n\n# Error Ratio\nplt.figure(figsize=(7, 6))\nsns.heatmap(error_ratio_choice_1_data, annot=True, cmap='mako', fmt=\".2f\")\nplt.title('Error Ratio by L1 and L2 Reg')\nplt.xlabel('L2 Reg')\nplt.ylabel('L1 Reg')\n\n# Save figure just in case\nplt.savefig('error_ratio.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor hidden activation=sigmoid, hidden layers = [96,96,96], optimizer=rmsprop\nVary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization)\nPlot training and validation error as a function of dropout rate\n\nPlot the ratio (or difference) of validation to training error as a function of dropout rate\n\n\n# model_fn2\ndef model_fn2(dropout_rate=0.0):\n    return NeuralNetwork(input_size=28*28, layer_sizes=[96, 96, 96], activation_funcs=['sigmoid', 'sigmoid', 'sigmoid'], dropout_rate=dropout_rate)\n\ndropout_rates = np.linspace(0.0, 0.9, 10) \n\nresults2 = []\n\nfor dropout_rate in dropout_rates:\n    print(f\"Training with Dropout Rate: {dropout_rate}\")\n    avg_train_loss, avg_val_loss, avg_val_accuracy = training_model_kfold(\n        k_folds=4,\n        num_epochs=100,\n        dataset=training_data,\n        model_fn=lambda: model_fn2(dropout_rate=dropout_rate), \n        loss_fn=nn.CrossEntropyLoss(),\n        optimizer_name=\"RMSProp\",  # \n        lr=0.01,\n        l1_reg=0, \n        l2_reg=0,\n        device=device,\n        patience=4,\n        plot=False)\n    \n    results2.append({\n        'dropout_rate': dropout_rate,\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n        'val_accuracy': avg_val_accuracy})\n\nresults_choice_2 = pd.DataFrame(results2)\nprint(results_choice_2)\n\nTraining with Dropout Rate: 0.0\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0149, Val Loss: 0.6797, Val Accuracy: 0.7635\nEpoch 2/100 - Loss: 0.0080, Val Loss: 0.6904, Val Accuracy: 0.7749\nEpoch 3/100 - Loss: 0.0071, Val Loss: 0.5336, Val Accuracy: 0.8151\nEpoch 4/100 - Loss: 0.0067, Val Loss: 0.4747, Val Accuracy: 0.8267\nEpoch 5/100 - Loss: 0.0065, Val Loss: 0.4366, Val Accuracy: 0.8459\nEpoch 6/100 - Loss: 0.0063, Val Loss: 0.4410, Val Accuracy: 0.8434\nEpoch 7/100 - Loss: 0.0061, Val Loss: 0.4566, Val Accuracy: 0.8116\nEpoch 8/100 - Loss: 0.0060, Val Loss: 0.4628, Val Accuracy: 0.8339\nEpoch 9/100 - Loss: 0.0059, Val Loss: 0.4293, Val Accuracy: 0.8459\nEpoch 10/100 - Loss: 0.0058, Val Loss: 0.5254, Val Accuracy: 0.8048\nEpoch 11/100 - Loss: 0.0057, Val Loss: 0.3938, Val Accuracy: 0.8599\nEpoch 12/100 - Loss: 0.0056, Val Loss: 0.4080, Val Accuracy: 0.8571\nEpoch 13/100 - Loss: 0.0056, Val Loss: 0.4058, Val Accuracy: 0.8571\nEpoch 14/100 - Loss: 0.0055, Val Loss: 0.4593, Val Accuracy: 0.8443\nStopping early at epoch 15\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0145, Val Loss: 0.5871, Val Accuracy: 0.7849\nEpoch 2/100 - Loss: 0.0078, Val Loss: 0.6239, Val Accuracy: 0.7758\nEpoch 3/100 - Loss: 0.0070, Val Loss: 0.5172, Val Accuracy: 0.7965\nEpoch 4/100 - Loss: 0.0066, Val Loss: 0.5219, Val Accuracy: 0.7971\nEpoch 5/100 - Loss: 0.0064, Val Loss: 0.4407, Val Accuracy: 0.8289\nEpoch 6/100 - Loss: 0.0062, Val Loss: 0.4542, Val Accuracy: 0.8287\nEpoch 7/100 - Loss: 0.0061, Val Loss: 0.4589, Val Accuracy: 0.8413\nEpoch 8/100 - Loss: 0.0060, Val Loss: 0.3975, Val Accuracy: 0.8610\nEpoch 9/100 - Loss: 0.0058, Val Loss: 0.4417, Val Accuracy: 0.8220\nEpoch 10/100 - Loss: 0.0058, Val Loss: 0.4756, Val Accuracy: 0.8326\nEpoch 11/100 - Loss: 0.0056, Val Loss: 0.4003, Val Accuracy: 0.8596\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0131, Val Loss: 0.8770, Val Accuracy: 0.6934\nEpoch 2/100 - Loss: 0.0080, Val Loss: 0.6589, Val Accuracy: 0.7642\nEpoch 3/100 - Loss: 0.0071, Val Loss: 0.4513, Val Accuracy: 0.8383\nEpoch 4/100 - Loss: 0.0067, Val Loss: 0.4299, Val Accuracy: 0.8513\nEpoch 5/100 - Loss: 0.0064, Val Loss: 0.8011, Val Accuracy: 0.6885\nEpoch 6/100 - Loss: 0.0062, Val Loss: 0.4721, Val Accuracy: 0.8265\nEpoch 7/100 - Loss: 0.0060, Val Loss: 0.4446, Val Accuracy: 0.8345\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0145, Val Loss: 1.0461, Val Accuracy: 0.6389\nEpoch 2/100 - Loss: 0.0080, Val Loss: 0.6594, Val Accuracy: 0.7727\nEpoch 3/100 - Loss: 0.0071, Val Loss: 0.7239, Val Accuracy: 0.7329\nEpoch 4/100 - Loss: 0.0068, Val Loss: 0.5455, Val Accuracy: 0.7991\nEpoch 5/100 - Loss: 0.0065, Val Loss: 0.4989, Val Accuracy: 0.8243\nEpoch 6/100 - Loss: 0.0062, Val Loss: 0.4405, Val Accuracy: 0.8375\nEpoch 7/100 - Loss: 0.0060, Val Loss: 0.4514, Val Accuracy: 0.8415\nEpoch 8/100 - Loss: 0.0059, Val Loss: 0.6356, Val Accuracy: 0.7794\nEpoch 9/100 - Loss: 0.0058, Val Loss: 0.4439, Val Accuracy: 0.8289\nEpoch 10/100 - Loss: 0.0058, Val Loss: 0.4396, Val Accuracy: 0.8473\nEpoch 11/100 - Loss: 0.0057, Val Loss: 0.4292, Val Accuracy: 0.8465\nEpoch 12/100 - Loss: 0.0057, Val Loss: 0.4844, Val Accuracy: 0.8323\nEpoch 13/100 - Loss: 0.0056, Val Loss: 0.4105, Val Accuracy: 0.8589\nEpoch 14/100 - Loss: 0.0055, Val Loss: 0.4215, Val Accuracy: 0.8504\nEpoch 15/100 - Loss: 0.0055, Val Loss: 0.4017, Val Accuracy: 0.8615\nEpoch 16/100 - Loss: 0.0053, Val Loss: 0.4581, Val Accuracy: 0.8407\nEpoch 17/100 - Loss: 0.0053, Val Loss: 0.3965, Val Accuracy: 0.8608\nEpoch 18/100 - Loss: 0.0052, Val Loss: 0.5352, Val Accuracy: 0.8010\nEpoch 19/100 - Loss: 0.0053, Val Loss: 0.4783, Val Accuracy: 0.8354\nEpoch 20/100 - Loss: 0.0052, Val Loss: 0.4401, Val Accuracy: 0.8643\nEpoch 21/100 - Loss: 0.0052, Val Loss: 0.3765, Val Accuracy: 0.8673\nEpoch 22/100 - Loss: 0.0051, Val Loss: 0.4208, Val Accuracy: 0.8460\nEpoch 23/100 - Loss: 0.0051, Val Loss: 0.3926, Val Accuracy: 0.8627\nEpoch 24/100 - Loss: 0.0050, Val Loss: 0.4151, Val Accuracy: 0.8583\nStopping early at epoch 25\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.39338877023534574, Validation Accuracy = 0.8599333333333333\nFold 1: Validation Loss = 0.398093506376794, Validation Accuracy = 0.861\nFold 2: Validation Loss = 0.43015647904669985, Validation Accuracy = 0.8513333333333334\nFold 3: Validation Loss = 0.37609021834870604, Validation Accuracy = 0.8672666666666666\nAverage Validation Loss across folds: 0.3994322435018864\nAverage Validation Accuracy across folds: 0.8598833333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0135, Val Loss: 0.7462, Val Accuracy: 0.7168\nEpoch 2/100 - Loss: 0.0085, Val Loss: 0.5816, Val Accuracy: 0.8049\nEpoch 3/100 - Loss: 0.0077, Val Loss: 0.4228, Val Accuracy: 0.8482\nEpoch 4/100 - Loss: 0.0073, Val Loss: 0.4355, Val Accuracy: 0.8460\nEpoch 5/100 - Loss: 0.0070, Val Loss: 0.4290, Val Accuracy: 0.8447\nEpoch 6/100 - Loss: 0.0068, Val Loss: 0.4655, Val Accuracy: 0.8275\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0139, Val Loss: 0.5745, Val Accuracy: 0.7755\nEpoch 2/100 - Loss: 0.0085, Val Loss: 0.5323, Val Accuracy: 0.8029\nEpoch 3/100 - Loss: 0.0076, Val Loss: 0.4416, Val Accuracy: 0.8413\nEpoch 4/100 - Loss: 0.0072, Val Loss: 0.4608, Val Accuracy: 0.8331\nEpoch 5/100 - Loss: 0.0070, Val Loss: 0.4440, Val Accuracy: 0.8362\nEpoch 6/100 - Loss: 0.0068, Val Loss: 0.4572, Val Accuracy: 0.8438\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0147, Val Loss: 0.6639, Val Accuracy: 0.7465\nEpoch 2/100 - Loss: 0.0087, Val Loss: 0.5118, Val Accuracy: 0.8269\nEpoch 3/100 - Loss: 0.0076, Val Loss: 0.4950, Val Accuracy: 0.8260\nEpoch 4/100 - Loss: 0.0073, Val Loss: 0.7773, Val Accuracy: 0.7218\nEpoch 5/100 - Loss: 0.0070, Val Loss: 0.4587, Val Accuracy: 0.8229\nEpoch 6/100 - Loss: 0.0069, Val Loss: 0.4402, Val Accuracy: 0.8394\nEpoch 7/100 - Loss: 0.0067, Val Loss: 0.4286, Val Accuracy: 0.8413\nEpoch 8/100 - Loss: 0.0065, Val Loss: 0.4064, Val Accuracy: 0.8576\nEpoch 9/100 - Loss: 0.0064, Val Loss: 0.4158, Val Accuracy: 0.8523\nEpoch 10/100 - Loss: 0.0062, Val Loss: 0.3926, Val Accuracy: 0.8607\nEpoch 11/100 - Loss: 0.0061, Val Loss: 0.4085, Val Accuracy: 0.8601\nEpoch 12/100 - Loss: 0.0062, Val Loss: 0.4265, Val Accuracy: 0.8271\nEpoch 13/100 - Loss: 0.0061, Val Loss: 0.5793, Val Accuracy: 0.7595\nEpoch 14/100 - Loss: 0.0061, Val Loss: 0.3794, Val Accuracy: 0.8639\nEpoch 15/100 - Loss: 0.0059, Val Loss: 0.3819, Val Accuracy: 0.8651\nEpoch 16/100 - Loss: 0.0059, Val Loss: 0.4307, Val Accuracy: 0.8443\nEpoch 17/100 - Loss: 0.0059, Val Loss: 0.4406, Val Accuracy: 0.8427\nEpoch 18/100 - Loss: 0.0057, Val Loss: 0.3604, Val Accuracy: 0.8716\nEpoch 19/100 - Loss: 0.0057, Val Loss: 0.3998, Val Accuracy: 0.8584\nEpoch 20/100 - Loss: 0.0058, Val Loss: 0.3878, Val Accuracy: 0.8625\nEpoch 21/100 - Loss: 0.0057, Val Loss: 0.3930, Val Accuracy: 0.8699\nStopping early at epoch 22\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0157, Val Loss: 0.5807, Val Accuracy: 0.7693\nEpoch 2/100 - Loss: 0.0084, Val Loss: 0.4969, Val Accuracy: 0.8166\nEpoch 3/100 - Loss: 0.0075, Val Loss: 0.4678, Val Accuracy: 0.8332\nEpoch 4/100 - Loss: 0.0070, Val Loss: 0.4963, Val Accuracy: 0.8165\nEpoch 5/100 - Loss: 0.0068, Val Loss: 0.4965, Val Accuracy: 0.8181\nEpoch 6/100 - Loss: 0.0066, Val Loss: 0.4813, Val Accuracy: 0.8362\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.42180526782857614, Validation Accuracy = 0.8482\nFold 1: Validation Loss = 0.4418250153673456, Validation Accuracy = 0.8412666666666667\nFold 2: Validation Loss = 0.3601872922258174, Validation Accuracy = 0.8716\nFold 3: Validation Loss = 0.466986808814901, Validation Accuracy = 0.8332\nAverage Validation Loss across folds: 0.42270109605916\nAverage Validation Accuracy across folds: 0.8485666666666667\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.2\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0145, Val Loss: 0.9972, Val Accuracy: 0.6769\nEpoch 2/100 - Loss: 0.0087, Val Loss: 0.5679, Val Accuracy: 0.7798\nEpoch 3/100 - Loss: 0.0079, Val Loss: 0.4856, Val Accuracy: 0.8160\nEpoch 4/100 - Loss: 0.0076, Val Loss: 0.4528, Val Accuracy: 0.8340\nEpoch 5/100 - Loss: 0.0073, Val Loss: 0.4050, Val Accuracy: 0.8471\nEpoch 6/100 - Loss: 0.0072, Val Loss: 0.4403, Val Accuracy: 0.8381\nEpoch 7/100 - Loss: 0.0071, Val Loss: 0.3981, Val Accuracy: 0.8523\nEpoch 8/100 - Loss: 0.0069, Val Loss: 0.4267, Val Accuracy: 0.8419\nEpoch 9/100 - Loss: 0.0067, Val Loss: 0.4342, Val Accuracy: 0.8389\nEpoch 10/100 - Loss: 0.0067, Val Loss: 0.3797, Val Accuracy: 0.8573\nEpoch 11/100 - Loss: 0.0067, Val Loss: 0.4323, Val Accuracy: 0.8329\nEpoch 12/100 - Loss: 0.0065, Val Loss: 0.3933, Val Accuracy: 0.8516\nEpoch 13/100 - Loss: 0.0065, Val Loss: 0.3756, Val Accuracy: 0.8634\nEpoch 14/100 - Loss: 0.0065, Val Loss: 0.3918, Val Accuracy: 0.8588\nEpoch 15/100 - Loss: 0.0064, Val Loss: 0.3983, Val Accuracy: 0.8555\nEpoch 16/100 - Loss: 0.0065, Val Loss: 0.4101, Val Accuracy: 0.8415\nStopping early at epoch 17\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0141, Val Loss: 0.6304, Val Accuracy: 0.7747\nEpoch 2/100 - Loss: 0.0094, Val Loss: 0.5978, Val Accuracy: 0.7621\nEpoch 3/100 - Loss: 0.0083, Val Loss: 0.5297, Val Accuracy: 0.8133\nEpoch 4/100 - Loss: 0.0080, Val Loss: 0.4793, Val Accuracy: 0.8308\nEpoch 5/100 - Loss: 0.0077, Val Loss: 0.4409, Val Accuracy: 0.8490\nEpoch 6/100 - Loss: 0.0074, Val Loss: 0.4881, Val Accuracy: 0.8229\nEpoch 7/100 - Loss: 0.0072, Val Loss: 0.4389, Val Accuracy: 0.8446\nEpoch 8/100 - Loss: 0.0071, Val Loss: 0.4334, Val Accuracy: 0.8492\nEpoch 9/100 - Loss: 0.0070, Val Loss: 0.3999, Val Accuracy: 0.8573\nEpoch 10/100 - Loss: 0.0069, Val Loss: 0.4424, Val Accuracy: 0.8345\nEpoch 11/100 - Loss: 0.0068, Val Loss: 0.4030, Val Accuracy: 0.8537\nEpoch 12/100 - Loss: 0.0066, Val Loss: 0.3908, Val Accuracy: 0.8545\nEpoch 13/100 - Loss: 0.0066, Val Loss: 0.3930, Val Accuracy: 0.8592\nEpoch 14/100 - Loss: 0.0065, Val Loss: 0.4348, Val Accuracy: 0.8431\nEpoch 15/100 - Loss: 0.0066, Val Loss: 0.4543, Val Accuracy: 0.8357\nStopping early at epoch 16\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0136, Val Loss: 0.7337, Val Accuracy: 0.7360\nEpoch 2/100 - Loss: 0.0090, Val Loss: 0.5160, Val Accuracy: 0.8143\nEpoch 3/100 - Loss: 0.0082, Val Loss: 0.4720, Val Accuracy: 0.8340\nEpoch 4/100 - Loss: 0.0077, Val Loss: 0.5342, Val Accuracy: 0.7985\nEpoch 5/100 - Loss: 0.0076, Val Loss: 0.5448, Val Accuracy: 0.7973\nEpoch 6/100 - Loss: 0.0074, Val Loss: 0.4398, Val Accuracy: 0.8393\nEpoch 7/100 - Loss: 0.0071, Val Loss: 0.4392, Val Accuracy: 0.8351\nEpoch 8/100 - Loss: 0.0070, Val Loss: 0.4351, Val Accuracy: 0.8500\nEpoch 9/100 - Loss: 0.0070, Val Loss: 0.4117, Val Accuracy: 0.8502\nEpoch 10/100 - Loss: 0.0068, Val Loss: 0.4026, Val Accuracy: 0.8550\nEpoch 11/100 - Loss: 0.0068, Val Loss: 0.3963, Val Accuracy: 0.8544\nEpoch 12/100 - Loss: 0.0067, Val Loss: 0.4500, Val Accuracy: 0.8415\nEpoch 13/100 - Loss: 0.0066, Val Loss: 0.3979, Val Accuracy: 0.8616\nEpoch 14/100 - Loss: 0.0066, Val Loss: 0.4363, Val Accuracy: 0.8342\nStopping early at epoch 15\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0135, Val Loss: 0.6715, Val Accuracy: 0.7301\nEpoch 2/100 - Loss: 0.0090, Val Loss: 0.5065, Val Accuracy: 0.8096\nEpoch 3/100 - Loss: 0.0080, Val Loss: 0.4929, Val Accuracy: 0.8122\nEpoch 4/100 - Loss: 0.0076, Val Loss: 0.4666, Val Accuracy: 0.8142\nEpoch 5/100 - Loss: 0.0074, Val Loss: 0.4799, Val Accuracy: 0.7929\nEpoch 6/100 - Loss: 0.0072, Val Loss: 0.4252, Val Accuracy: 0.8414\nEpoch 7/100 - Loss: 0.0070, Val Loss: 0.4292, Val Accuracy: 0.8344\nEpoch 8/100 - Loss: 0.0070, Val Loss: 0.4283, Val Accuracy: 0.8416\nEpoch 9/100 - Loss: 0.0068, Val Loss: 0.4498, Val Accuracy: 0.8333\nEpoch 10/100 - Loss: 0.0068, Val Loss: 0.4025, Val Accuracy: 0.8515\nEpoch 11/100 - Loss: 0.0067, Val Loss: 0.3929, Val Accuracy: 0.8620\nEpoch 12/100 - Loss: 0.0066, Val Loss: 0.3781, Val Accuracy: 0.8650\nEpoch 13/100 - Loss: 0.0066, Val Loss: 0.3897, Val Accuracy: 0.8559\nEpoch 14/100 - Loss: 0.0065, Val Loss: 0.3854, Val Accuracy: 0.8645\nEpoch 15/100 - Loss: 0.0065, Val Loss: 0.4201, Val Accuracy: 0.8487\nEpoch 16/100 - Loss: 0.0064, Val Loss: 0.3704, Val Accuracy: 0.8595\nEpoch 17/100 - Loss: 0.0064, Val Loss: 0.3893, Val Accuracy: 0.8619\nEpoch 18/100 - Loss: 0.0064, Val Loss: 0.4822, Val Accuracy: 0.8149\nEpoch 19/100 - Loss: 0.0063, Val Loss: 0.3814, Val Accuracy: 0.8629\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.3761543355723645, Validation Accuracy = 0.8634\nFold 1: Validation Loss = 0.3920463411731923, Validation Accuracy = 0.8544666666666667\nFold 2: Validation Loss = 0.39532898876895295, Validation Accuracy = 0.8544\nFold 3: Validation Loss = 0.36997933406779104, Validation Accuracy = 0.8594666666666667\nAverage Validation Loss across folds: 0.3833772498955752\nAverage Validation Accuracy across folds: 0.8579333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.30000000000000004\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0145, Val Loss: 0.7604, Val Accuracy: 0.7199\nEpoch 2/100 - Loss: 0.0099, Val Loss: 0.6103, Val Accuracy: 0.7728\nEpoch 3/100 - Loss: 0.0089, Val Loss: 0.4969, Val Accuracy: 0.8169\nEpoch 4/100 - Loss: 0.0085, Val Loss: 0.6252, Val Accuracy: 0.8087\nEpoch 5/100 - Loss: 0.0081, Val Loss: 0.5061, Val Accuracy: 0.8245\nEpoch 6/100 - Loss: 0.0078, Val Loss: 0.4784, Val Accuracy: 0.8311\nEpoch 7/100 - Loss: 0.0077, Val Loss: 0.4669, Val Accuracy: 0.8312\nEpoch 8/100 - Loss: 0.0076, Val Loss: 0.5277, Val Accuracy: 0.8097\nEpoch 9/100 - Loss: 0.0075, Val Loss: 0.4571, Val Accuracy: 0.8392\nEpoch 10/100 - Loss: 0.0075, Val Loss: 0.4316, Val Accuracy: 0.8407\nEpoch 11/100 - Loss: 0.0073, Val Loss: 0.4328, Val Accuracy: 0.8416\nEpoch 12/100 - Loss: 0.0072, Val Loss: 0.4703, Val Accuracy: 0.8289\nEpoch 13/100 - Loss: 0.0071, Val Loss: 0.4525, Val Accuracy: 0.8372\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0146, Val Loss: 0.7173, Val Accuracy: 0.7061\nEpoch 2/100 - Loss: 0.0099, Val Loss: 0.5725, Val Accuracy: 0.7883\nEpoch 3/100 - Loss: 0.0090, Val Loss: 0.5208, Val Accuracy: 0.8136\nEpoch 4/100 - Loss: 0.0085, Val Loss: 0.6026, Val Accuracy: 0.7719\nEpoch 5/100 - Loss: 0.0083, Val Loss: 0.4891, Val Accuracy: 0.8248\nEpoch 6/100 - Loss: 0.0080, Val Loss: 0.4617, Val Accuracy: 0.8351\nEpoch 7/100 - Loss: 0.0078, Val Loss: 0.4208, Val Accuracy: 0.8467\nEpoch 8/100 - Loss: 0.0076, Val Loss: 0.4270, Val Accuracy: 0.8395\nEpoch 9/100 - Loss: 0.0075, Val Loss: 0.4294, Val Accuracy: 0.8485\nEpoch 10/100 - Loss: 0.0076, Val Loss: 0.4374, Val Accuracy: 0.8425\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0151, Val Loss: 0.6268, Val Accuracy: 0.7647\nEpoch 2/100 - Loss: 0.0100, Val Loss: 0.8888, Val Accuracy: 0.7232\nEpoch 3/100 - Loss: 0.0090, Val Loss: 0.5041, Val Accuracy: 0.8021\nEpoch 4/100 - Loss: 0.0085, Val Loss: 0.5040, Val Accuracy: 0.8046\nEpoch 5/100 - Loss: 0.0080, Val Loss: 0.4505, Val Accuracy: 0.8421\nEpoch 6/100 - Loss: 0.0079, Val Loss: 0.4365, Val Accuracy: 0.8399\nEpoch 7/100 - Loss: 0.0078, Val Loss: 0.4232, Val Accuracy: 0.8467\nEpoch 8/100 - Loss: 0.0076, Val Loss: 0.4475, Val Accuracy: 0.8415\nEpoch 9/100 - Loss: 0.0075, Val Loss: 0.4436, Val Accuracy: 0.8397\nEpoch 10/100 - Loss: 0.0075, Val Loss: 0.4116, Val Accuracy: 0.8541\nEpoch 11/100 - Loss: 0.0075, Val Loss: 0.4117, Val Accuracy: 0.8524\nEpoch 12/100 - Loss: 0.0073, Val Loss: 0.4015, Val Accuracy: 0.8603\nEpoch 13/100 - Loss: 0.0073, Val Loss: 0.4200, Val Accuracy: 0.8531\nEpoch 14/100 - Loss: 0.0072, Val Loss: 0.4216, Val Accuracy: 0.8502\nEpoch 15/100 - Loss: 0.0071, Val Loss: 0.4207, Val Accuracy: 0.8537\nStopping early at epoch 16\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0155, Val Loss: 0.6625, Val Accuracy: 0.7335\nEpoch 2/100 - Loss: 0.0100, Val Loss: 0.6379, Val Accuracy: 0.7475\nEpoch 3/100 - Loss: 0.0091, Val Loss: 0.5072, Val Accuracy: 0.8157\nEpoch 4/100 - Loss: 0.0086, Val Loss: 0.4262, Val Accuracy: 0.8412\nEpoch 5/100 - Loss: 0.0083, Val Loss: 0.4551, Val Accuracy: 0.8338\nEpoch 6/100 - Loss: 0.0080, Val Loss: 0.4318, Val Accuracy: 0.8457\nEpoch 7/100 - Loss: 0.0079, Val Loss: 0.4059, Val Accuracy: 0.8505\nEpoch 8/100 - Loss: 0.0078, Val Loss: 0.4441, Val Accuracy: 0.8333\nEpoch 9/100 - Loss: 0.0077, Val Loss: 0.4579, Val Accuracy: 0.8307\nEpoch 10/100 - Loss: 0.0076, Val Loss: 0.4033, Val Accuracy: 0.8495\nEpoch 11/100 - Loss: 0.0075, Val Loss: 0.4089, Val Accuracy: 0.8547\nEpoch 12/100 - Loss: 0.0075, Val Loss: 0.3896, Val Accuracy: 0.8595\nEpoch 13/100 - Loss: 0.0073, Val Loss: 0.3829, Val Accuracy: 0.8601\nEpoch 14/100 - Loss: 0.0073, Val Loss: 0.3951, Val Accuracy: 0.8599\nEpoch 15/100 - Loss: 0.0073, Val Loss: 0.4325, Val Accuracy: 0.8383\nEpoch 16/100 - Loss: 0.0071, Val Loss: 0.3805, Val Accuracy: 0.8570\nEpoch 17/100 - Loss: 0.0070, Val Loss: 0.3995, Val Accuracy: 0.8573\nEpoch 18/100 - Loss: 0.0070, Val Loss: 0.4048, Val Accuracy: 0.8478\nEpoch 19/100 - Loss: 0.0070, Val Loss: 0.3824, Val Accuracy: 0.8611\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.43241417205080074, Validation Accuracy = 0.8406666666666667\nFold 1: Validation Loss = 0.4199806311663161, Validation Accuracy = 0.8466666666666667\nFold 2: Validation Loss = 0.4011681196537424, Validation Accuracy = 0.8602666666666666\nFold 3: Validation Loss = 0.380200864501456, Validation Accuracy = 0.857\nAverage Validation Loss across folds: 0.40844094684307886\nAverage Validation Accuracy across folds: 0.8511500000000001\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.4\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0153, Val Loss: 0.6760, Val Accuracy: 0.7079\nEpoch 2/100 - Loss: 0.0108, Val Loss: 0.6038, Val Accuracy: 0.7746\nEpoch 3/100 - Loss: 0.0099, Val Loss: 0.5495, Val Accuracy: 0.7869\nEpoch 4/100 - Loss: 0.0095, Val Loss: 0.4979, Val Accuracy: 0.8145\nEpoch 5/100 - Loss: 0.0092, Val Loss: 0.4765, Val Accuracy: 0.8208\nEpoch 6/100 - Loss: 0.0089, Val Loss: 0.4627, Val Accuracy: 0.8386\nEpoch 7/100 - Loss: 0.0087, Val Loss: 0.4653, Val Accuracy: 0.8333\nEpoch 8/100 - Loss: 0.0085, Val Loss: 0.4544, Val Accuracy: 0.8399\nEpoch 9/100 - Loss: 0.0084, Val Loss: 0.4359, Val Accuracy: 0.8389\nEpoch 10/100 - Loss: 0.0083, Val Loss: 0.4355, Val Accuracy: 0.8407\nEpoch 11/100 - Loss: 0.0082, Val Loss: 0.4437, Val Accuracy: 0.8448\nEpoch 12/100 - Loss: 0.0082, Val Loss: 0.4295, Val Accuracy: 0.8441\nEpoch 13/100 - Loss: 0.0080, Val Loss: 0.4146, Val Accuracy: 0.8519\nEpoch 14/100 - Loss: 0.0080, Val Loss: 0.4315, Val Accuracy: 0.8463\nEpoch 15/100 - Loss: 0.0079, Val Loss: 0.4251, Val Accuracy: 0.8465\nEpoch 16/100 - Loss: 0.0078, Val Loss: 0.4364, Val Accuracy: 0.8479\nStopping early at epoch 17\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0167, Val Loss: 0.6957, Val Accuracy: 0.7241\nEpoch 2/100 - Loss: 0.0111, Val Loss: 0.5813, Val Accuracy: 0.7495\nEpoch 3/100 - Loss: 0.0101, Val Loss: 0.5659, Val Accuracy: 0.7595\nEpoch 4/100 - Loss: 0.0097, Val Loss: 0.5177, Val Accuracy: 0.8009\nEpoch 5/100 - Loss: 0.0093, Val Loss: 0.5031, Val Accuracy: 0.8104\nEpoch 6/100 - Loss: 0.0089, Val Loss: 0.5451, Val Accuracy: 0.7700\nEpoch 7/100 - Loss: 0.0088, Val Loss: 0.4642, Val Accuracy: 0.8327\nEpoch 8/100 - Loss: 0.0086, Val Loss: 0.4353, Val Accuracy: 0.8429\nEpoch 9/100 - Loss: 0.0084, Val Loss: 0.4951, Val Accuracy: 0.8268\nEpoch 10/100 - Loss: 0.0083, Val Loss: 0.4353, Val Accuracy: 0.8495\nEpoch 11/100 - Loss: 0.0081, Val Loss: 0.4275, Val Accuracy: 0.8480\nEpoch 12/100 - Loss: 0.0081, Val Loss: 0.4725, Val Accuracy: 0.8117\nEpoch 13/100 - Loss: 0.0081, Val Loss: 0.4456, Val Accuracy: 0.8431\nEpoch 14/100 - Loss: 0.0080, Val Loss: 0.4491, Val Accuracy: 0.8455\nEpoch 15/100 - Loss: 0.0080, Val Loss: 0.4125, Val Accuracy: 0.8562\nEpoch 16/100 - Loss: 0.0078, Val Loss: 0.4524, Val Accuracy: 0.8457\nEpoch 17/100 - Loss: 0.0078, Val Loss: 0.4148, Val Accuracy: 0.8519\nEpoch 18/100 - Loss: 0.0078, Val Loss: 0.4027, Val Accuracy: 0.8559\nEpoch 19/100 - Loss: 0.0077, Val Loss: 0.4282, Val Accuracy: 0.8490\nEpoch 20/100 - Loss: 0.0077, Val Loss: 0.4249, Val Accuracy: 0.8473\nEpoch 21/100 - Loss: 0.0077, Val Loss: 0.4167, Val Accuracy: 0.8483\nStopping early at epoch 22\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0168, Val Loss: 0.6948, Val Accuracy: 0.7217\nEpoch 2/100 - Loss: 0.0112, Val Loss: 0.5786, Val Accuracy: 0.7732\nEpoch 3/100 - Loss: 0.0100, Val Loss: 0.5121, Val Accuracy: 0.8015\nEpoch 4/100 - Loss: 0.0093, Val Loss: 0.4931, Val Accuracy: 0.8148\nEpoch 5/100 - Loss: 0.0090, Val Loss: 0.4446, Val Accuracy: 0.8429\nEpoch 6/100 - Loss: 0.0088, Val Loss: 0.4467, Val Accuracy: 0.8433\nEpoch 7/100 - Loss: 0.0085, Val Loss: 0.4317, Val Accuracy: 0.8423\nEpoch 8/100 - Loss: 0.0084, Val Loss: 0.4536, Val Accuracy: 0.8423\nEpoch 9/100 - Loss: 0.0084, Val Loss: 0.4484, Val Accuracy: 0.8391\nEpoch 10/100 - Loss: 0.0082, Val Loss: 0.4386, Val Accuracy: 0.8431\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0156, Val Loss: 0.6936, Val Accuracy: 0.7159\nEpoch 2/100 - Loss: 0.0107, Val Loss: 0.5412, Val Accuracy: 0.7902\nEpoch 3/100 - Loss: 0.0099, Val Loss: 0.5445, Val Accuracy: 0.7990\nEpoch 4/100 - Loss: 0.0093, Val Loss: 0.5067, Val Accuracy: 0.8263\nEpoch 5/100 - Loss: 0.0089, Val Loss: 0.4585, Val Accuracy: 0.8359\nEpoch 6/100 - Loss: 0.0087, Val Loss: 0.4565, Val Accuracy: 0.8376\nEpoch 7/100 - Loss: 0.0085, Val Loss: 0.4395, Val Accuracy: 0.8419\nEpoch 8/100 - Loss: 0.0083, Val Loss: 0.4266, Val Accuracy: 0.8489\nEpoch 9/100 - Loss: 0.0083, Val Loss: 0.4452, Val Accuracy: 0.8443\nEpoch 10/100 - Loss: 0.0081, Val Loss: 0.4447, Val Accuracy: 0.8338\nEpoch 11/100 - Loss: 0.0080, Val Loss: 0.4164, Val Accuracy: 0.8524\nEpoch 12/100 - Loss: 0.0079, Val Loss: 0.4426, Val Accuracy: 0.8486\nEpoch 13/100 - Loss: 0.0078, Val Loss: 0.4136, Val Accuracy: 0.8509\nEpoch 14/100 - Loss: 0.0078, Val Loss: 0.4215, Val Accuracy: 0.8563\nEpoch 15/100 - Loss: 0.0078, Val Loss: 0.4173, Val Accuracy: 0.8515\nEpoch 16/100 - Loss: 0.0077, Val Loss: 0.3996, Val Accuracy: 0.8597\nEpoch 17/100 - Loss: 0.0076, Val Loss: 0.4078, Val Accuracy: 0.8525\nEpoch 18/100 - Loss: 0.0076, Val Loss: 0.4071, Val Accuracy: 0.8529\nEpoch 19/100 - Loss: 0.0077, Val Loss: 0.4061, Val Accuracy: 0.8573\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.41576223373413085, Validation Accuracy = 0.8519333333333333\nFold 1: Validation Loss = 0.4040565474236265, Validation Accuracy = 0.8558666666666667\nFold 2: Validation Loss = 0.4315893105369933, Validation Accuracy = 0.8423333333333334\nFold 3: Validation Loss = 0.3999258263314024, Validation Accuracy = 0.8596666666666667\nAverage Validation Loss across folds: 0.41283347950653826\nAverage Validation Accuracy across folds: 0.8524499999999999\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.5\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0175, Val Loss: 0.7656, Val Accuracy: 0.6767\nEpoch 2/100 - Loss: 0.0130, Val Loss: 0.6674, Val Accuracy: 0.7293\nEpoch 3/100 - Loss: 0.0116, Val Loss: 0.6054, Val Accuracy: 0.7841\nEpoch 4/100 - Loss: 0.0108, Val Loss: 0.5525, Val Accuracy: 0.7873\nEpoch 5/100 - Loss: 0.0106, Val Loss: 0.5251, Val Accuracy: 0.8097\nEpoch 6/100 - Loss: 0.0102, Val Loss: 0.5064, Val Accuracy: 0.8169\nEpoch 7/100 - Loss: 0.0099, Val Loss: 0.4917, Val Accuracy: 0.8321\nEpoch 8/100 - Loss: 0.0097, Val Loss: 0.4967, Val Accuracy: 0.8239\nEpoch 9/100 - Loss: 0.0096, Val Loss: 0.4738, Val Accuracy: 0.8328\nEpoch 10/100 - Loss: 0.0095, Val Loss: 0.4716, Val Accuracy: 0.8326\nEpoch 11/100 - Loss: 0.0094, Val Loss: 0.4636, Val Accuracy: 0.8410\nEpoch 12/100 - Loss: 0.0093, Val Loss: 0.4510, Val Accuracy: 0.8393\nEpoch 13/100 - Loss: 0.0093, Val Loss: 0.4557, Val Accuracy: 0.8436\nEpoch 14/100 - Loss: 0.0091, Val Loss: 0.4700, Val Accuracy: 0.8338\nEpoch 15/100 - Loss: 0.0091, Val Loss: 0.4545, Val Accuracy: 0.8435\nStopping early at epoch 16\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0184, Val Loss: 0.7466, Val Accuracy: 0.6954\nEpoch 2/100 - Loss: 0.0124, Val Loss: 0.6205, Val Accuracy: 0.7365\nEpoch 3/100 - Loss: 0.0115, Val Loss: 0.6061, Val Accuracy: 0.7668\nEpoch 4/100 - Loss: 0.0109, Val Loss: 0.5617, Val Accuracy: 0.7899\nEpoch 5/100 - Loss: 0.0106, Val Loss: 0.5498, Val Accuracy: 0.7934\nEpoch 6/100 - Loss: 0.0103, Val Loss: 0.5692, Val Accuracy: 0.7789\nEpoch 7/100 - Loss: 0.0102, Val Loss: 0.5273, Val Accuracy: 0.7999\nEpoch 8/100 - Loss: 0.0099, Val Loss: 0.5096, Val Accuracy: 0.8085\nEpoch 9/100 - Loss: 0.0097, Val Loss: 0.4935, Val Accuracy: 0.8235\nEpoch 10/100 - Loss: 0.0097, Val Loss: 0.4856, Val Accuracy: 0.8311\nEpoch 11/100 - Loss: 0.0095, Val Loss: 0.4764, Val Accuracy: 0.8202\nEpoch 12/100 - Loss: 0.0093, Val Loss: 0.4819, Val Accuracy: 0.8256\nEpoch 13/100 - Loss: 0.0092, Val Loss: 0.4633, Val Accuracy: 0.8303\nEpoch 14/100 - Loss: 0.0091, Val Loss: 0.4575, Val Accuracy: 0.8283\nEpoch 15/100 - Loss: 0.0091, Val Loss: 0.4476, Val Accuracy: 0.8419\nEpoch 16/100 - Loss: 0.0091, Val Loss: 0.4482, Val Accuracy: 0.8422\nEpoch 17/100 - Loss: 0.0089, Val Loss: 0.4538, Val Accuracy: 0.8375\nEpoch 18/100 - Loss: 0.0090, Val Loss: 0.4364, Val Accuracy: 0.8424\nEpoch 19/100 - Loss: 0.0089, Val Loss: 0.4638, Val Accuracy: 0.8327\nEpoch 20/100 - Loss: 0.0089, Val Loss: 0.4531, Val Accuracy: 0.8388\nEpoch 21/100 - Loss: 0.0088, Val Loss: 0.4259, Val Accuracy: 0.8503\nEpoch 22/100 - Loss: 0.0088, Val Loss: 0.4318, Val Accuracy: 0.8502\nEpoch 23/100 - Loss: 0.0088, Val Loss: 0.4354, Val Accuracy: 0.8402\nEpoch 24/100 - Loss: 0.0087, Val Loss: 0.4456, Val Accuracy: 0.8425\nStopping early at epoch 25\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0180, Val Loss: 0.7047, Val Accuracy: 0.7449\nEpoch 2/100 - Loss: 0.0121, Val Loss: 0.5850, Val Accuracy: 0.7670\nEpoch 3/100 - Loss: 0.0112, Val Loss: 0.5353, Val Accuracy: 0.8126\nEpoch 4/100 - Loss: 0.0105, Val Loss: 0.5054, Val Accuracy: 0.8247\nEpoch 5/100 - Loss: 0.0100, Val Loss: 0.5010, Val Accuracy: 0.8250\nEpoch 6/100 - Loss: 0.0098, Val Loss: 0.5026, Val Accuracy: 0.8209\nEpoch 7/100 - Loss: 0.0098, Val Loss: 0.4767, Val Accuracy: 0.8270\nEpoch 8/100 - Loss: 0.0095, Val Loss: 0.4943, Val Accuracy: 0.8290\nEpoch 9/100 - Loss: 0.0093, Val Loss: 0.4559, Val Accuracy: 0.8340\nEpoch 10/100 - Loss: 0.0093, Val Loss: 0.4568, Val Accuracy: 0.8385\nEpoch 11/100 - Loss: 0.0092, Val Loss: 0.4523, Val Accuracy: 0.8361\nEpoch 12/100 - Loss: 0.0092, Val Loss: 0.4675, Val Accuracy: 0.8364\nEpoch 13/100 - Loss: 0.0092, Val Loss: 0.4494, Val Accuracy: 0.8427\nEpoch 14/100 - Loss: 0.0089, Val Loss: 0.4560, Val Accuracy: 0.8419\nEpoch 15/100 - Loss: 0.0090, Val Loss: 0.4678, Val Accuracy: 0.8383\nEpoch 16/100 - Loss: 0.0089, Val Loss: 0.4512, Val Accuracy: 0.8443\nStopping early at epoch 17\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0188, Val Loss: 0.6506, Val Accuracy: 0.7356\nEpoch 2/100 - Loss: 0.0123, Val Loss: 0.6610, Val Accuracy: 0.7343\nEpoch 3/100 - Loss: 0.0112, Val Loss: 0.5919, Val Accuracy: 0.7856\nEpoch 4/100 - Loss: 0.0107, Val Loss: 0.5375, Val Accuracy: 0.7927\nEpoch 5/100 - Loss: 0.0104, Val Loss: 0.5000, Val Accuracy: 0.8313\nEpoch 6/100 - Loss: 0.0100, Val Loss: 0.5263, Val Accuracy: 0.8089\nEpoch 7/100 - Loss: 0.0098, Val Loss: 0.4842, Val Accuracy: 0.8309\nEpoch 8/100 - Loss: 0.0097, Val Loss: 0.4689, Val Accuracy: 0.8343\nEpoch 9/100 - Loss: 0.0095, Val Loss: 0.4838, Val Accuracy: 0.8173\nEpoch 10/100 - Loss: 0.0093, Val Loss: 0.4604, Val Accuracy: 0.8421\nEpoch 11/100 - Loss: 0.0092, Val Loss: 0.4474, Val Accuracy: 0.8471\nEpoch 12/100 - Loss: 0.0092, Val Loss: 0.4432, Val Accuracy: 0.8411\nEpoch 13/100 - Loss: 0.0090, Val Loss: 0.4515, Val Accuracy: 0.8437\nEpoch 14/100 - Loss: 0.0089, Val Loss: 0.4485, Val Accuracy: 0.8464\nEpoch 15/100 - Loss: 0.0089, Val Loss: 0.4421, Val Accuracy: 0.8453\nEpoch 16/100 - Loss: 0.0089, Val Loss: 0.4485, Val Accuracy: 0.8485\nEpoch 17/100 - Loss: 0.0088, Val Loss: 0.4394, Val Accuracy: 0.8518\nEpoch 18/100 - Loss: 0.0087, Val Loss: 0.4254, Val Accuracy: 0.8537\nEpoch 19/100 - Loss: 0.0088, Val Loss: 0.4405, Val Accuracy: 0.8529\nEpoch 20/100 - Loss: 0.0088, Val Loss: 0.4372, Val Accuracy: 0.8555\nEpoch 21/100 - Loss: 0.0087, Val Loss: 0.4322, Val Accuracy: 0.8559\nStopping early at epoch 22\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.4509390001601361, Validation Accuracy = 0.8393333333333334\nFold 1: Validation Loss = 0.4266947881338444, Validation Accuracy = 0.8502666666666666\nFold 2: Validation Loss = 0.45053254492739414, Validation Accuracy = 0.8426666666666667\nFold 3: Validation Loss = 0.4256088764743602, Validation Accuracy = 0.8536666666666667\nAverage Validation Loss across folds: 0.4384438024239337\nAverage Validation Accuracy across folds: 0.8464833333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.6000000000000001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0206, Val Loss: 0.8309, Val Accuracy: 0.6485\nEpoch 2/100 - Loss: 0.0144, Val Loss: 0.6799, Val Accuracy: 0.7097\nEpoch 3/100 - Loss: 0.0130, Val Loss: 0.6096, Val Accuracy: 0.7553\nEpoch 4/100 - Loss: 0.0125, Val Loss: 0.6284, Val Accuracy: 0.7533\nEpoch 5/100 - Loss: 0.0122, Val Loss: 0.6094, Val Accuracy: 0.7585\nEpoch 6/100 - Loss: 0.0119, Val Loss: 0.5651, Val Accuracy: 0.7881\nEpoch 7/100 - Loss: 0.0116, Val Loss: 0.5602, Val Accuracy: 0.7869\nEpoch 8/100 - Loss: 0.0115, Val Loss: 0.5673, Val Accuracy: 0.7821\nEpoch 9/100 - Loss: 0.0114, Val Loss: 0.5620, Val Accuracy: 0.7922\nEpoch 10/100 - Loss: 0.0111, Val Loss: 0.5416, Val Accuracy: 0.7986\nEpoch 11/100 - Loss: 0.0111, Val Loss: 0.5427, Val Accuracy: 0.7989\nEpoch 12/100 - Loss: 0.0110, Val Loss: 0.5364, Val Accuracy: 0.7983\nEpoch 13/100 - Loss: 0.0108, Val Loss: 0.5371, Val Accuracy: 0.8026\nEpoch 14/100 - Loss: 0.0109, Val Loss: 0.5343, Val Accuracy: 0.7985\nEpoch 15/100 - Loss: 0.0108, Val Loss: 0.5261, Val Accuracy: 0.8040\nEpoch 16/100 - Loss: 0.0105, Val Loss: 0.5315, Val Accuracy: 0.8008\nEpoch 17/100 - Loss: 0.0106, Val Loss: 0.5217, Val Accuracy: 0.7863\nEpoch 18/100 - Loss: 0.0107, Val Loss: 0.5213, Val Accuracy: 0.8102\nEpoch 19/100 - Loss: 0.0106, Val Loss: 0.5198, Val Accuracy: 0.8067\nEpoch 20/100 - Loss: 0.0105, Val Loss: 0.5253, Val Accuracy: 0.8099\nEpoch 21/100 - Loss: 0.0105, Val Loss: 0.5271, Val Accuracy: 0.8094\nEpoch 22/100 - Loss: 0.0104, Val Loss: 0.5082, Val Accuracy: 0.8110\nEpoch 23/100 - Loss: 0.0107, Val Loss: 0.5120, Val Accuracy: 0.8059\nEpoch 24/100 - Loss: 0.0105, Val Loss: 0.5303, Val Accuracy: 0.8137\nEpoch 25/100 - Loss: 0.0105, Val Loss: 0.5070, Val Accuracy: 0.8093\nEpoch 26/100 - Loss: 0.0106, Val Loss: 0.5147, Val Accuracy: 0.8045\nEpoch 27/100 - Loss: 0.0105, Val Loss: 0.5158, Val Accuracy: 0.8101\nEpoch 28/100 - Loss: 0.0105, Val Loss: 0.5134, Val Accuracy: 0.7991\nEpoch 29/100 - Loss: 0.0104, Val Loss: 0.4957, Val Accuracy: 0.8118\nEpoch 30/100 - Loss: 0.0103, Val Loss: 0.4990, Val Accuracy: 0.8144\nEpoch 31/100 - Loss: 0.0104, Val Loss: 0.5024, Val Accuracy: 0.8124\nEpoch 32/100 - Loss: 0.0103, Val Loss: 0.4965, Val Accuracy: 0.8189\nStopping early at epoch 33\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0238, Val Loss: 1.0427, Val Accuracy: 0.5513\nEpoch 2/100 - Loss: 0.0161, Val Loss: 0.6829, Val Accuracy: 0.7221\nEpoch 3/100 - Loss: 0.0135, Val Loss: 0.6284, Val Accuracy: 0.7762\nEpoch 4/100 - Loss: 0.0126, Val Loss: 0.6264, Val Accuracy: 0.7439\nEpoch 5/100 - Loss: 0.0122, Val Loss: 0.5886, Val Accuracy: 0.7544\nEpoch 6/100 - Loss: 0.0119, Val Loss: 0.5634, Val Accuracy: 0.7909\nEpoch 7/100 - Loss: 0.0116, Val Loss: 0.5483, Val Accuracy: 0.7979\nEpoch 8/100 - Loss: 0.0113, Val Loss: 0.5473, Val Accuracy: 0.8033\nEpoch 9/100 - Loss: 0.0114, Val Loss: 0.5347, Val Accuracy: 0.8013\nEpoch 10/100 - Loss: 0.0113, Val Loss: 0.5528, Val Accuracy: 0.7848\nEpoch 11/100 - Loss: 0.0111, Val Loss: 0.5502, Val Accuracy: 0.8207\nEpoch 12/100 - Loss: 0.0108, Val Loss: 0.5297, Val Accuracy: 0.8053\nEpoch 13/100 - Loss: 0.0108, Val Loss: 0.5309, Val Accuracy: 0.8237\nEpoch 14/100 - Loss: 0.0109, Val Loss: 0.5074, Val Accuracy: 0.8137\nEpoch 15/100 - Loss: 0.0107, Val Loss: 0.5007, Val Accuracy: 0.8302\nEpoch 16/100 - Loss: 0.0107, Val Loss: 0.5362, Val Accuracy: 0.8161\nEpoch 17/100 - Loss: 0.0107, Val Loss: 0.5073, Val Accuracy: 0.8252\nEpoch 18/100 - Loss: 0.0106, Val Loss: 0.4737, Val Accuracy: 0.8337\nEpoch 19/100 - Loss: 0.0105, Val Loss: 0.4969, Val Accuracy: 0.8369\nEpoch 20/100 - Loss: 0.0104, Val Loss: 0.4878, Val Accuracy: 0.8333\nEpoch 21/100 - Loss: 0.0104, Val Loss: 0.4896, Val Accuracy: 0.8348\nStopping early at epoch 22\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0211, Val Loss: 0.7963, Val Accuracy: 0.6729\nEpoch 2/100 - Loss: 0.0141, Val Loss: 0.6835, Val Accuracy: 0.7205\nEpoch 3/100 - Loss: 0.0130, Val Loss: 0.6544, Val Accuracy: 0.7046\nEpoch 4/100 - Loss: 0.0124, Val Loss: 0.5929, Val Accuracy: 0.7814\nEpoch 5/100 - Loss: 0.0121, Val Loss: 0.5936, Val Accuracy: 0.7783\nEpoch 6/100 - Loss: 0.0118, Val Loss: 0.5560, Val Accuracy: 0.7951\nEpoch 7/100 - Loss: 0.0117, Val Loss: 0.5614, Val Accuracy: 0.7951\nEpoch 8/100 - Loss: 0.0114, Val Loss: 0.5542, Val Accuracy: 0.7953\nEpoch 9/100 - Loss: 0.0112, Val Loss: 0.5419, Val Accuracy: 0.8123\nEpoch 10/100 - Loss: 0.0111, Val Loss: 0.5420, Val Accuracy: 0.8124\nEpoch 11/100 - Loss: 0.0110, Val Loss: 0.5309, Val Accuracy: 0.8075\nEpoch 12/100 - Loss: 0.0109, Val Loss: 0.5235, Val Accuracy: 0.8225\nEpoch 13/100 - Loss: 0.0109, Val Loss: 0.5369, Val Accuracy: 0.8235\nEpoch 14/100 - Loss: 0.0107, Val Loss: 0.5590, Val Accuracy: 0.8273\nEpoch 15/100 - Loss: 0.0106, Val Loss: 0.5153, Val Accuracy: 0.8227\nEpoch 16/100 - Loss: 0.0106, Val Loss: 0.5166, Val Accuracy: 0.8286\nEpoch 17/100 - Loss: 0.0106, Val Loss: 0.5251, Val Accuracy: 0.8216\nEpoch 18/100 - Loss: 0.0106, Val Loss: 0.4969, Val Accuracy: 0.8341\nEpoch 19/100 - Loss: 0.0104, Val Loss: 0.5054, Val Accuracy: 0.8281\nEpoch 20/100 - Loss: 0.0106, Val Loss: 0.5107, Val Accuracy: 0.8280\nEpoch 21/100 - Loss: 0.0105, Val Loss: 0.4886, Val Accuracy: 0.8349\nEpoch 22/100 - Loss: 0.0104, Val Loss: 0.4930, Val Accuracy: 0.8355\nEpoch 23/100 - Loss: 0.0104, Val Loss: 0.5051, Val Accuracy: 0.8385\nEpoch 24/100 - Loss: 0.0104, Val Loss: 0.5021, Val Accuracy: 0.8331\nStopping early at epoch 25\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0216, Val Loss: 0.9356, Val Accuracy: 0.5539\nEpoch 2/100 - Loss: 0.0156, Val Loss: 0.6541, Val Accuracy: 0.7203\nEpoch 3/100 - Loss: 0.0131, Val Loss: 0.6356, Val Accuracy: 0.7345\nEpoch 4/100 - Loss: 0.0125, Val Loss: 0.6459, Val Accuracy: 0.7319\nEpoch 5/100 - Loss: 0.0123, Val Loss: 0.5959, Val Accuracy: 0.7525\nEpoch 6/100 - Loss: 0.0120, Val Loss: 0.5971, Val Accuracy: 0.7474\nEpoch 7/100 - Loss: 0.0117, Val Loss: 0.5904, Val Accuracy: 0.7826\nEpoch 8/100 - Loss: 0.0115, Val Loss: 0.5587, Val Accuracy: 0.7902\nEpoch 9/100 - Loss: 0.0113, Val Loss: 0.5696, Val Accuracy: 0.7881\nEpoch 10/100 - Loss: 0.0113, Val Loss: 0.5298, Val Accuracy: 0.7963\nEpoch 11/100 - Loss: 0.0111, Val Loss: 0.5337, Val Accuracy: 0.7977\nEpoch 12/100 - Loss: 0.0112, Val Loss: 0.5360, Val Accuracy: 0.7968\nEpoch 13/100 - Loss: 0.0109, Val Loss: 0.5521, Val Accuracy: 0.8041\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.4960966891430794, Validation Accuracy = 0.8118\nFold 1: Validation Loss = 0.4734361027149444, Validation Accuracy = 0.8336666666666667\nFold 2: Validation Loss = 0.48792572598508066, Validation Accuracy = 0.8348666666666666\nFold 3: Validation Loss = 0.5301355687861747, Validation Accuracy = 0.7963333333333333\nAverage Validation Loss across folds: 0.49689852165731985\nAverage Validation Accuracy across folds: 0.8191666666666666\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.7000000000000001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0260, Val Loss: 1.1420, Val Accuracy: 0.4323\nEpoch 2/100 - Loss: 0.0199, Val Loss: 1.0383, Val Accuracy: 0.5223\nEpoch 3/100 - Loss: 0.0183, Val Loss: 0.9313, Val Accuracy: 0.5486\nEpoch 4/100 - Loss: 0.0172, Val Loss: 0.8829, Val Accuracy: 0.5879\nEpoch 5/100 - Loss: 0.0166, Val Loss: 0.8435, Val Accuracy: 0.6369\nEpoch 6/100 - Loss: 0.0162, Val Loss: 0.7916, Val Accuracy: 0.6543\nEpoch 7/100 - Loss: 0.0156, Val Loss: 0.7629, Val Accuracy: 0.7153\nEpoch 8/100 - Loss: 0.0146, Val Loss: 0.6714, Val Accuracy: 0.7307\nEpoch 9/100 - Loss: 0.0145, Val Loss: 0.6608, Val Accuracy: 0.7333\nEpoch 10/100 - Loss: 0.0141, Val Loss: 0.6326, Val Accuracy: 0.7320\nEpoch 11/100 - Loss: 0.0139, Val Loss: 0.6345, Val Accuracy: 0.7464\nEpoch 12/100 - Loss: 0.0138, Val Loss: 0.6517, Val Accuracy: 0.7302\nEpoch 13/100 - Loss: 0.0138, Val Loss: 0.6389, Val Accuracy: 0.7396\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0275, Val Loss: 1.2305, Val Accuracy: 0.3511\nEpoch 2/100 - Loss: 0.0203, Val Loss: 1.0595, Val Accuracy: 0.4936\nEpoch 3/100 - Loss: 0.0188, Val Loss: 0.9954, Val Accuracy: 0.5453\nEpoch 4/100 - Loss: 0.0177, Val Loss: 0.9218, Val Accuracy: 0.5529\nEpoch 5/100 - Loss: 0.0170, Val Loss: 0.8921, Val Accuracy: 0.5527\nEpoch 6/100 - Loss: 0.0164, Val Loss: 0.8090, Val Accuracy: 0.6499\nEpoch 7/100 - Loss: 0.0158, Val Loss: 0.8094, Val Accuracy: 0.6721\nEpoch 8/100 - Loss: 0.0154, Val Loss: 0.7823, Val Accuracy: 0.6353\nEpoch 9/100 - Loss: 0.0153, Val Loss: 0.7653, Val Accuracy: 0.6476\nEpoch 10/100 - Loss: 0.0149, Val Loss: 0.7560, Val Accuracy: 0.6539\nEpoch 11/100 - Loss: 0.0148, Val Loss: 0.7349, Val Accuracy: 0.6915\nEpoch 12/100 - Loss: 0.0148, Val Loss: 0.7422, Val Accuracy: 0.6555\nEpoch 13/100 - Loss: 0.0146, Val Loss: 0.6835, Val Accuracy: 0.7179\nEpoch 14/100 - Loss: 0.0145, Val Loss: 0.6873, Val Accuracy: 0.7279\nEpoch 15/100 - Loss: 0.0143, Val Loss: 0.6511, Val Accuracy: 0.7258\nEpoch 16/100 - Loss: 0.0142, Val Loss: 0.6804, Val Accuracy: 0.7720\nEpoch 17/100 - Loss: 0.0140, Val Loss: 0.6545, Val Accuracy: 0.7797\nEpoch 18/100 - Loss: 0.0139, Val Loss: 0.6449, Val Accuracy: 0.7804\nEpoch 19/100 - Loss: 0.0139, Val Loss: 0.6448, Val Accuracy: 0.7435\nEpoch 20/100 - Loss: 0.0138, Val Loss: 0.6171, Val Accuracy: 0.7743\nEpoch 21/100 - Loss: 0.0138, Val Loss: 0.6491, Val Accuracy: 0.7389\nEpoch 22/100 - Loss: 0.0138, Val Loss: 0.6361, Val Accuracy: 0.7647\nEpoch 23/100 - Loss: 0.0138, Val Loss: 0.6344, Val Accuracy: 0.7492\nStopping early at epoch 24\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0262, Val Loss: 1.1150, Val Accuracy: 0.4625\nEpoch 2/100 - Loss: 0.0197, Val Loss: 0.9466, Val Accuracy: 0.5851\nEpoch 3/100 - Loss: 0.0178, Val Loss: 0.8603, Val Accuracy: 0.6177\nEpoch 4/100 - Loss: 0.0168, Val Loss: 0.7914, Val Accuracy: 0.6750\nEpoch 5/100 - Loss: 0.0161, Val Loss: 0.8027, Val Accuracy: 0.6830\nEpoch 6/100 - Loss: 0.0156, Val Loss: 0.7118, Val Accuracy: 0.7099\nEpoch 7/100 - Loss: 0.0149, Val Loss: 0.6860, Val Accuracy: 0.7276\nEpoch 8/100 - Loss: 0.0146, Val Loss: 0.6524, Val Accuracy: 0.7403\nEpoch 9/100 - Loss: 0.0143, Val Loss: 0.6488, Val Accuracy: 0.7371\nEpoch 10/100 - Loss: 0.0140, Val Loss: 0.6323, Val Accuracy: 0.7563\nEpoch 11/100 - Loss: 0.0140, Val Loss: 0.6465, Val Accuracy: 0.6934\nEpoch 12/100 - Loss: 0.0137, Val Loss: 0.6302, Val Accuracy: 0.7175\nEpoch 13/100 - Loss: 0.0138, Val Loss: 0.6207, Val Accuracy: 0.7446\nEpoch 14/100 - Loss: 0.0137, Val Loss: 0.6163, Val Accuracy: 0.7535\nEpoch 15/100 - Loss: 0.0136, Val Loss: 0.6199, Val Accuracy: 0.7473\nEpoch 16/100 - Loss: 0.0137, Val Loss: 0.6178, Val Accuracy: 0.7413\nEpoch 17/100 - Loss: 0.0136, Val Loss: 0.6273, Val Accuracy: 0.7349\nStopping early at epoch 18\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0275, Val Loss: 1.3070, Val Accuracy: 0.3988\nEpoch 2/100 - Loss: 0.0202, Val Loss: 0.9809, Val Accuracy: 0.5804\nEpoch 3/100 - Loss: 0.0179, Val Loss: 0.8524, Val Accuracy: 0.6129\nEpoch 4/100 - Loss: 0.0165, Val Loss: 0.7646, Val Accuracy: 0.6907\nEpoch 5/100 - Loss: 0.0156, Val Loss: 0.7166, Val Accuracy: 0.7151\nEpoch 6/100 - Loss: 0.0149, Val Loss: 0.6970, Val Accuracy: 0.7266\nEpoch 7/100 - Loss: 0.0146, Val Loss: 0.6853, Val Accuracy: 0.7218\nEpoch 8/100 - Loss: 0.0140, Val Loss: 0.6594, Val Accuracy: 0.7401\nEpoch 9/100 - Loss: 0.0141, Val Loss: 0.6877, Val Accuracy: 0.7155\nEpoch 10/100 - Loss: 0.0140, Val Loss: 0.6628, Val Accuracy: 0.7270\nEpoch 11/100 - Loss: 0.0139, Val Loss: 0.6611, Val Accuracy: 0.7376\nEpoch 12/100 - Loss: 0.0138, Val Loss: 0.6569, Val Accuracy: 0.7386\nEpoch 13/100 - Loss: 0.0136, Val Loss: 0.6666, Val Accuracy: 0.7321\nEpoch 14/100 - Loss: 0.0135, Val Loss: 0.6526, Val Accuracy: 0.7351\nEpoch 15/100 - Loss: 0.0135, Val Loss: 0.6422, Val Accuracy: 0.7425\nEpoch 16/100 - Loss: 0.0135, Val Loss: 0.6343, Val Accuracy: 0.7369\nEpoch 17/100 - Loss: 0.0133, Val Loss: 0.6380, Val Accuracy: 0.7431\nEpoch 18/100 - Loss: 0.0134, Val Loss: 0.6429, Val Accuracy: 0.7346\nEpoch 19/100 - Loss: 0.0135, Val Loss: 0.6412, Val Accuracy: 0.7413\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.6331945125092852, Validation Accuracy = 0.732\nFold 1: Validation Loss = 0.6169780119936517, Validation Accuracy = 0.7743333333333333\nFold 2: Validation Loss = 0.6168175093671109, Validation Accuracy = 0.7535333333333334\nFold 3: Validation Loss = 0.6341009737329281, Validation Accuracy = 0.7368666666666667\nAverage Validation Loss across folds: 0.6252727519007439\nAverage Validation Accuracy across folds: 0.7491833333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.8\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0307, Val Loss: 1.6540, Val Accuracy: 0.2332\nEpoch 2/100 - Loss: 0.0264, Val Loss: 1.5284, Val Accuracy: 0.2943\nEpoch 3/100 - Loss: 0.0253, Val Loss: 1.4759, Val Accuracy: 0.2929\nEpoch 4/100 - Loss: 0.0248, Val Loss: 1.3515, Val Accuracy: 0.3939\nEpoch 5/100 - Loss: 0.0231, Val Loss: 1.2401, Val Accuracy: 0.3823\nEpoch 6/100 - Loss: 0.0222, Val Loss: 1.2572, Val Accuracy: 0.3895\nEpoch 7/100 - Loss: 0.0221, Val Loss: 1.2167, Val Accuracy: 0.3889\nEpoch 8/100 - Loss: 0.0219, Val Loss: 1.2111, Val Accuracy: 0.4237\nEpoch 9/100 - Loss: 0.0219, Val Loss: 1.2083, Val Accuracy: 0.4037\nEpoch 10/100 - Loss: 0.0216, Val Loss: 1.2011, Val Accuracy: 0.4153\nEpoch 11/100 - Loss: 0.0215, Val Loss: 1.1991, Val Accuracy: 0.3911\nEpoch 12/100 - Loss: 0.0214, Val Loss: 1.1966, Val Accuracy: 0.4654\nEpoch 13/100 - Loss: 0.0212, Val Loss: 1.1660, Val Accuracy: 0.4833\nEpoch 14/100 - Loss: 0.0210, Val Loss: 1.1615, Val Accuracy: 0.4685\nEpoch 15/100 - Loss: 0.0217, Val Loss: 1.1540, Val Accuracy: 0.4733\nEpoch 16/100 - Loss: 0.0208, Val Loss: 1.1322, Val Accuracy: 0.4733\nEpoch 17/100 - Loss: 0.0208, Val Loss: 1.1165, Val Accuracy: 0.4819\nEpoch 18/100 - Loss: 0.0206, Val Loss: 1.1336, Val Accuracy: 0.4611\nEpoch 19/100 - Loss: 0.0207, Val Loss: 1.1168, Val Accuracy: 0.4564\nEpoch 20/100 - Loss: 0.0206, Val Loss: 1.1375, Val Accuracy: 0.4676\nStopping early at epoch 21\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0332, Val Loss: 1.7106, Val Accuracy: 0.2039\nEpoch 2/100 - Loss: 0.0275, Val Loss: 1.4895, Val Accuracy: 0.2892\nEpoch 3/100 - Loss: 0.0255, Val Loss: 1.2730, Val Accuracy: 0.3871\nEpoch 4/100 - Loss: 0.0232, Val Loss: 1.2180, Val Accuracy: 0.3715\nEpoch 5/100 - Loss: 0.0223, Val Loss: 1.1787, Val Accuracy: 0.4028\nEpoch 6/100 - Loss: 0.0218, Val Loss: 1.1421, Val Accuracy: 0.4337\nEpoch 7/100 - Loss: 0.0211, Val Loss: 1.0887, Val Accuracy: 0.4674\nEpoch 8/100 - Loss: 0.0208, Val Loss: 1.0942, Val Accuracy: 0.4699\nEpoch 9/100 - Loss: 0.0206, Val Loss: 1.0894, Val Accuracy: 0.5023\nEpoch 10/100 - Loss: 0.0205, Val Loss: 1.1441, Val Accuracy: 0.4671\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0359, Val Loss: 1.8481, Val Accuracy: 0.2003\nEpoch 2/100 - Loss: 0.0276, Val Loss: 1.5267, Val Accuracy: 0.2979\nEpoch 3/100 - Loss: 0.0253, Val Loss: 1.4785, Val Accuracy: 0.3244\nEpoch 4/100 - Loss: 0.0244, Val Loss: 1.4147, Val Accuracy: 0.3703\nEpoch 5/100 - Loss: 0.0240, Val Loss: 1.4077, Val Accuracy: 0.3763\nEpoch 6/100 - Loss: 0.0238, Val Loss: 1.3783, Val Accuracy: 0.4177\nEpoch 7/100 - Loss: 0.0233, Val Loss: 1.2397, Val Accuracy: 0.4588\nEpoch 8/100 - Loss: 0.0223, Val Loss: 1.1800, Val Accuracy: 0.4506\nEpoch 9/100 - Loss: 0.0218, Val Loss: 1.1090, Val Accuracy: 0.5086\nEpoch 10/100 - Loss: 0.0213, Val Loss: 1.0703, Val Accuracy: 0.5340\nEpoch 11/100 - Loss: 0.0210, Val Loss: 1.0734, Val Accuracy: 0.5399\nEpoch 12/100 - Loss: 0.0209, Val Loss: 1.0547, Val Accuracy: 0.5553\nEpoch 13/100 - Loss: 0.0208, Val Loss: 1.0393, Val Accuracy: 0.5550\nEpoch 14/100 - Loss: 0.0203, Val Loss: 1.0234, Val Accuracy: 0.5852\nEpoch 15/100 - Loss: 0.0203, Val Loss: 1.0191, Val Accuracy: 0.5543\nEpoch 16/100 - Loss: 0.0201, Val Loss: 1.0264, Val Accuracy: 0.5573\nEpoch 17/100 - Loss: 0.0200, Val Loss: 1.0010, Val Accuracy: 0.5939\nEpoch 18/100 - Loss: 0.0198, Val Loss: 1.0145, Val Accuracy: 0.5493\nEpoch 19/100 - Loss: 0.0198, Val Loss: 0.9956, Val Accuracy: 0.5603\nEpoch 20/100 - Loss: 0.0197, Val Loss: 0.9581, Val Accuracy: 0.5685\nEpoch 21/100 - Loss: 0.0195, Val Loss: 0.9684, Val Accuracy: 0.5547\nEpoch 22/100 - Loss: 0.0193, Val Loss: 0.9436, Val Accuracy: 0.5665\nEpoch 23/100 - Loss: 0.0196, Val Loss: 0.9408, Val Accuracy: 0.5735\nEpoch 24/100 - Loss: 0.0194, Val Loss: 0.9579, Val Accuracy: 0.6063\nEpoch 25/100 - Loss: 0.0193, Val Loss: 0.9497, Val Accuracy: 0.5759\nEpoch 26/100 - Loss: 0.0199, Val Loss: 0.9272, Val Accuracy: 0.5573\nEpoch 27/100 - Loss: 0.0193, Val Loss: 0.9357, Val Accuracy: 0.5655\nEpoch 28/100 - Loss: 0.0192, Val Loss: 0.9495, Val Accuracy: 0.6099\nEpoch 29/100 - Loss: 0.0197, Val Loss: 0.9278, Val Accuracy: 0.5522\nStopping early at epoch 30\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0310, Val Loss: 1.5845, Val Accuracy: 0.2816\nEpoch 2/100 - Loss: 0.0261, Val Loss: 1.4850, Val Accuracy: 0.2979\nEpoch 3/100 - Loss: 0.0243, Val Loss: 1.2504, Val Accuracy: 0.3813\nEpoch 4/100 - Loss: 0.0227, Val Loss: 1.2267, Val Accuracy: 0.3814\nEpoch 5/100 - Loss: 0.0222, Val Loss: 1.2396, Val Accuracy: 0.3820\nEpoch 6/100 - Loss: 0.0219, Val Loss: 1.2430, Val Accuracy: 0.3891\nEpoch 7/100 - Loss: 0.0218, Val Loss: 1.2083, Val Accuracy: 0.3855\nEpoch 8/100 - Loss: 0.0217, Val Loss: 1.1930, Val Accuracy: 0.3881\nEpoch 9/100 - Loss: 0.0216, Val Loss: 1.1814, Val Accuracy: 0.4075\nEpoch 10/100 - Loss: 0.0212, Val Loss: 1.2066, Val Accuracy: 0.3877\nEpoch 11/100 - Loss: 0.0214, Val Loss: 1.1777, Val Accuracy: 0.3889\nEpoch 12/100 - Loss: 0.0213, Val Loss: 1.1646, Val Accuracy: 0.4226\nEpoch 13/100 - Loss: 0.0212, Val Loss: 1.1123, Val Accuracy: 0.4793\nEpoch 14/100 - Loss: 0.0210, Val Loss: 1.1011, Val Accuracy: 0.4733\nEpoch 15/100 - Loss: 0.0207, Val Loss: 1.1115, Val Accuracy: 0.5345\nEpoch 16/100 - Loss: 0.0205, Val Loss: 1.1119, Val Accuracy: 0.4851\nEpoch 17/100 - Loss: 0.0206, Val Loss: 1.0813, Val Accuracy: 0.4643\nEpoch 18/100 - Loss: 0.0204, Val Loss: 1.0627, Val Accuracy: 0.4815\nEpoch 19/100 - Loss: 0.0203, Val Loss: 1.1073, Val Accuracy: 0.4609\nEpoch 20/100 - Loss: 0.0204, Val Loss: 1.0307, Val Accuracy: 0.5469\nEpoch 21/100 - Loss: 0.0201, Val Loss: 1.0035, Val Accuracy: 0.5457\nEpoch 22/100 - Loss: 0.0200, Val Loss: 0.9902, Val Accuracy: 0.5627\nEpoch 23/100 - Loss: 0.0198, Val Loss: 0.9980, Val Accuracy: 0.5593\nEpoch 24/100 - Loss: 0.0198, Val Loss: 0.9803, Val Accuracy: 0.5489\nEpoch 25/100 - Loss: 0.0199, Val Loss: 0.9659, Val Accuracy: 0.5695\nEpoch 26/100 - Loss: 0.0199, Val Loss: 0.9451, Val Accuracy: 0.5709\nEpoch 27/100 - Loss: 0.0197, Val Loss: 0.9809, Val Accuracy: 0.5596\nEpoch 28/100 - Loss: 0.0196, Val Loss: 0.9791, Val Accuracy: 0.5423\nEpoch 29/100 - Loss: 0.0195, Val Loss: 0.9938, Val Accuracy: 0.5706\nStopping early at epoch 30\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.116874812004414, Validation Accuracy = 0.4819333333333333\nFold 1: Validation Loss = 1.0887298857912104, Validation Accuracy = 0.4674\nFold 2: Validation Loss = 0.9241859925554153, Validation Accuracy = 0.5572666666666667\nFold 3: Validation Loss = 0.9458217945504696, Validation Accuracy = 0.5708666666666666\nAverage Validation Loss across folds: 1.0189031212253774\nAverage Validation Accuracy across folds: 0.5193666666666668\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.9\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0367, Val Loss: 2.3031, Val Accuracy: 0.0993\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3048, Val Accuracy: 0.0998\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3040, Val Accuracy: 0.0993\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3056, Val Accuracy: 0.0998\nEpoch 5/100 - Loss: 0.0363, Val Loss: 2.3031, Val Accuracy: 0.0993\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3031, Val Accuracy: 0.1019\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3049, Val Accuracy: 0.0961\nEpoch 8/100 - Loss: 0.0363, Val Loss: 2.3036, Val Accuracy: 0.0997\nEpoch 9/100 - Loss: 0.0363, Val Loss: 2.3035, Val Accuracy: 0.0998\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0367, Val Loss: 2.3054, Val Accuracy: 0.0975\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3032, Val Accuracy: 0.1026\nEpoch 3/100 - Loss: 0.0363, Val Loss: 2.3037, Val Accuracy: 0.0989\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3044, Val Accuracy: 0.1005\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3027, Val Accuracy: 0.1026\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3047, Val Accuracy: 0.0975\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3037, Val Accuracy: 0.0971\nEpoch 8/100 - Loss: 0.0362, Val Loss: 2.3033, Val Accuracy: 0.0997\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0367, Val Loss: 2.3036, Val Accuracy: 0.0979\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3038, Val Accuracy: 0.1013\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3042, Val Accuracy: 0.0993\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3030, Val Accuracy: 0.1018\nEpoch 5/100 - Loss: 0.0363, Val Loss: 2.3033, Val Accuracy: 0.1018\nEpoch 6/100 - Loss: 0.0363, Val Loss: 2.3034, Val Accuracy: 0.0978\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3030, Val Accuracy: 0.0978\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0367, Val Loss: 2.3040, Val Accuracy: 0.0948\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3031, Val Accuracy: 0.1025\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3032, Val Accuracy: 0.1015\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3037, Val Accuracy: 0.0991\nEpoch 5/100 - Loss: 0.0363, Val Loss: 2.3035, Val Accuracy: 0.1003\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3030511450260245, Validation Accuracy = 0.10193333333333333\nFold 1: Validation Loss = 2.3027690126540814, Validation Accuracy = 0.1026\nFold 2: Validation Loss = 2.3029771409136184, Validation Accuracy = 0.1018\nFold 3: Validation Loss = 2.3031324701106293, Validation Accuracy = 0.10246666666666666\nAverage Validation Loss across folds: 2.3029824421760883\nAverage Validation Accuracy across folds: 0.1022\n--------------------------------\nK-fold Cross-validation completed.\n   dropout_rate  train_loss  val_loss  val_accuracy\n0           0.0    0.399432  0.859883      0.006611\n1           0.1    0.422701  0.848567      0.007496\n2           0.2    0.383377  0.857933      0.007475\n3           0.3    0.408441  0.851150      0.008318\n4           0.4    0.412833  0.852450      0.008976\n5           0.5    0.438444  0.846483      0.010086\n6           0.6    0.496899  0.819167      0.011739\n7           0.7    0.625273  0.749183      0.015652\n8           0.8    1.018903  0.519367      0.021895\n9           0.9    2.302982  0.102200      0.036298\n\n\n\n# rename columns as they were ordered wrong (fixing)\nresults_choice_2.rename(columns={'train_loss': 'average_val_loss'}, inplace=True)\nresults_choice_2.rename(columns={'val_loss': 'average_val_accuracy'}, inplace=True)\nresults_choice_2.rename(columns={'val_accuracy': 'average_train_loss'}, inplace=True)\n\n# save results_df2 to csv\nresults_choice_2.to_csv('results/results_choice_2.csv')\n\n\n# display results\nresults_choice_2\n\n\n\n\n\n\n\n\ndropout_rate\naverage_val_loss\naverage_val_accuracy\naverage_train_loss\n\n\n\n\n0\n0.0\n0.399432\n0.859883\n0.006611\n\n\n1\n0.1\n0.422701\n0.848567\n0.007496\n\n\n2\n0.2\n0.383377\n0.857933\n0.007475\n\n\n3\n0.3\n0.408441\n0.851150\n0.008318\n\n\n4\n0.4\n0.412833\n0.852450\n0.008976\n\n\n5\n0.5\n0.438444\n0.846483\n0.010086\n\n\n6\n0.6\n0.496899\n0.819167\n0.011739\n\n\n7\n0.7\n0.625273\n0.749183\n0.015652\n\n\n8\n0.8\n1.018903\n0.519367\n0.021895\n\n\n9\n0.9\n2.302982\n0.102200\n0.036298\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nresults_choice_2 = pd.read_csv('results/results_choice_2.csv')\n\n# Plot\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(results_choice_2['dropout_rate'], results_choice_2['average_train_loss'], label='Training Loss', marker='o', color='green')\nplt.plot(results_choice_2['dropout_rate'], results_choice_2['average_val_loss'], label='Validation Loss', marker='o', color='red')\nplt.title('Training and Validation Loss vs. Dropout Rate')\nplt.xlabel('Dropout Rate')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot\nplt.subplot(1, 2, 2)\nerror_ratio = results_choice_2['average_val_loss'] / results_choice_2['average_train_loss']\nplt.plot(results_choice_2['dropout_rate'], error_ratio, label='Val/Train Loss Ratio', marker='o')\nplt.title('Error Ratio vs. Dropout Rate')\nplt.xlabel('Dropout Rate')\nplt.ylabel('Ratio')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nfor hidden activation=relu, hidden layers = [96,96,96], optimizer=adam\nVary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization)\nPlot training and validation as a function of dropout rate\n\nPlot the ratio (or difference) of validation to training error as a function of dropout rate\n\n\ndef model_fn3(dropout_rate=0.0):\n    return NeuralNetwork(input_size=28*28, layer_sizes=[96, 96, 96], activation_funcs=['relu', 'relu', 'relu'], dropout_rate=dropout_rate)\n\ndropout_rates = np.linspace(0.0, 0.9, 10) \n\nresults3 = []\n\nfor dropout_rate in dropout_rates:\n    print(f\"Training with Dropout Rate: {dropout_rate}\")\n    avg_train_loss, avg_val_loss, avg_val_accuracy = training_model_kfold(\n        k_folds=2,\n        num_epochs=100,\n        dataset=training_data,\n        model_fn=lambda: model_fn3(dropout_rate=dropout_rate),\n        loss_fn=nn.CrossEntropyLoss(),\n        optimizer_name=\"Adam\",\n        lr=0.01, \n        l1_reg=0,   \n        l2_reg=0,\n        device=device,\n        patience=4,\n        plot=False)\n    \n    results3.append({\n        'dropout_rate': dropout_rate,\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n        'val_accuracy': avg_val_accuracy})\n\nresults_choice_3 = pd.DataFrame(results3)\nprint(results_choice_3)\n\nTraining with Dropout Rate: 0.0\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0098, Val Loss: 0.5491, Val Accuracy: 0.8206\nEpoch 2/100 - Loss: 0.0070, Val Loss: 0.4296, Val Accuracy: 0.8517\nEpoch 3/100 - Loss: 0.0065, Val Loss: 0.4732, Val Accuracy: 0.8323\nEpoch 4/100 - Loss: 0.0062, Val Loss: 0.5201, Val Accuracy: 0.8206\nEpoch 5/100 - Loss: 0.0060, Val Loss: 0.4274, Val Accuracy: 0.8539\nEpoch 6/100 - Loss: 0.0059, Val Loss: 0.4200, Val Accuracy: 0.8551\nEpoch 7/100 - Loss: 0.0057, Val Loss: 0.4063, Val Accuracy: 0.8614\nEpoch 8/100 - Loss: 0.0057, Val Loss: 0.4160, Val Accuracy: 0.8585\nEpoch 9/100 - Loss: 0.0056, Val Loss: 0.3955, Val Accuracy: 0.8626\nEpoch 10/100 - Loss: 0.0057, Val Loss: 0.4452, Val Accuracy: 0.8545\nEpoch 11/100 - Loss: 0.0053, Val Loss: 0.4184, Val Accuracy: 0.8596\nEpoch 12/100 - Loss: 0.0053, Val Loss: 0.4096, Val Accuracy: 0.8604\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0098, Val Loss: 0.4839, Val Accuracy: 0.8247\nEpoch 2/100 - Loss: 0.0073, Val Loss: 0.4649, Val Accuracy: 0.8188\nEpoch 3/100 - Loss: 0.0067, Val Loss: 0.4727, Val Accuracy: 0.8407\nEpoch 4/100 - Loss: 0.0063, Val Loss: 0.4230, Val Accuracy: 0.8535\nEpoch 5/100 - Loss: 0.0064, Val Loss: 0.4133, Val Accuracy: 0.8573\nEpoch 6/100 - Loss: 0.0062, Val Loss: 0.4242, Val Accuracy: 0.8513\nEpoch 7/100 - Loss: 0.0061, Val Loss: 0.4338, Val Accuracy: 0.8493\nEpoch 8/100 - Loss: 0.0061, Val Loss: 0.4527, Val Accuracy: 0.8339\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.39545452480377163, Validation Accuracy = 0.8626\nFold 1: Validation Loss = 0.4133657828005138, Validation Accuracy = 0.8573333333333333\nAverage Validation Loss across folds: 0.40441015380214274\nAverage Validation Accuracy across folds: 0.8599666666666667\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0108, Val Loss: 0.5447, Val Accuracy: 0.8124\nEpoch 2/100 - Loss: 0.0087, Val Loss: 0.5180, Val Accuracy: 0.8059\nEpoch 3/100 - Loss: 0.0083, Val Loss: 0.6249, Val Accuracy: 0.7878\nEpoch 4/100 - Loss: 0.0081, Val Loss: 0.4612, Val Accuracy: 0.8344\nEpoch 5/100 - Loss: 0.0079, Val Loss: 0.4898, Val Accuracy: 0.8184\nEpoch 6/100 - Loss: 0.0077, Val Loss: 0.4968, Val Accuracy: 0.8356\nEpoch 7/100 - Loss: 0.0077, Val Loss: 0.4967, Val Accuracy: 0.8346\nEpoch 8/100 - Loss: 0.0075, Val Loss: 0.4430, Val Accuracy: 0.8398\nEpoch 9/100 - Loss: 0.0074, Val Loss: 0.4704, Val Accuracy: 0.8399\nEpoch 10/100 - Loss: 0.0076, Val Loss: 0.4747, Val Accuracy: 0.8393\nEpoch 11/100 - Loss: 0.0076, Val Loss: 0.4762, Val Accuracy: 0.8389\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0112, Val Loss: 0.5145, Val Accuracy: 0.8150\nEpoch 2/100 - Loss: 0.0089, Val Loss: 0.6359, Val Accuracy: 0.8161\nEpoch 3/100 - Loss: 0.0087, Val Loss: 0.5082, Val Accuracy: 0.8140\nEpoch 4/100 - Loss: 0.0087, Val Loss: 0.4684, Val Accuracy: 0.8407\nEpoch 5/100 - Loss: 0.0083, Val Loss: 0.4925, Val Accuracy: 0.8277\nEpoch 6/100 - Loss: 0.0082, Val Loss: 0.4487, Val Accuracy: 0.8458\nEpoch 7/100 - Loss: 0.0077, Val Loss: 0.4398, Val Accuracy: 0.8493\nEpoch 8/100 - Loss: 0.0080, Val Loss: 0.4338, Val Accuracy: 0.8541\nEpoch 9/100 - Loss: 0.0078, Val Loss: 0.4431, Val Accuracy: 0.8476\nEpoch 10/100 - Loss: 0.0077, Val Loss: 0.4262, Val Accuracy: 0.8525\nEpoch 11/100 - Loss: 0.0077, Val Loss: 0.4548, Val Accuracy: 0.8427\nEpoch 12/100 - Loss: 0.0080, Val Loss: 0.4893, Val Accuracy: 0.8350\nEpoch 13/100 - Loss: 0.0076, Val Loss: 0.4807, Val Accuracy: 0.8342\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.44300877285410345, Validation Accuracy = 0.8397666666666667\nFold 1: Validation Loss = 0.4261147111400104, Validation Accuracy = 0.8525\nAverage Validation Loss across folds: 0.43456174199705694\nAverage Validation Accuracy across folds: 0.8461333333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.2\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0121, Val Loss: 0.5563, Val Accuracy: 0.7988\nEpoch 2/100 - Loss: 0.0101, Val Loss: 0.5211, Val Accuracy: 0.8208\nEpoch 3/100 - Loss: 0.0099, Val Loss: 0.4837, Val Accuracy: 0.8216\nEpoch 4/100 - Loss: 0.0095, Val Loss: 0.4971, Val Accuracy: 0.8363\nEpoch 5/100 - Loss: 0.0093, Val Loss: 0.4973, Val Accuracy: 0.8338\nEpoch 6/100 - Loss: 0.0093, Val Loss: 0.5189, Val Accuracy: 0.8233\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0123, Val Loss: 0.5502, Val Accuracy: 0.8066\nEpoch 2/100 - Loss: 0.0103, Val Loss: 0.5314, Val Accuracy: 0.8066\nEpoch 3/100 - Loss: 0.0097, Val Loss: 0.5155, Val Accuracy: 0.8196\nEpoch 4/100 - Loss: 0.0101, Val Loss: 0.4947, Val Accuracy: 0.8238\nEpoch 5/100 - Loss: 0.0097, Val Loss: 0.4686, Val Accuracy: 0.8373\nEpoch 6/100 - Loss: 0.0093, Val Loss: 0.5167, Val Accuracy: 0.8300\nEpoch 7/100 - Loss: 0.0095, Val Loss: 0.5008, Val Accuracy: 0.8208\nEpoch 8/100 - Loss: 0.0093, Val Loss: 0.5024, Val Accuracy: 0.8314\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.4835835076662015, Validation Accuracy = 0.8215666666666667\nFold 1: Validation Loss = 0.4686416948019569, Validation Accuracy = 0.8373333333333334\nAverage Validation Loss across folds: 0.47611260123407917\nAverage Validation Accuracy across folds: 0.82945\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.30000000000000004\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0141, Val Loss: 0.6420, Val Accuracy: 0.7581\nEpoch 2/100 - Loss: 0.0120, Val Loss: 0.5877, Val Accuracy: 0.7855\nEpoch 3/100 - Loss: 0.0119, Val Loss: 0.5830, Val Accuracy: 0.7896\nEpoch 4/100 - Loss: 0.0123, Val Loss: 0.6106, Val Accuracy: 0.7738\nEpoch 5/100 - Loss: 0.0121, Val Loss: 0.5829, Val Accuracy: 0.7804\nEpoch 6/100 - Loss: 0.0119, Val Loss: 0.5796, Val Accuracy: 0.7982\nEpoch 7/100 - Loss: 0.0119, Val Loss: 0.5898, Val Accuracy: 0.7951\nEpoch 8/100 - Loss: 0.0116, Val Loss: 0.5699, Val Accuracy: 0.7919\nEpoch 9/100 - Loss: 0.0115, Val Loss: 0.6045, Val Accuracy: 0.7967\nEpoch 10/100 - Loss: 0.0121, Val Loss: 0.5799, Val Accuracy: 0.7992\nEpoch 11/100 - Loss: 0.0119, Val Loss: 0.5467, Val Accuracy: 0.8078\nEpoch 12/100 - Loss: 0.0120, Val Loss: 0.5934, Val Accuracy: 0.7433\nEpoch 13/100 - Loss: 0.0118, Val Loss: 0.5665, Val Accuracy: 0.8012\nEpoch 14/100 - Loss: 0.0113, Val Loss: 0.6123, Val Accuracy: 0.8027\nStopping early at epoch 15\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0137, Val Loss: 0.6281, Val Accuracy: 0.7791\nEpoch 2/100 - Loss: 0.0123, Val Loss: 0.6268, Val Accuracy: 0.7585\nEpoch 3/100 - Loss: 0.0118, Val Loss: 0.5643, Val Accuracy: 0.8005\nEpoch 4/100 - Loss: 0.0116, Val Loss: 0.5418, Val Accuracy: 0.7992\nEpoch 5/100 - Loss: 0.0118, Val Loss: 0.6143, Val Accuracy: 0.7992\nEpoch 6/100 - Loss: 0.0120, Val Loss: 0.5568, Val Accuracy: 0.8107\nEpoch 7/100 - Loss: 0.0115, Val Loss: 0.5466, Val Accuracy: 0.7926\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.5467079385384314, Validation Accuracy = 0.8078\nFold 1: Validation Loss = 0.5418768748164431, Validation Accuracy = 0.7991666666666667\nAverage Validation Loss across folds: 0.5442924066774373\nAverage Validation Accuracy across folds: 0.8034833333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.4\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0156, Val Loss: 0.6666, Val Accuracy: 0.7227\nEpoch 2/100 - Loss: 0.0142, Val Loss: 0.6869, Val Accuracy: 0.7155\nEpoch 3/100 - Loss: 0.0144, Val Loss: 0.6780, Val Accuracy: 0.7094\nEpoch 4/100 - Loss: 0.0141, Val Loss: 0.6626, Val Accuracy: 0.7483\nEpoch 5/100 - Loss: 0.0141, Val Loss: 0.6199, Val Accuracy: 0.7611\nEpoch 6/100 - Loss: 0.0142, Val Loss: 0.6648, Val Accuracy: 0.7348\nEpoch 7/100 - Loss: 0.0141, Val Loss: 0.6324, Val Accuracy: 0.7224\nEpoch 8/100 - Loss: 0.0142, Val Loss: 0.6649, Val Accuracy: 0.7485\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0161, Val Loss: 0.6815, Val Accuracy: 0.7331\nEpoch 2/100 - Loss: 0.0144, Val Loss: 0.6863, Val Accuracy: 0.6918\nEpoch 3/100 - Loss: 0.0145, Val Loss: 0.7126, Val Accuracy: 0.7145\nEpoch 4/100 - Loss: 0.0146, Val Loss: 0.6817, Val Accuracy: 0.7148\nEpoch 5/100 - Loss: 0.0141, Val Loss: 0.6263, Val Accuracy: 0.7741\nEpoch 6/100 - Loss: 0.0152, Val Loss: 0.6770, Val Accuracy: 0.7164\nEpoch 7/100 - Loss: 0.0148, Val Loss: 0.6736, Val Accuracy: 0.7492\nEpoch 8/100 - Loss: 0.0142, Val Loss: 0.6564, Val Accuracy: 0.7215\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.6199763058853556, Validation Accuracy = 0.7610666666666667\nFold 1: Validation Loss = 0.6263655792675547, Validation Accuracy = 0.7741333333333333\nAverage Validation Loss across folds: 0.6231709425764551\nAverage Validation Accuracy across folds: 0.7676000000000001\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.5\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0189, Val Loss: 0.7519, Val Accuracy: 0.7015\nEpoch 2/100 - Loss: 0.0177, Val Loss: 0.7277, Val Accuracy: 0.6994\nEpoch 3/100 - Loss: 0.0176, Val Loss: 0.7176, Val Accuracy: 0.7089\nEpoch 4/100 - Loss: 0.0179, Val Loss: 0.7189, Val Accuracy: 0.7266\nEpoch 5/100 - Loss: 0.0180, Val Loss: 0.7946, Val Accuracy: 0.6856\nEpoch 6/100 - Loss: 0.0185, Val Loss: 0.7706, Val Accuracy: 0.7138\nEpoch 7/100 - Loss: 0.0183, Val Loss: 0.7171, Val Accuracy: 0.7007\nEpoch 8/100 - Loss: 0.0178, Val Loss: 0.7257, Val Accuracy: 0.7119\nEpoch 9/100 - Loss: 0.0186, Val Loss: 0.8717, Val Accuracy: 0.6662\nEpoch 10/100 - Loss: 0.0182, Val Loss: 0.7840, Val Accuracy: 0.6975\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0195, Val Loss: 0.7994, Val Accuracy: 0.7015\nEpoch 2/100 - Loss: 0.0175, Val Loss: 0.8095, Val Accuracy: 0.6960\nEpoch 3/100 - Loss: 0.0182, Val Loss: 0.8052, Val Accuracy: 0.6905\nEpoch 4/100 - Loss: 0.0190, Val Loss: 0.8562, Val Accuracy: 0.6962\nStopping early at epoch 5\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.7170110413514729, Validation Accuracy = 0.7007333333333333\nFold 1: Validation Loss = 0.7994299579276714, Validation Accuracy = 0.7015\nAverage Validation Loss across folds: 0.758220499639572\nAverage Validation Accuracy across folds: 0.7011166666666666\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.6000000000000001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0242, Val Loss: 1.2340, Val Accuracy: 0.4771\nEpoch 2/100 - Loss: 0.0236, Val Loss: 1.2048, Val Accuracy: 0.5119\nEpoch 3/100 - Loss: 0.0239, Val Loss: 1.2743, Val Accuracy: 0.4634\nEpoch 4/100 - Loss: 0.0246, Val Loss: 1.2974, Val Accuracy: 0.4934\nEpoch 5/100 - Loss: 0.0247, Val Loss: 1.3499, Val Accuracy: 0.4290\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0232, Val Loss: 0.9873, Val Accuracy: 0.5486\nEpoch 2/100 - Loss: 0.0221, Val Loss: 0.9696, Val Accuracy: 0.6246\nEpoch 3/100 - Loss: 0.0228, Val Loss: 0.9915, Val Accuracy: 0.6192\nEpoch 4/100 - Loss: 0.0229, Val Loss: 1.1766, Val Accuracy: 0.5793\nEpoch 5/100 - Loss: 0.0233, Val Loss: 1.1956, Val Accuracy: 0.5367\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.2048083548861017, Validation Accuracy = 0.5118666666666667\nFold 1: Validation Loss = 0.969661431653159, Validation Accuracy = 0.6246\nAverage Validation Loss across folds: 1.0872348932696303\nAverage Validation Accuracy across folds: 0.5682333333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.7000000000000001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0308, Val Loss: 1.5046, Val Accuracy: 0.4832\nEpoch 2/100 - Loss: 0.0305, Val Loss: 1.5145, Val Accuracy: 0.4152\nEpoch 3/100 - Loss: 0.0308, Val Loss: 1.5467, Val Accuracy: 0.3959\nEpoch 4/100 - Loss: 0.0318, Val Loss: 1.5415, Val Accuracy: 0.3685\nStopping early at epoch 5\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0314, Val Loss: 1.6463, Val Accuracy: 0.3461\nEpoch 2/100 - Loss: 0.0302, Val Loss: 1.7507, Val Accuracy: 0.2485\nEpoch 3/100 - Loss: 0.0309, Val Loss: 1.6355, Val Accuracy: 0.3016\nEpoch 4/100 - Loss: 0.0321, Val Loss: 1.8940, Val Accuracy: 0.2018\nEpoch 5/100 - Loss: 0.0317, Val Loss: 1.7913, Val Accuracy: 0.2018\nEpoch 6/100 - Loss: 0.0310, Val Loss: 1.8040, Val Accuracy: 0.2263\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.5047882640285533, Validation Accuracy = 0.4831666666666667\nFold 1: Validation Loss = 1.6354421125546192, Validation Accuracy = 0.30156666666666665\nAverage Validation Loss across folds: 1.5701151882915863\nAverage Validation Accuracy across folds: 0.39236666666666664\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.8\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0357, Val Loss: 2.1137, Val Accuracy: 0.1825\nEpoch 2/100 - Loss: 0.0355, Val Loss: 2.0820, Val Accuracy: 0.1930\nEpoch 3/100 - Loss: 0.0357, Val Loss: 2.3042, Val Accuracy: 0.0984\nEpoch 4/100 - Loss: 0.0360, Val Loss: 2.3039, Val Accuracy: 0.1003\nEpoch 5/100 - Loss: 0.0360, Val Loss: 2.3035, Val Accuracy: 0.0987\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0355, Val Loss: 2.1630, Val Accuracy: 0.1757\nEpoch 2/100 - Loss: 0.0356, Val Loss: 2.1392, Val Accuracy: 0.1745\nEpoch 3/100 - Loss: 0.0356, Val Loss: 2.2030, Val Accuracy: 0.1490\nEpoch 4/100 - Loss: 0.0358, Val Loss: 2.3031, Val Accuracy: 0.0981\nEpoch 5/100 - Loss: 0.0360, Val Loss: 2.3036, Val Accuracy: 0.0981\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.0820757237070406, Validation Accuracy = 0.193\nFold 1: Validation Loss = 2.139180318633122, Validation Accuracy = 0.1745\nAverage Validation Loss across folds: 2.1106280211700814\nAverage Validation Accuracy across folds: 0.18375\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.9\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0362, Val Loss: 2.3032, Val Accuracy: 0.0998\nEpoch 2/100 - Loss: 0.0360, Val Loss: 2.3036, Val Accuracy: 0.0985\nEpoch 3/100 - Loss: 0.0360, Val Loss: 2.3030, Val Accuracy: 0.0993\nEpoch 4/100 - Loss: 0.0360, Val Loss: 2.3029, Val Accuracy: 0.1011\nEpoch 5/100 - Loss: 0.0360, Val Loss: 2.3035, Val Accuracy: 0.0985\nEpoch 6/100 - Loss: 0.0360, Val Loss: 2.3030, Val Accuracy: 0.0998\nEpoch 7/100 - Loss: 0.0360, Val Loss: 2.3044, Val Accuracy: 0.1011\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0363, Val Loss: 2.3038, Val Accuracy: 0.0989\nEpoch 2/100 - Loss: 0.0360, Val Loss: 2.3039, Val Accuracy: 0.0993\nEpoch 3/100 - Loss: 0.0360, Val Loss: 2.3037, Val Accuracy: 0.0999\nEpoch 4/100 - Loss: 0.0360, Val Loss: 2.3034, Val Accuracy: 0.0999\nEpoch 5/100 - Loss: 0.0360, Val Loss: 2.3031, Val Accuracy: 0.0993\nEpoch 6/100 - Loss: 0.0360, Val Loss: 2.3039, Val Accuracy: 0.0989\nEpoch 7/100 - Loss: 0.0360, Val Loss: 2.3035, Val Accuracy: 0.0989\nEpoch 8/100 - Loss: 0.0360, Val Loss: 2.3042, Val Accuracy: 0.0993\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3028677414729395, Validation Accuracy = 0.10113333333333334\nFold 1: Validation Loss = 2.3030834497927604, Validation Accuracy = 0.09933333333333333\nAverage Validation Loss across folds: 2.3029755956328497\nAverage Validation Accuracy across folds: 0.10023333333333334\n--------------------------------\nK-fold Cross-validation completed.\n   dropout_rate  train_loss  val_loss  val_accuracy\n0           0.0    0.404410  0.859967      0.006408\n1           0.1    0.434562  0.846133      0.008214\n2           0.2    0.476113  0.829450      0.009961\n3           0.3    0.544292  0.803483      0.012022\n4           0.4    0.623171  0.767600      0.014555\n5           0.5    0.758220  0.701117      0.018314\n6           0.6    1.087235  0.568233      0.023683\n7           0.7    1.570115  0.392367      0.031110\n8           0.8    2.110628  0.183750      0.035786\n9           0.9    2.302976  0.100233      0.036044\n\n\n\n# rename columns as they were ordered wrong (fixing)\n\nresults_choice_3.rename(columns={'train_loss': 'average_val_loss'}, inplace=True)\nresults_choice_3.rename(columns={'val_loss': 'average_val_accuracy'}, inplace=True)\nresults_choice_3.rename(columns={'val_accuracy': 'average_train_loss'}, inplace=True)\n\n# save results_df3 to csv\nresults_choice_3.to_csv('results/results_choice_3.csv')\n\n\n# display results\nresults_choice_3\n\n\n\n\n\n\n\n\ndropout_rate\naverage_val_loss\naverage_val_accuracy\naverage_train_loss\n\n\n\n\n0\n0.0\n0.404410\n0.859967\n0.006408\n\n\n1\n0.1\n0.434562\n0.846133\n0.008214\n\n\n2\n0.2\n0.476113\n0.829450\n0.009961\n\n\n3\n0.3\n0.544292\n0.803483\n0.012022\n\n\n4\n0.4\n0.623171\n0.767600\n0.014555\n\n\n5\n0.5\n0.758220\n0.701117\n0.018314\n\n\n6\n0.6\n1.087235\n0.568233\n0.023683\n\n\n7\n0.7\n1.570115\n0.392367\n0.031110\n\n\n8\n0.8\n2.110628\n0.183750\n0.035786\n\n\n9\n0.9\n2.302976\n0.100233\n0.036044\n\n\n\n\n\n\n\n\nresults_choice_3 = pd.read_csv('results/results_choice_3.csv')\n\n# Plot\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(results_choice_3['dropout_rate'], results_choice_3['average_train_loss'], label='Training Loss', marker='o', color='green')\nplt.plot(results_choice_3['dropout_rate'], results_choice_3['average_val_loss'], label='Validation Loss', marker='o', color='red')\nplt.title('Training and Validation Loss vs. Dropout Rate')\nplt.xlabel('Dropout Rate')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot\nplt.subplot(1, 2, 2)\nerror_ratio = results_choice_3['average_val_loss'] / results_choice_3['average_train_loss']\nplt.plot(results_choice_3['dropout_rate'], error_ratio, label='Val/Train Loss Ratio', marker='o')\nplt.title('Error Ratio vs. Dropout Rate')\nplt.xlabel('Dropout Rate')\nplt.ylabel('Ratio')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAt the very end, select a best fit model and report, training, validation, and test errors at the very end\nMake sure your “plotting variable=True” when for the final training\n\nMy best model was the one with the following parameters: * hidden activation=relu, hidden layers = [96,96,96], optimizer=adam, learning rate=0.001, L1=0, L2=0, dropout=0, weight initialization=random\n\n# Best model\ndef model_fn_final():\n    return NeuralNetwork(input_size=28*28, layer_sizes=[96, 96, 96], activation_funcs=['relu', 'relu', 'relu'], dropout_rate=0)\n\nfinal_results = []\n\navg_val_loss, avg_val_accuracy, avg_train_loss = training_model_kfold(k_folds=4, dataset=training_data, model_fn=model_fn_final, loss_fn=nn.CrossEntropyLoss(), optimizer_name=\"Adam\", lr=0.001, l1_reg=0, l2_reg=0, num_epochs=100, device='cpu', plot=True, patience=4)\n\nfinal_results.append({\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n        'val_accuracy': avg_val_accuracy\n    })\n\nfinal_results = pd.DataFrame(final_results)\nprint(final_results)\n\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0098, Val Loss: 0.4877, Val Accuracy: 0.8215\nEpoch 2/100 - Loss: 0.0065, Val Loss: 0.4001, Val Accuracy: 0.8525\nEpoch 3/100 - Loss: 0.0058, Val Loss: 0.3777, Val Accuracy: 0.8595\nEpoch 4/100 - Loss: 0.0055, Val Loss: 0.3714, Val Accuracy: 0.8619\nEpoch 5/100 - Loss: 0.0051, Val Loss: 0.3436, Val Accuracy: 0.8764\nEpoch 6/100 - Loss: 0.0049, Val Loss: 0.3358, Val Accuracy: 0.8739\nEpoch 7/100 - Loss: 0.0046, Val Loss: 0.3054, Val Accuracy: 0.8892\nEpoch 8/100 - Loss: 0.0044, Val Loss: 0.3431, Val Accuracy: 0.8756\nEpoch 9/100 - Loss: 0.0042, Val Loss: 0.3166, Val Accuracy: 0.8860\nEpoch 10/100 - Loss: 0.0041, Val Loss: 0.3118, Val Accuracy: 0.8873\nStopping early at epoch 11\n\n\n\n\n\n\n\n\n\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0100, Val Loss: 0.4680, Val Accuracy: 0.8311\nEpoch 2/100 - Loss: 0.0065, Val Loss: 0.4125, Val Accuracy: 0.8465\nEpoch 3/100 - Loss: 0.0059, Val Loss: 0.3794, Val Accuracy: 0.8656\nEpoch 4/100 - Loss: 0.0054, Val Loss: 0.3961, Val Accuracy: 0.8559\nEpoch 5/100 - Loss: 0.0051, Val Loss: 0.3500, Val Accuracy: 0.8723\nEpoch 6/100 - Loss: 0.0048, Val Loss: 0.3646, Val Accuracy: 0.8695\nEpoch 7/100 - Loss: 0.0046, Val Loss: 0.3671, Val Accuracy: 0.8704\nEpoch 8/100 - Loss: 0.0044, Val Loss: 0.3426, Val Accuracy: 0.8803\nEpoch 9/100 - Loss: 0.0043, Val Loss: 0.3402, Val Accuracy: 0.8771\nEpoch 10/100 - Loss: 0.0041, Val Loss: 0.3702, Val Accuracy: 0.8669\nEpoch 11/100 - Loss: 0.0039, Val Loss: 0.3457, Val Accuracy: 0.8785\nEpoch 12/100 - Loss: 0.0038, Val Loss: 0.3315, Val Accuracy: 0.8853\nEpoch 13/100 - Loss: 0.0037, Val Loss: 0.3715, Val Accuracy: 0.8730\nEpoch 14/100 - Loss: 0.0035, Val Loss: 0.3392, Val Accuracy: 0.8790\nEpoch 15/100 - Loss: 0.0034, Val Loss: 0.3400, Val Accuracy: 0.8804\nStopping early at epoch 16\n\n\n\n\n\n\n\n\n\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0099, Val Loss: 0.4422, Val Accuracy: 0.8445\nEpoch 2/100 - Loss: 0.0065, Val Loss: 0.3862, Val Accuracy: 0.8597\nEpoch 3/100 - Loss: 0.0058, Val Loss: 0.3709, Val Accuracy: 0.8644\nEpoch 4/100 - Loss: 0.0054, Val Loss: 0.3443, Val Accuracy: 0.8763\nEpoch 5/100 - Loss: 0.0050, Val Loss: 0.3434, Val Accuracy: 0.8763\nEpoch 6/100 - Loss: 0.0049, Val Loss: 0.3322, Val Accuracy: 0.8795\nEpoch 7/100 - Loss: 0.0046, Val Loss: 0.3170, Val Accuracy: 0.8875\nEpoch 8/100 - Loss: 0.0044, Val Loss: 0.3164, Val Accuracy: 0.8856\nEpoch 9/100 - Loss: 0.0043, Val Loss: 0.3170, Val Accuracy: 0.8868\nEpoch 10/100 - Loss: 0.0041, Val Loss: 0.3228, Val Accuracy: 0.8808\nEpoch 11/100 - Loss: 0.0040, Val Loss: 0.3112, Val Accuracy: 0.8851\nEpoch 12/100 - Loss: 0.0039, Val Loss: 0.3166, Val Accuracy: 0.8866\nEpoch 13/100 - Loss: 0.0037, Val Loss: 0.3131, Val Accuracy: 0.8903\nEpoch 14/100 - Loss: 0.0037, Val Loss: 0.3038, Val Accuracy: 0.8893\nEpoch 15/100 - Loss: 0.0035, Val Loss: 0.3038, Val Accuracy: 0.8941\nEpoch 16/100 - Loss: 0.0034, Val Loss: 0.3144, Val Accuracy: 0.8884\nEpoch 17/100 - Loss: 0.0034, Val Loss: 0.3054, Val Accuracy: 0.8929\nStopping early at epoch 18\n\n\n\n\n\n\n\n\n\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0098, Val Loss: 0.4526, Val Accuracy: 0.8384\nEpoch 2/100 - Loss: 0.0065, Val Loss: 0.4153, Val Accuracy: 0.8541\nEpoch 3/100 - Loss: 0.0058, Val Loss: 0.3749, Val Accuracy: 0.8682\nEpoch 4/100 - Loss: 0.0054, Val Loss: 0.3635, Val Accuracy: 0.8652\nEpoch 5/100 - Loss: 0.0051, Val Loss: 0.3493, Val Accuracy: 0.8727\nEpoch 6/100 - Loss: 0.0048, Val Loss: 0.3533, Val Accuracy: 0.8706\nEpoch 7/100 - Loss: 0.0046, Val Loss: 0.3431, Val Accuracy: 0.8752\nEpoch 8/100 - Loss: 0.0044, Val Loss: 0.3582, Val Accuracy: 0.8745\nEpoch 9/100 - Loss: 0.0043, Val Loss: 0.3276, Val Accuracy: 0.8806\nEpoch 10/100 - Loss: 0.0041, Val Loss: 0.3176, Val Accuracy: 0.8866\nEpoch 11/100 - Loss: 0.0039, Val Loss: 0.3192, Val Accuracy: 0.8842\nEpoch 12/100 - Loss: 0.0038, Val Loss: 0.3209, Val Accuracy: 0.8873\nEpoch 13/100 - Loss: 0.0037, Val Loss: 0.3349, Val Accuracy: 0.8833\nEpoch 14/100 - Loss: 0.0035, Val Loss: 0.3159, Val Accuracy: 0.8897\nEpoch 15/100 - Loss: 0.0035, Val Loss: 0.3241, Val Accuracy: 0.8868\nEpoch 16/100 - Loss: 0.0034, Val Loss: 0.3303, Val Accuracy: 0.8880\nEpoch 17/100 - Loss: 0.0033, Val Loss: 0.3327, Val Accuracy: 0.8857\nStopping early at epoch 18\n\n\n\n\n\n\n\n\n\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.30621458574178373, Validation Accuracy = 0.8892\nFold 1: Validation Loss = 0.33166823387145994, Validation Accuracy = 0.8853333333333333\nFold 2: Validation Loss = 0.3046102743833623, Validation Accuracy = 0.8893333333333333\nFold 3: Validation Loss = 0.31593505687853124, Validation Accuracy = 0.8896666666666667\nAverage Validation Loss across folds: 0.3146070377187843\nAverage Validation Accuracy across folds: 0.8883833333333333\n--------------------------------\nK-fold Cross-validation completed.\n   train_loss  val_loss  val_accuracy\n0    0.004801  0.314607      0.888383\n\n\n\n# save final_results_df to csv\nfinal_results.to_csv('results/final_results.csv')\n\n\n\n\n\n# Model evaluation\nmodel = NeuralNetwork(input_size=28*28, layer_sizes=[96, 96, 96], activation_funcs=['relu', 'relu', 'relu'], dropout_rate=0)\nmodel.load_state_dict(torch.load(\"best_model_fold_2.pth\"))\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\ntest(test_dataloader, model, loss_fn, device)\nprint(\"Done!\")\n\nTest Error: \n Accuracy: 88.2%, Avg loss: 0.337591 \n\nDone!"
  },
  {
    "objectID": "ANN.html#overview",
    "href": "ANN.html#overview",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "Classification is one of the most common forms of supervised machine learning\nIn this homework we will explore “model tuning” for the case of a multi-class classification problem, as applied the MNIST data set\nYou can do this assignment in either Keras OR PyTorch (or both), it is your choice."
  },
  {
    "objectID": "ANN.html#submission",
    "href": "ANN.html#submission",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "You need to upload TWO documents to Canvas when you are done\n\n\nA PDF (or HTML) of the completed form of the HW-2.ipynb document\n\n\nThe final uploaded version should NOT have any code-errors present\nAll outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\nIMPORTANT: THERE ARE MANY WAYS TO DO THIS, SO FEEL FREE TO DEVIATE SLIGHTLY FROM THE EXACT DETAILS, BUT THE OVERALL RESULT AND FLOW SHOULD MATCH WHAT IS OUTLINED BELOW."
  },
  {
    "objectID": "ANN.html#important-notes",
    "href": "ANN.html#important-notes",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "I printed the outputs as the runtimes were very long and it is common practice. Furthermore, even though it will be mentioned later, the gridsearch for the choice with l1 and l2 was done in a 5 by 5 grid instead of a 10 by 10 grid. Professor James said that it was okay to do so."
  },
  {
    "objectID": "ANN.html#hw-2.1-data-preparation",
    "href": "ANN.html#hw-2.1-data-preparation",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "Normalize the data as needed\nPartition data into training, validation, and test (i.e. leave one out CV)\n\nOne option to do this is to give these arrays global scope so they are seen inside the training function (so they don’t need to be passed to functions)\n\nOptional but recommended: Create a K-fold cross validation data set, rather than just doing leave one out\nDo any other preprocessing you feel is needed\n\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader, random_split\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom sklearn.model_selection import KFold\nimport copy\nimport pandas as pd\nimport seaborn as sns\n\n\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\n# You can also create DataLoader instances for both datasets to easily iterate during training and validation\nbatch_size = 64\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\n\n# explore data-set object\nprint(\"\\n TYPE:\\n\",type(training_data))\nprint(\"\\n SUMMARY:\\n\",training_data)\nprint(\"\\n ATTRIBUTES:\\n\",training_data.__dict__.keys())\nprint(\"\\n FIRST DATA POINT:\\n\",)\nimg, label = training_data[0]\nprint(img.shape,label)\nprint(len(training_data))\n# print(training_data.samples)\n\n\n TYPE:\n &lt;class 'torchvision.datasets.mnist.FashionMNIST'&gt;\n\n SUMMARY:\n Dataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor()\n\n ATTRIBUTES:\n dict_keys(['root', 'transform', 'target_transform', 'transforms', 'train', 'data', 'targets'])\n\n FIRST DATA POINT:\n\ntorch.Size([1, 28, 28]) 9\n60000\n\n\n\n# from demonstart data loader\n# for X, y in test_dataloader:\nfor X, y in train_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64"
  },
  {
    "objectID": "ANN.html#hw-2.2-generalized-model",
    "href": "ANN.html#hw-2.2-generalized-model",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "Create a General model function (or class) that takes hyper-parameters and evaluates the model\n\nThe function should work with a set of hyper parameters than can be easily be controlled and varied by the user (for later parameter tuning)\nThis should work for the training, test, and validation set\n\nFeel free to recycle code from the lab assignments and demo’s\n\nUse the deep learning best practices that we discussed in class.\nDocument what is going on in the code, as needed, with narrative markdown text between cells.\n\n\n# Get cpu or gpu device for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Define neural network\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, layer_sizes, activation_funcs, dropout_rate=0.0, weight_init='random'):\n        # Initialize neural network\n        super(NeuralNetwork, self).__init__()\n        # flatten the imput (as they are images)\n        layers = [nn.Flatten()]\n        # prev_size as size of previous imputs to track the next layer size\n        prev_size = input_size\n\n        # weight initialization\n        def init_weights(layer):\n            if weight_init == 'xavier':\n                if isinstance(layer, nn.Linear):\n                    init.xavier_uniform_(layer.weight)\n\n        # loop through the layers or the network\n        for i, size in enumerate(layer_sizes):\n            # fully connected layers\n            linear_layer = nn.Linear(prev_size, size)\n\n            layers.append(linear_layer)\n            \n            # activation functions\n            activation_func = activation_funcs[i]\n            if activation_func == 'relu':\n                layers.append(nn.ReLU())\n            # sigmoid activation function as default for classification\n            elif activation_func == 'sigmoid':\n                layers.append(nn.Sigmoid())\n            # add dropout layer if dropout rate is greater than 0\n            if dropout_rate &gt; 0:\n                layers.append(nn.Dropout(dropout_rate))\n            # set the size for the next layer\n            prev_size = size\n        \n        # Output layer with 10 classes\n        output_layer = nn.Linear(prev_size, 10) \n\n        # Initialize weights of output layer\n        init_weights(output_layer)\n        \n        layers.append(output_layer)\n\n        # Store layers in Sequential container\n        self.model = nn.Sequential(*layers)\n    # forward pass through the network\n    def forward(self, X):\n        return self.model(X)\n\n# define model\ndef model_fn():\n    return NeuralNetwork(input_size=28*28, layer_sizes=[512, 256], activation_funcs=['relu', 'relu'], dropout_rate=0.5)\n\nUsing cpu device"
  },
  {
    "objectID": "ANN.html#hw-2.3-model-training-function",
    "href": "ANN.html#hw-2.3-model-training-function",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "You can do this in either a function (or python class), or however you think is best.\nCreate a training function (or class) that takes hyper-parameter choices and trains the model\n\nIf you are doing “leave one out”, your training function only needs to do one training per hyper-parameter choice\nIf you are doing K-fold cross validation, you should train the model K times for each hyper-parameter choice, and report the average result cross the training runs at the end (this is technically a better practice but requires more computation).\nUse a dense feed forward ANN model, with the correct output layer activation, and correct loss function\nYou MUST use early stopping inside the function, otherwise it defeats the point\nHave at least the following hyper-parameters as inputs to this function\n\nL1 regularization constant, L2 regularization constant, dropout rate\nLearning rate\nWeight Initialization: Fully random vs Xavier Weight Initialization\nHidden layer activation function choice (use relu, sigmoid, or tanh)\nNumber and size of ANN hidden layers\nOptimizer choice, have at least three included (Adam, SGD, or RmsProp)\nYou can wrap all of the hyper-parameter arguments into a dictionary, or do it however you want\n\n\nVisualization\n\nInclude a boolean parameter as a function input that controls whether visualization is created or not\nIf true, Monitor training and validation throughout training by plotting\nReport a confusion matrix\n\nReturn the final training and validation error (averaged if using K-fold)\n\nagain, you must use early stopping to report the best training/validation loss without over-fitting\n\n\nDepending how you do this, it can be a lot of computation, start small and scale up and consider using Co-lab\n\n\n# model evaluation\ndef evaluate_model(model, dataloader, loss_fn, device):\n    # Set model to evaluation mode\n    model.eval()\n    # Initialize metrics\n    total_loss, correct, total = 0, 0, 0\n    # no gradients during evaluation (saving memory and computations)\n    with torch.no_grad():\n        # loop over batches of data\n        for X, y in dataloader:\n            # move data to device\n            X, y = X.to(device), y.to(device)\n            # predict on X\n            outputs = model(X)\n            # calculate loss\n            loss = loss_fn(outputs, y)\n            # extract and add loss to the total loss\n            total_loss += loss.item()\n            # calculate correct predictions\n            correct += (outputs.argmax(1) == y).sum().item()\n            # number of samples\n            total += y.size(0)\n    # return average loss and accuracy\n    return total_loss / len(dataloader), correct / total\n\n# train model\ndef training_model_kfold(k_folds, dataset, model_fn, loss_fn, optimizer_name, lr, l1_reg, l2_reg, num_epochs=100, device='cpu', plot=False, patience=4):\n    # Divide into k folds\n    kfold = KFold(n_splits=k_folds, shuffle=True)\n    # Initialize metrics to store\n    metrics_record = {'val_loss': [], 'val_accuracy': [], ['train_loss'] : []}\n\n    # Loop through the folds\n    for fold, (train_ids, test_ids) in enumerate(kfold.split(np.arange(len(dataset)))):\n        print(f'FOLD {fold}')\n        print('--------------------------------')\n\n        # Reset early stopping and best model tracking for each fold\n        early_stopping_wait_number = 0\n        best_val_loss = np.inf\n        best_model = None\n        \n        # Initialize plotting lists for each fold (plot=True only)\n        fold_performance = {'train_loss': [], 'val_loss': [], 'val_accuracy': []} if plot else None\n\n        # data loaders for training and testing data in this fold (better for batch processing)\n        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n        trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=train_subsampler)\n        testloader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=test_subsampler)\n\n        # Initialize neural network\n        model = model_fn().to(device)\n\n        # Choose optimizer (added more as probably will be used in the future)\n        optimizer = {\n            \"Adam\": torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg),\n            \"SGD\": torch.optim.SGD(model.parameters(), lr=lr, weight_decay=l2_reg),\n            \"RMSProp\": torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=l2_reg),\n            \"LBFGS\": torch.optim.LBFGS(model.parameters(), lr=lr)\n        }.get(optimizer_name, None)\n        # raise flag if optimizer is not recognized\n        if optimizer is None:\n            raise ValueError(f'Optimizer {optimizer_name} not recognized')\n        # loop through the epochs\n        for epoch in range(num_epochs):\n            # Training\n            model.train()\n            # initialize total loss\n            total_loss = 0.0\n            # loop through the batches\n            for batch, (X, y) in enumerate(trainloader):\n                # move data to device\n                X, y = X.to(device), y.to(device)\n                # zero the gradients\n                optimizer.zero_grad()\n                outputs = model(X)\n                loss = loss_fn(outputs, y)\n                l1_norm = sum(p.abs().sum() for p in model.parameters())\n                loss += l1_reg * l1_norm\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            average_train_loss_for_fold = total_loss / len(train_ids)\n            metrics_record['train_loss'].append(average_train_loss_for_fold)\n\n\n            # Validation\n            val_loss, correct = 0.0, 0\n            # set model to evaluation mode\n            model.eval()\n            with torch.no_grad():\n                for X, y in testloader:\n                    X, y = X.to(device), y.to(device)\n                    outputs = model(X)\n                    loss = loss_fn(outputs, y)\n                    val_loss += loss.item()\n                    correct += (outputs.argmax(1) == y).sum().item()\n\n            val_loss /= len(testloader)\n            val_accuracy = correct / len(test_ids)\n\n            # plotting data for this fold\n            if plot:\n                fold_performance['train_loss'].append(total_loss / len(train_ids))\n                fold_performance['val_loss'].append(val_loss)\n                fold_performance['val_accuracy'].append(val_accuracy)\n\n            # Set early stopping\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_model = copy.deepcopy(model.state_dict())\n                early_stopping_wait_number = 0\n            else:\n                early_stopping_wait_number += 1\n                if early_stopping_wait_number &gt;= patience:\n                    print(f\"Stopping early at epoch {epoch+1}\")\n                    break\n\n            print(f'Epoch {epoch+1}/{num_epochs} - Loss: {total_loss / len(train_ids):.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n\n        # Load best model fold\n        model.load_state_dict(best_model)\n\n        # save the best model of each fold to disk\n        torch.save(best_model, f'best_model_fold_{fold}.pth')\n\n        # Evaluate the model on test set \n        fold_val_loss, fold_val_accuracy = evaluate_model(model, testloader, loss_fn, device)\n        metrics_record['val_loss'].append(fold_val_loss)\n        metrics_record['val_accuracy'].append(fold_val_accuracy)\n\n        # Plot for each fold\n        if plot:\n            plt.figure(figsize=(12, 6))\n            plt.subplot(1, 3, 1)\n            plt.plot(fold_performance['train_loss'], label='Training Loss')\n            plt.title('Training Loss')\n            plt.legend()\n            plt.subplot(1, 3, 2)\n            plt.plot(fold_performance['val_loss'], label='Validation Loss')\n            plt.title('Validation Loss')\n            plt.legend()\n            plt.subplot(1, 3, 3)\n            plt.plot(fold_performance['val_accuracy'], label='Validation Accuracy')\n            plt.title('Validation Accuracy')\n            plt.legend()\n            plt.show()\n        \n        print('--------------------------------')\n        print(f'Finished FOLD {fold}')\n        print('--------------------------------')\n    \n    # print validation loss and accuracy per fold\n    print(\"Validation Loss and Accuracy per Fold:\")\n    for i, (loss, accuracy) in enumerate(zip(metrics_record['val_loss'], metrics_record['val_accuracy'])):\n        print(f\"Fold {i}: Validation Loss = {loss}, Validation Accuracy = {accuracy}\")\n\n    # avg performance across folds\n    average_val_loss = np.mean(metrics_record['val_loss'])\n    average_val_accuracy = np.mean(metrics_record['val_accuracy'])\n\n    average_val_loss = np.mean(metrics_record['val_loss'])\n    average_val_accuracy = np.mean(metrics_record['val_accuracy'])\n    average_train_loss = np.mean(metrics_record['train_loss'])\n\n    print(f\"Average Validation Loss across folds: {average_val_loss}\")\n    print(f\"Average Validation Accuracy across folds: {average_val_accuracy}\")\n    print('--------------------------------')\n    print('K-fold Cross-validation completed.')\n    # return avg validation loss, avg validation accuracy, and avg training loss\n    return average_val_loss, average_val_accuracy, average_train_loss\n\nHere, we are training the model once to see how it works and with the default parameters we picked.\n\n# train model\ntraining_model_kfold(k_folds=4, dataset=training_data, model_fn=model_fn, loss_fn=nn.CrossEntropyLoss(), optimizer_name=\"Adam\", lr=0.001, l1_reg=0, l2_reg=0, num_epochs=100, device='cpu', plot=True)\n\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0102, Val Loss: 0.4388, Val Accuracy: 0.8372\nEpoch 2/100 - Loss: 0.0074, Val Loss: 0.3866, Val Accuracy: 0.8532\nEpoch 3/100 - Loss: 0.0069, Val Loss: 0.3615, Val Accuracy: 0.8645\nEpoch 4/100 - Loss: 0.0065, Val Loss: 0.3604, Val Accuracy: 0.8628\nEpoch 5/100 - Loss: 0.0063, Val Loss: 0.3463, Val Accuracy: 0.8689\nEpoch 6/100 - Loss: 0.0061, Val Loss: 0.3351, Val Accuracy: 0.8772\nEpoch 7/100 - Loss: 0.0059, Val Loss: 0.3452, Val Accuracy: 0.8725\nEpoch 8/100 - Loss: 0.0058, Val Loss: 0.3350, Val Accuracy: 0.8766\nEpoch 9/100 - Loss: 0.0057, Val Loss: 0.3224, Val Accuracy: 0.8785\nEpoch 10/100 - Loss: 0.0056, Val Loss: 0.3237, Val Accuracy: 0.8793\nEpoch 11/100 - Loss: 0.0055, Val Loss: 0.3174, Val Accuracy: 0.8814\nEpoch 12/100 - Loss: 0.0054, Val Loss: 0.3174, Val Accuracy: 0.8799\nEpoch 13/100 - Loss: 0.0053, Val Loss: 0.3184, Val Accuracy: 0.8829\nEpoch 14/100 - Loss: 0.0053, Val Loss: 0.3096, Val Accuracy: 0.8849\nEpoch 15/100 - Loss: 0.0052, Val Loss: 0.3127, Val Accuracy: 0.8847\nEpoch 16/100 - Loss: 0.0052, Val Loss: 0.3216, Val Accuracy: 0.8812\nEpoch 17/100 - Loss: 0.0051, Val Loss: 0.3096, Val Accuracy: 0.8861\nEpoch 18/100 - Loss: 0.0050, Val Loss: 0.3079, Val Accuracy: 0.8875\nEpoch 19/100 - Loss: 0.0051, Val Loss: 0.3158, Val Accuracy: 0.8822\nEpoch 20/100 - Loss: 0.0050, Val Loss: 0.3032, Val Accuracy: 0.8879\nEpoch 21/100 - Loss: 0.0050, Val Loss: 0.3124, Val Accuracy: 0.8808\nEpoch 22/100 - Loss: 0.0048, Val Loss: 0.3084, Val Accuracy: 0.8871\nEpoch 23/100 - Loss: 0.0048, Val Loss: 0.3042, Val Accuracy: 0.8857\nEpoch 24/100 - Loss: 0.0048, Val Loss: 0.3045, Val Accuracy: 0.8864\nStopping early at epoch 25\n\n\n\n\n\n\n\n\n\nAverage Validation Loss across folds: 0.3041553478291694\nAverage Validation Accuracy across folds: 0.8878666666666667\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0102, Val Loss: 0.4408, Val Accuracy: 0.8400\nEpoch 2/100 - Loss: 0.0075, Val Loss: 0.4002, Val Accuracy: 0.8505\nEpoch 3/100 - Loss: 0.0068, Val Loss: 0.3707, Val Accuracy: 0.8677\nEpoch 4/100 - Loss: 0.0066, Val Loss: 0.3751, Val Accuracy: 0.8635\nEpoch 5/100 - Loss: 0.0063, Val Loss: 0.3623, Val Accuracy: 0.8673\nEpoch 6/100 - Loss: 0.0062, Val Loss: 0.3423, Val Accuracy: 0.8753\nEpoch 7/100 - Loss: 0.0059, Val Loss: 0.3427, Val Accuracy: 0.8754\nEpoch 8/100 - Loss: 0.0058, Val Loss: 0.3425, Val Accuracy: 0.8767\nEpoch 9/100 - Loss: 0.0058, Val Loss: 0.3278, Val Accuracy: 0.8814\nEpoch 10/100 - Loss: 0.0056, Val Loss: 0.3278, Val Accuracy: 0.8817\nEpoch 11/100 - Loss: 0.0056, Val Loss: 0.3329, Val Accuracy: 0.8795\nEpoch 12/100 - Loss: 0.0054, Val Loss: 0.3309, Val Accuracy: 0.8803\nEpoch 13/100 - Loss: 0.0054, Val Loss: 0.3216, Val Accuracy: 0.8834\nEpoch 14/100 - Loss: 0.0053, Val Loss: 0.3155, Val Accuracy: 0.8862\nEpoch 15/100 - Loss: 0.0052, Val Loss: 0.3307, Val Accuracy: 0.8825\nEpoch 16/100 - Loss: 0.0052, Val Loss: 0.3109, Val Accuracy: 0.8865\nEpoch 17/100 - Loss: 0.0051, Val Loss: 0.3207, Val Accuracy: 0.8860\nEpoch 18/100 - Loss: 0.0051, Val Loss: 0.3162, Val Accuracy: 0.8842\nEpoch 19/100 - Loss: 0.0050, Val Loss: 0.3171, Val Accuracy: 0.8847\nEpoch 20/100 - Loss: 0.0050, Val Loss: 0.3247, Val Accuracy: 0.8839\nEpoch 21/100 - Loss: 0.0049, Val Loss: 0.3091, Val Accuracy: 0.8883\nEpoch 22/100 - Loss: 0.0049, Val Loss: 0.3155, Val Accuracy: 0.8889\nEpoch 23/100 - Loss: 0.0048, Val Loss: 0.3176, Val Accuracy: 0.8882\nEpoch 24/100 - Loss: 0.0048, Val Loss: 0.3089, Val Accuracy: 0.8939\nEpoch 25/100 - Loss: 0.0049, Val Loss: 0.3017, Val Accuracy: 0.8932\nEpoch 26/100 - Loss: 0.0047, Val Loss: 0.3256, Val Accuracy: 0.8843\nEpoch 27/100 - Loss: 0.0047, Val Loss: 0.3252, Val Accuracy: 0.8799\nEpoch 28/100 - Loss: 0.0046, Val Loss: 0.3190, Val Accuracy: 0.8903\nEpoch 29/100 - Loss: 0.0047, Val Loss: 0.3098, Val Accuracy: 0.8893\nStopping early at epoch 30\n\n\n\n\n\n\n\n\n\nAverage Validation Loss across folds: 0.30303091040634095\nAverage Validation Accuracy across folds: 0.8905333333333334\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0101, Val Loss: 0.4239, Val Accuracy: 0.8476\nEpoch 2/100 - Loss: 0.0074, Val Loss: 0.3951, Val Accuracy: 0.8584\nEpoch 3/100 - Loss: 0.0069, Val Loss: 0.3622, Val Accuracy: 0.8667\nEpoch 4/100 - Loss: 0.0066, Val Loss: 0.3618, Val Accuracy: 0.8703\nEpoch 5/100 - Loss: 0.0063, Val Loss: 0.3505, Val Accuracy: 0.8724\nEpoch 6/100 - Loss: 0.0063, Val Loss: 0.3495, Val Accuracy: 0.8761\nEpoch 7/100 - Loss: 0.0060, Val Loss: 0.3389, Val Accuracy: 0.8799\nEpoch 8/100 - Loss: 0.0059, Val Loss: 0.3376, Val Accuracy: 0.8791\nEpoch 9/100 - Loss: 0.0058, Val Loss: 0.3334, Val Accuracy: 0.8809\nEpoch 10/100 - Loss: 0.0057, Val Loss: 0.3264, Val Accuracy: 0.8831\nEpoch 11/100 - Loss: 0.0055, Val Loss: 0.3250, Val Accuracy: 0.8833\nEpoch 12/100 - Loss: 0.0055, Val Loss: 0.3181, Val Accuracy: 0.8866\nEpoch 13/100 - Loss: 0.0054, Val Loss: 0.3309, Val Accuracy: 0.8794\nEpoch 14/100 - Loss: 0.0053, Val Loss: 0.3145, Val Accuracy: 0.8875\nEpoch 15/100 - Loss: 0.0053, Val Loss: 0.3254, Val Accuracy: 0.8825\nEpoch 16/100 - Loss: 0.0052, Val Loss: 0.3078, Val Accuracy: 0.8891\nEpoch 17/100 - Loss: 0.0052, Val Loss: 0.3187, Val Accuracy: 0.8844\nEpoch 18/100 - Loss: 0.0051, Val Loss: 0.3078, Val Accuracy: 0.8905\nEpoch 19/100 - Loss: 0.0050, Val Loss: 0.3125, Val Accuracy: 0.8881\nEpoch 20/100 - Loss: 0.0050, Val Loss: 0.3138, Val Accuracy: 0.8837\nEpoch 21/100 - Loss: 0.0050, Val Loss: 0.3023, Val Accuracy: 0.8906\nEpoch 22/100 - Loss: 0.0048, Val Loss: 0.3089, Val Accuracy: 0.8883\nEpoch 23/100 - Loss: 0.0049, Val Loss: 0.3162, Val Accuracy: 0.8813\nEpoch 24/100 - Loss: 0.0048, Val Loss: 0.3111, Val Accuracy: 0.8886\nEpoch 25/100 - Loss: 0.0048, Val Loss: 0.3056, Val Accuracy: 0.8881\nStopping early at epoch 26\n\n\n\n\n\n\n\n\n\nAverage Validation Loss across folds: 0.3031931030412092\nAverage Validation Accuracy across folds: 0.8905555555555557\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0102, Val Loss: 0.4590, Val Accuracy: 0.8276\nEpoch 2/100 - Loss: 0.0074, Val Loss: 0.3938, Val Accuracy: 0.8553\nEpoch 3/100 - Loss: 0.0068, Val Loss: 0.3764, Val Accuracy: 0.8630\nEpoch 4/100 - Loss: 0.0065, Val Loss: 0.3597, Val Accuracy: 0.8689\nEpoch 5/100 - Loss: 0.0063, Val Loss: 0.3629, Val Accuracy: 0.8644\nEpoch 6/100 - Loss: 0.0061, Val Loss: 0.3504, Val Accuracy: 0.8704\nEpoch 7/100 - Loss: 0.0059, Val Loss: 0.3478, Val Accuracy: 0.8666\nEpoch 8/100 - Loss: 0.0057, Val Loss: 0.3447, Val Accuracy: 0.8745\nEpoch 9/100 - Loss: 0.0057, Val Loss: 0.3512, Val Accuracy: 0.8686\nEpoch 10/100 - Loss: 0.0056, Val Loss: 0.3322, Val Accuracy: 0.8759\nEpoch 11/100 - Loss: 0.0055, Val Loss: 0.3250, Val Accuracy: 0.8819\nEpoch 12/100 - Loss: 0.0055, Val Loss: 0.3284, Val Accuracy: 0.8801\nEpoch 13/100 - Loss: 0.0053, Val Loss: 0.3178, Val Accuracy: 0.8854\nEpoch 14/100 - Loss: 0.0052, Val Loss: 0.3230, Val Accuracy: 0.8819\nEpoch 15/100 - Loss: 0.0052, Val Loss: 0.3225, Val Accuracy: 0.8807\nEpoch 16/100 - Loss: 0.0051, Val Loss: 0.3126, Val Accuracy: 0.8848\nEpoch 17/100 - Loss: 0.0051, Val Loss: 0.3088, Val Accuracy: 0.8860\nEpoch 18/100 - Loss: 0.0050, Val Loss: 0.3171, Val Accuracy: 0.8856\nEpoch 19/100 - Loss: 0.0050, Val Loss: 0.3192, Val Accuracy: 0.8806\nEpoch 20/100 - Loss: 0.0050, Val Loss: 0.3192, Val Accuracy: 0.8852\nEpoch 21/100 - Loss: 0.0050, Val Loss: 0.3105, Val Accuracy: 0.8886\nStopping early at epoch 22\n\n\n\n\n\n\n\n\n\nAverage Validation Loss across folds: 0.3047214560686274\nAverage Validation Accuracy across folds: 0.8894166666666667\n--------------------------------\nFinished FOLD 3\n--------------------------------\n\n\n\n# test model\ndef test(dataloader, model, loss_fn, device):\n    # size of the dataset\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    # model to evaluation mode\n    model.eval()\n    test_loss, correct = 0, 0\n    # no gradients during evaluation\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    # average loss\n    test_loss /= num_batches \n    #  accuracy \n    correct /= size\n\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n# load model\nmodel = NeuralNetwork(input_size=28*28, layer_sizes=[512, 256], activation_funcs=['relu', 'relu'], dropout_rate=0.5)\n# load the best model from disk\nmodel.load_state_dict(torch.load(\"best_model_fold_2.pth\"))\n# test the model with cross-entropy loss\nloss_fn = nn.CrossEntropyLoss()\n# optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\ntest(test_dataloader, model, loss_fn, device)\nprint(\"Done!\")\n\nTest Error: \n Accuracy: 88.3%, Avg loss: 0.331863 \n\nDone!"
  },
  {
    "objectID": "ANN.html#hw-2.4-hyper-parameter-tuning",
    "href": "ANN.html#hw-2.4-hyper-parameter-tuning",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "Keep detailed records of hyper-parameter choices and associated training & validation errors\nThink critically and visualize the results of the search as needed\nDo each of these in a different sub-section of your notebook\nExplore hyper-parameter choice-0\n\nfor hidden activation=Relu, hidden layers = [32,32], optimizer=adam\nVary the learning rate via a grid search pattern\nPlot training and validation error as a function of the learning rate\nRepeat this exercise for both random and Xavier weight initialization\n\nExplore hyper-parameter choice-1\n\nfor hidden activation=relu, hidden layers = [64,64], optimizer=adam\nVary L1 and L2 in a 10x10 grid search (without dropout)\nPlot validation and training error as a function of L1 and L2 regularization in a 2D heatmap\nPlot the ratio (or difference) of validation to training error as a function of L1 and L2 regularization in a 2D heatmap\n\nExplore hyper-parameter choice-2\n\nfor hidden activation=sigmoid, hidden layers = [96,96,96], optimizer=rmsprop\nVary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization)\nPlot training and validation error as a function of dropout rate\n\nPlot the ratio (or difference) of validation to training error as a function of dropout rate\n\nExplore hyper-parameter choice-3:\n\nfor hidden activation=relu, hidden layers = [96,96,96], optimizer=adam\nVary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization)\nPlot training and validation as a function of dropout rate\n\nPlot the ratio (or difference) of validation to training error as a function of dropout rate\n\nOptional Systematically search for the best regularization parameters choice (3D search) using random search algorithm\n\n(https://en.wikipedia.org/wiki/Random_search)[https://en.wikipedia.org/wiki/Random_search]\nTry to see how deep you can get the ANN (max hidden layers) without suffering from the vanishing gradient effect\n\nFinal fit\n\nAt the very end, select a best fit model and report, training, validation, and test errors at the very end\nMake sure your “plotting variable=True” when for the final training"
  },
  {
    "objectID": "ANN.html#choice-0",
    "href": "ANN.html#choice-0",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "for hidden activation=Relu, hidden layers = [32,32], optimizer=adam\nVary the learning rate via a grid search pattern\nPlot training and validation error as a function of the learning rate\nRepeat this exercise for both random and Xavier weight initialization\n\nDue to the long runtimes, I will only do grid search of learning rate 0.001, 0.01, 0.1.\n\n# model with random weight\ndef model_fn_random_init():\n    return NeuralNetwork(input_size=28*28, layer_sizes=[32, 32], activation_funcs=['relu', 'relu'], dropout_rate=0.5, weight_init='random')\n# model with xavier weight\ndef model_fn_xavier_init():\n    return NeuralNetwork(input_size=28*28, layer_sizes=[32, 32], activation_funcs=['relu', 'relu'], dropout_rate=0.5, weight_init='xavier')\n\n\ndef mod_by_lr(learning_rates, k_folds, dataset, model_fn, loss_fn, optimizer_name, l1_reg, l2_reg, num_epochs=100, device='cpu', patience=5):\n    lr_performance = {'lr': [], 'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n    \n    for lr in learning_rates:\n        print(f\"Training with learning rate: {lr}\")\n        val_loss, val_accuracy, train_loss = training_model_kfold(k_folds=k_folds, dataset=dataset, model_fn=model_fn, loss_fn=loss_fn, optimizer_name=optimizer_name, lr=lr, l1_reg=l1_reg, l2_reg=l2_reg, num_epochs=num_epochs, device=device, plot=False, patience=patience)\n        \n        lr_performance['lr'].append(lr)\n        lr_performance['train_loss'].append(train_loss)\n        lr_performance['val_loss'].append(val_loss)\n        lr_performance['val_accuracy'].append(val_accuracy)\n    \n    # Plotting performance as a function of learning rate\n    plt.figure(figsize=(18, 6))\n    \n    plt.subplot(1, 3, 1)\n    plt.plot(lr_performance['lr'], lr_performance['train_loss'], marker='o', linestyle='-', label='Training Loss')\n    plt.title('Training Loss vs. Learning Rate')\n    plt.xlabel('Learning Rate')\n    plt.ylabel('Training Loss')\n    plt.xscale('log')\n    plt.legend()\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(lr_performance['lr'], lr_performance['val_loss'], marker='o', linestyle='-', label='Validation Loss')\n    plt.title('Validation Loss vs. Learning Rate')\n    plt.xlabel('Learning Rate')\n    plt.ylabel('Validation Loss')\n    plt.xscale('log')\n    plt.legend()\n    \n    plt.subplot(1, 3, 3)\n    plt.plot(lr_performance['lr'], lr_performance['val_accuracy'], marker='o', linestyle='-', label='Validation Accuracy')\n    plt.title('Validation Accuracy vs. Learning Rate')\n    plt.xlabel('Learning Rate')\n    plt.ylabel('Validation Accuracy')\n    plt.xscale('log')\n    plt.legend()\n    \n    plt.show()\n\n\nlearning_rates = [0.1, 0.01, 0.001]\n\nprint(\"Experiment with Random Initialization\")\nmod_by_lr(learning_rates, k_folds=4, dataset=training_data, model_fn=model_fn_random_init, loss_fn=nn.CrossEntropyLoss(), optimizer_name=\"Adam\", l1_reg=0, l2_reg=0, num_epochs=100, device='cpu', patience=5)\n\nprint(\"Experiment with Xavier Initialization\")\nmod_by_lr(learning_rates, k_folds=4, dataset=training_data, model_fn=model_fn_xavier_init, loss_fn=nn.CrossEntropyLoss(), optimizer_name=\"Adam\", l1_reg=0, l2_reg=0, num_epochs=100, device='cpu', patience=5)\n\nExperiment with Random Initialization\nTraining with learning rate: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0375, Val Loss: 2.3139, Val Accuracy: 0.0960\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3122, Val Accuracy: 0.0971\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3053, Val Accuracy: 0.0988\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3100, Val Accuracy: 0.1036\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3114, Val Accuracy: 0.0991\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3119, Val Accuracy: 0.0995\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3113, Val Accuracy: 0.1015\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0368, Val Loss: 2.3172, Val Accuracy: 0.1008\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3112, Val Accuracy: 0.1008\nEpoch 3/100 - Loss: 0.0363, Val Loss: 2.3100, Val Accuracy: 0.0991\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3174, Val Accuracy: 0.0991\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3115, Val Accuracy: 0.0973\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3116, Val Accuracy: 0.0987\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3074, Val Accuracy: 0.0985\nEpoch 8/100 - Loss: 0.0362, Val Loss: 2.3078, Val Accuracy: 0.0973\nEpoch 9/100 - Loss: 0.0362, Val Loss: 2.3096, Val Accuracy: 0.1015\nEpoch 10/100 - Loss: 0.0362, Val Loss: 2.3066, Val Accuracy: 0.0985\nEpoch 11/100 - Loss: 0.0362, Val Loss: 2.3146, Val Accuracy: 0.1015\nEpoch 12/100 - Loss: 0.0362, Val Loss: 2.3119, Val Accuracy: 0.0985\nEpoch 13/100 - Loss: 0.0362, Val Loss: 2.3114, Val Accuracy: 0.1015\nEpoch 14/100 - Loss: 0.0362, Val Loss: 2.3199, Val Accuracy: 0.0991\nStopping early at epoch 15\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0366, Val Loss: 2.3086, Val Accuracy: 0.0961\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3131, Val Accuracy: 0.0969\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3067, Val Accuracy: 0.1041\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3071, Val Accuracy: 0.0993\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3115, Val Accuracy: 0.1059\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3172, Val Accuracy: 0.0977\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3075, Val Accuracy: 0.1059\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0365, Val Loss: 2.3234, Val Accuracy: 0.1025\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3112, Val Accuracy: 0.1009\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3064, Val Accuracy: 0.1003\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3186, Val Accuracy: 0.0990\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3068, Val Accuracy: 0.0997\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3054, Val Accuracy: 0.0985\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3049, Val Accuracy: 0.0993\nEpoch 8/100 - Loss: 0.0362, Val Loss: 2.3061, Val Accuracy: 0.1002\nEpoch 9/100 - Loss: 0.0362, Val Loss: 2.3087, Val Accuracy: 0.0985\nEpoch 10/100 - Loss: 0.0362, Val Loss: 2.3087, Val Accuracy: 0.0993\nEpoch 11/100 - Loss: 0.0362, Val Loss: 2.3072, Val Accuracy: 0.1003\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.305333677251288, Validation Accuracy = 0.0988\nFold 1: Validation Loss = 2.3066133042599293, Validation Accuracy = 0.09853333333333333\nFold 2: Validation Loss = 2.3067821360648946, Validation Accuracy = 0.10413333333333333\nFold 3: Validation Loss = 2.3049005062022108, Validation Accuracy = 0.09926666666666667\nAverage Validation Loss across folds: 2.3059074059445805\nAverage Validation Accuracy across folds: 0.10018333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with learning rate: 0.01\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0197, Val Loss: 0.7694, Val Accuracy: 0.7041\nEpoch 2/100 - Loss: 0.0180, Val Loss: 0.7087, Val Accuracy: 0.7120\nEpoch 3/100 - Loss: 0.0177, Val Loss: 0.7038, Val Accuracy: 0.7156\nEpoch 4/100 - Loss: 0.0177, Val Loss: 0.6906, Val Accuracy: 0.7364\nEpoch 5/100 - Loss: 0.0179, Val Loss: 0.6841, Val Accuracy: 0.7288\nEpoch 6/100 - Loss: 0.0177, Val Loss: 0.6859, Val Accuracy: 0.7368\nEpoch 7/100 - Loss: 0.0174, Val Loss: 0.6665, Val Accuracy: 0.7345\nEpoch 8/100 - Loss: 0.0176, Val Loss: 0.6815, Val Accuracy: 0.7205\nEpoch 9/100 - Loss: 0.0178, Val Loss: 0.6759, Val Accuracy: 0.7392\nEpoch 10/100 - Loss: 0.0178, Val Loss: 0.7153, Val Accuracy: 0.7173\nEpoch 11/100 - Loss: 0.0189, Val Loss: 0.7043, Val Accuracy: 0.7271\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0196, Val Loss: 0.7754, Val Accuracy: 0.7030\nEpoch 2/100 - Loss: 0.0181, Val Loss: 0.7947, Val Accuracy: 0.7236\nEpoch 3/100 - Loss: 0.0179, Val Loss: 0.7320, Val Accuracy: 0.7219\nEpoch 4/100 - Loss: 0.0178, Val Loss: 0.7243, Val Accuracy: 0.7330\nEpoch 5/100 - Loss: 0.0185, Val Loss: 0.8466, Val Accuracy: 0.6280\nEpoch 6/100 - Loss: 0.0182, Val Loss: 0.8026, Val Accuracy: 0.6901\nEpoch 7/100 - Loss: 0.0180, Val Loss: 0.7515, Val Accuracy: 0.7240\nEpoch 8/100 - Loss: 0.0181, Val Loss: 0.7614, Val Accuracy: 0.7084\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0184, Val Loss: 0.7341, Val Accuracy: 0.7186\nEpoch 2/100 - Loss: 0.0166, Val Loss: 0.7014, Val Accuracy: 0.7171\nEpoch 3/100 - Loss: 0.0166, Val Loss: 0.6950, Val Accuracy: 0.7187\nEpoch 4/100 - Loss: 0.0164, Val Loss: 0.7151, Val Accuracy: 0.7229\nEpoch 5/100 - Loss: 0.0163, Val Loss: 0.6356, Val Accuracy: 0.7763\nEpoch 6/100 - Loss: 0.0162, Val Loss: 0.6856, Val Accuracy: 0.7225\nEpoch 7/100 - Loss: 0.0163, Val Loss: 0.6639, Val Accuracy: 0.7247\nEpoch 8/100 - Loss: 0.0162, Val Loss: 0.6852, Val Accuracy: 0.7105\nEpoch 9/100 - Loss: 0.0162, Val Loss: 0.6635, Val Accuracy: 0.7351\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0200, Val Loss: 0.7814, Val Accuracy: 0.7161\nEpoch 2/100 - Loss: 0.0183, Val Loss: 0.8294, Val Accuracy: 0.7033\nEpoch 3/100 - Loss: 0.0184, Val Loss: 0.8536, Val Accuracy: 0.6405\nEpoch 4/100 - Loss: 0.0189, Val Loss: 0.8221, Val Accuracy: 0.7103\nEpoch 5/100 - Loss: 0.0185, Val Loss: 0.7859, Val Accuracy: 0.7043\nEpoch 6/100 - Loss: 0.0184, Val Loss: 0.7678, Val Accuracy: 0.7309\nEpoch 7/100 - Loss: 0.0184, Val Loss: 0.8313, Val Accuracy: 0.6947\nEpoch 8/100 - Loss: 0.0183, Val Loss: 0.8306, Val Accuracy: 0.6425\nEpoch 9/100 - Loss: 0.0182, Val Loss: 0.8218, Val Accuracy: 0.6655\nEpoch 10/100 - Loss: 0.0181, Val Loss: 0.8310, Val Accuracy: 0.6519\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.6665161415617517, Validation Accuracy = 0.7345333333333334\nFold 1: Validation Loss = 0.7249018516946346, Validation Accuracy = 0.733\nFold 2: Validation Loss = 0.635443138442141, Validation Accuracy = 0.7762666666666667\nFold 3: Validation Loss = 0.7673437339194278, Validation Accuracy = 0.7309333333333333\nAverage Validation Loss across folds: 0.6985512164044887\nAverage Validation Accuracy across folds: 0.7436833333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with learning rate: 0.001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0213, Val Loss: 0.7794, Val Accuracy: 0.7019\nEpoch 2/100 - Loss: 0.0160, Val Loss: 0.6762, Val Accuracy: 0.7703\nEpoch 3/100 - Loss: 0.0146, Val Loss: 0.6215, Val Accuracy: 0.7928\nEpoch 4/100 - Loss: 0.0141, Val Loss: 0.5870, Val Accuracy: 0.8017\nEpoch 5/100 - Loss: 0.0136, Val Loss: 0.5717, Val Accuracy: 0.8181\nEpoch 6/100 - Loss: 0.0135, Val Loss: 0.5628, Val Accuracy: 0.8097\nEpoch 7/100 - Loss: 0.0132, Val Loss: 0.5438, Val Accuracy: 0.8243\nEpoch 8/100 - Loss: 0.0132, Val Loss: 0.5335, Val Accuracy: 0.8258\nEpoch 9/100 - Loss: 0.0130, Val Loss: 0.5272, Val Accuracy: 0.8285\nEpoch 10/100 - Loss: 0.0129, Val Loss: 0.5298, Val Accuracy: 0.8275\nEpoch 11/100 - Loss: 0.0128, Val Loss: 0.5225, Val Accuracy: 0.8272\nEpoch 12/100 - Loss: 0.0127, Val Loss: 0.5090, Val Accuracy: 0.8299\nEpoch 13/100 - Loss: 0.0127, Val Loss: 0.5103, Val Accuracy: 0.8273\nEpoch 14/100 - Loss: 0.0127, Val Loss: 0.5275, Val Accuracy: 0.8330\nEpoch 15/100 - Loss: 0.0125, Val Loss: 0.5145, Val Accuracy: 0.8334\nEpoch 16/100 - Loss: 0.0123, Val Loss: 0.5000, Val Accuracy: 0.8321\nEpoch 17/100 - Loss: 0.0124, Val Loss: 0.5097, Val Accuracy: 0.8330\nEpoch 18/100 - Loss: 0.0123, Val Loss: 0.5105, Val Accuracy: 0.8307\nEpoch 19/100 - Loss: 0.0123, Val Loss: 0.5074, Val Accuracy: 0.8313\nEpoch 20/100 - Loss: 0.0124, Val Loss: 0.4910, Val Accuracy: 0.8382\nEpoch 21/100 - Loss: 0.0122, Val Loss: 0.4925, Val Accuracy: 0.8323\nEpoch 22/100 - Loss: 0.0121, Val Loss: 0.4898, Val Accuracy: 0.8343\nEpoch 23/100 - Loss: 0.0121, Val Loss: 0.4786, Val Accuracy: 0.8345\nEpoch 24/100 - Loss: 0.0121, Val Loss: 0.4852, Val Accuracy: 0.8324\nEpoch 25/100 - Loss: 0.0121, Val Loss: 0.4854, Val Accuracy: 0.8381\nEpoch 26/100 - Loss: 0.0120, Val Loss: 0.4824, Val Accuracy: 0.8376\nEpoch 27/100 - Loss: 0.0120, Val Loss: 0.4797, Val Accuracy: 0.8373\nStopping early at epoch 28\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0212, Val Loss: 0.7428, Val Accuracy: 0.7428\nEpoch 2/100 - Loss: 0.0155, Val Loss: 0.6410, Val Accuracy: 0.7874\nEpoch 3/100 - Loss: 0.0144, Val Loss: 0.5818, Val Accuracy: 0.7929\nEpoch 4/100 - Loss: 0.0138, Val Loss: 0.5640, Val Accuracy: 0.8121\nEpoch 5/100 - Loss: 0.0135, Val Loss: 0.5499, Val Accuracy: 0.8007\nEpoch 6/100 - Loss: 0.0133, Val Loss: 0.5404, Val Accuracy: 0.8206\nEpoch 7/100 - Loss: 0.0130, Val Loss: 0.5248, Val Accuracy: 0.8154\nEpoch 8/100 - Loss: 0.0130, Val Loss: 0.5168, Val Accuracy: 0.8173\nEpoch 9/100 - Loss: 0.0129, Val Loss: 0.5306, Val Accuracy: 0.8192\nEpoch 10/100 - Loss: 0.0127, Val Loss: 0.5148, Val Accuracy: 0.8206\nEpoch 11/100 - Loss: 0.0127, Val Loss: 0.5056, Val Accuracy: 0.8115\nEpoch 12/100 - Loss: 0.0125, Val Loss: 0.5066, Val Accuracy: 0.8325\nEpoch 13/100 - Loss: 0.0124, Val Loss: 0.4951, Val Accuracy: 0.8326\nEpoch 14/100 - Loss: 0.0125, Val Loss: 0.4886, Val Accuracy: 0.8371\nEpoch 15/100 - Loss: 0.0124, Val Loss: 0.4863, Val Accuracy: 0.8361\nEpoch 16/100 - Loss: 0.0122, Val Loss: 0.5080, Val Accuracy: 0.8272\nEpoch 17/100 - Loss: 0.0122, Val Loss: 0.4951, Val Accuracy: 0.8366\nEpoch 18/100 - Loss: 0.0122, Val Loss: 0.4971, Val Accuracy: 0.8367\nEpoch 19/100 - Loss: 0.0123, Val Loss: 0.4961, Val Accuracy: 0.8303\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0199, Val Loss: 0.7251, Val Accuracy: 0.7438\nEpoch 2/100 - Loss: 0.0146, Val Loss: 0.6198, Val Accuracy: 0.7737\nEpoch 3/100 - Loss: 0.0133, Val Loss: 0.5596, Val Accuracy: 0.7913\nEpoch 4/100 - Loss: 0.0128, Val Loss: 0.5538, Val Accuracy: 0.8020\nEpoch 5/100 - Loss: 0.0124, Val Loss: 0.5378, Val Accuracy: 0.8129\nEpoch 6/100 - Loss: 0.0121, Val Loss: 0.5186, Val Accuracy: 0.8220\nEpoch 7/100 - Loss: 0.0119, Val Loss: 0.5089, Val Accuracy: 0.8207\nEpoch 8/100 - Loss: 0.0118, Val Loss: 0.5079, Val Accuracy: 0.8263\nEpoch 9/100 - Loss: 0.0116, Val Loss: 0.5013, Val Accuracy: 0.8253\nEpoch 10/100 - Loss: 0.0115, Val Loss: 0.4925, Val Accuracy: 0.8273\nEpoch 11/100 - Loss: 0.0115, Val Loss: 0.4844, Val Accuracy: 0.8345\nEpoch 12/100 - Loss: 0.0113, Val Loss: 0.4886, Val Accuracy: 0.8326\nEpoch 13/100 - Loss: 0.0112, Val Loss: 0.4974, Val Accuracy: 0.8256\nEpoch 14/100 - Loss: 0.0112, Val Loss: 0.4757, Val Accuracy: 0.8355\nEpoch 15/100 - Loss: 0.0112, Val Loss: 0.4862, Val Accuracy: 0.8361\nEpoch 16/100 - Loss: 0.0111, Val Loss: 0.4737, Val Accuracy: 0.8347\nEpoch 17/100 - Loss: 0.0111, Val Loss: 0.4921, Val Accuracy: 0.8271\nEpoch 18/100 - Loss: 0.0111, Val Loss: 0.4642, Val Accuracy: 0.8410\nEpoch 19/100 - Loss: 0.0109, Val Loss: 0.4817, Val Accuracy: 0.8333\nEpoch 20/100 - Loss: 0.0110, Val Loss: 0.4765, Val Accuracy: 0.8347\nEpoch 21/100 - Loss: 0.0108, Val Loss: 0.4763, Val Accuracy: 0.8377\nEpoch 22/100 - Loss: 0.0108, Val Loss: 0.4655, Val Accuracy: 0.8401\nStopping early at epoch 23\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0191, Val Loss: 0.7063, Val Accuracy: 0.7469\nEpoch 2/100 - Loss: 0.0141, Val Loss: 0.6135, Val Accuracy: 0.7872\nEpoch 3/100 - Loss: 0.0129, Val Loss: 0.5457, Val Accuracy: 0.8214\nEpoch 4/100 - Loss: 0.0121, Val Loss: 0.5253, Val Accuracy: 0.8118\nEpoch 5/100 - Loss: 0.0119, Val Loss: 0.4931, Val Accuracy: 0.8302\nEpoch 6/100 - Loss: 0.0115, Val Loss: 0.4812, Val Accuracy: 0.8321\nEpoch 7/100 - Loss: 0.0114, Val Loss: 0.4634, Val Accuracy: 0.8399\nEpoch 8/100 - Loss: 0.0112, Val Loss: 0.4687, Val Accuracy: 0.8327\nEpoch 9/100 - Loss: 0.0110, Val Loss: 0.4562, Val Accuracy: 0.8350\nEpoch 10/100 - Loss: 0.0109, Val Loss: 0.4555, Val Accuracy: 0.8447\nEpoch 11/100 - Loss: 0.0109, Val Loss: 0.4520, Val Accuracy: 0.8415\nEpoch 12/100 - Loss: 0.0107, Val Loss: 0.4518, Val Accuracy: 0.8437\nEpoch 13/100 - Loss: 0.0108, Val Loss: 0.4586, Val Accuracy: 0.8479\nEpoch 14/100 - Loss: 0.0107, Val Loss: 0.4420, Val Accuracy: 0.8450\nEpoch 15/100 - Loss: 0.0106, Val Loss: 0.4505, Val Accuracy: 0.8434\nEpoch 16/100 - Loss: 0.0105, Val Loss: 0.4423, Val Accuracy: 0.8504\nEpoch 17/100 - Loss: 0.0105, Val Loss: 0.4412, Val Accuracy: 0.8477\nEpoch 18/100 - Loss: 0.0104, Val Loss: 0.4388, Val Accuracy: 0.8482\nEpoch 19/100 - Loss: 0.0104, Val Loss: 0.4407, Val Accuracy: 0.8523\nEpoch 20/100 - Loss: 0.0103, Val Loss: 0.4444, Val Accuracy: 0.8513\nEpoch 21/100 - Loss: 0.0104, Val Loss: 0.4329, Val Accuracy: 0.8475\nEpoch 22/100 - Loss: 0.0103, Val Loss: 0.4463, Val Accuracy: 0.8519\nEpoch 23/100 - Loss: 0.0103, Val Loss: 0.4367, Val Accuracy: 0.8483\nEpoch 24/100 - Loss: 0.0102, Val Loss: 0.4417, Val Accuracy: 0.8549\nEpoch 25/100 - Loss: 0.0102, Val Loss: 0.4456, Val Accuracy: 0.8462\nStopping early at epoch 26\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.478112856504765, Validation Accuracy = 0.8344666666666667\nFold 1: Validation Loss = 0.4862562452224975, Validation Accuracy = 0.8361333333333333\nFold 2: Validation Loss = 0.46516420929989916, Validation Accuracy = 0.841\nFold 3: Validation Loss = 0.4339830860178521, Validation Accuracy = 0.8474666666666667\nAverage Validation Loss across folds: 0.4658790992612534\nAverage Validation Accuracy across folds: 0.8397666666666666\n--------------------------------\nK-fold Cross-validation completed.\n\n\n\n\n\n\n\n\n\nExperiment with Xavier Initialization\nTraining with learning rate: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0370, Val Loss: 2.3088, Val Accuracy: 0.0997\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3065, Val Accuracy: 0.1003\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3103, Val Accuracy: 0.0984\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3067, Val Accuracy: 0.1007\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3086, Val Accuracy: 0.1037\nEpoch 6/100 - Loss: 0.0361, Val Loss: 2.3135, Val Accuracy: 0.0984\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0371, Val Loss: 2.3130, Val Accuracy: 0.1010\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3127, Val Accuracy: 0.0974\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3070, Val Accuracy: 0.1010\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3063, Val Accuracy: 0.0974\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3164, Val Accuracy: 0.0949\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3096, Val Accuracy: 0.1015\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3084, Val Accuracy: 0.0986\nEpoch 8/100 - Loss: 0.0362, Val Loss: 2.3078, Val Accuracy: 0.0949\nEpoch 9/100 - Loss: 0.0362, Val Loss: 2.3061, Val Accuracy: 0.1035\nEpoch 10/100 - Loss: 0.0362, Val Loss: 2.3066, Val Accuracy: 0.1037\nEpoch 11/100 - Loss: 0.0362, Val Loss: 2.3202, Val Accuracy: 0.0986\nEpoch 12/100 - Loss: 0.0362, Val Loss: 2.3167, Val Accuracy: 0.0949\nEpoch 13/100 - Loss: 0.0362, Val Loss: 2.3158, Val Accuracy: 0.1035\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0368, Val Loss: 2.3155, Val Accuracy: 0.0977\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3132, Val Accuracy: 0.0997\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3088, Val Accuracy: 0.0975\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3116, Val Accuracy: 0.0997\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3138, Val Accuracy: 0.0997\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3159, Val Accuracy: 0.0977\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3096, Val Accuracy: 0.1001\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0376, Val Loss: 2.3100, Val Accuracy: 0.1005\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3127, Val Accuracy: 0.1008\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3076, Val Accuracy: 0.1034\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3041, Val Accuracy: 0.0969\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3182, Val Accuracy: 0.1008\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3133, Val Accuracy: 0.1008\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3103, Val Accuracy: 0.0969\nEpoch 8/100 - Loss: 0.0362, Val Loss: 2.3205, Val Accuracy: 0.0997\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.30643939566105, Validation Accuracy = 0.10033333333333333\nFold 1: Validation Loss = 2.306070342976996, Validation Accuracy = 0.10353333333333334\nFold 2: Validation Loss = 2.3087029294764743, Validation Accuracy = 0.09746666666666666\nFold 3: Validation Loss = 2.3041324899551716, Validation Accuracy = 0.09686666666666667\nAverage Validation Loss across folds: 2.3063362895174233\nAverage Validation Accuracy across folds: 0.09955\n--------------------------------\nK-fold Cross-validation completed.\nTraining with learning rate: 0.01\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0272, Val Loss: 1.2507, Val Accuracy: 0.5132\nEpoch 2/100 - Loss: 0.0250, Val Loss: 1.2151, Val Accuracy: 0.5541\nEpoch 3/100 - Loss: 0.0245, Val Loss: 1.2594, Val Accuracy: 0.5125\nEpoch 4/100 - Loss: 0.0245, Val Loss: 1.2815, Val Accuracy: 0.5239\nEpoch 5/100 - Loss: 0.0244, Val Loss: 1.1889, Val Accuracy: 0.5364\nEpoch 6/100 - Loss: 0.0245, Val Loss: 1.2305, Val Accuracy: 0.4761\nEpoch 7/100 - Loss: 0.0245, Val Loss: 1.2703, Val Accuracy: 0.4694\nEpoch 8/100 - Loss: 0.0244, Val Loss: 1.2046, Val Accuracy: 0.5203\nEpoch 9/100 - Loss: 0.0246, Val Loss: 1.2656, Val Accuracy: 0.4925\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0218, Val Loss: 0.9223, Val Accuracy: 0.6578\nEpoch 2/100 - Loss: 0.0196, Val Loss: 0.8823, Val Accuracy: 0.6870\nEpoch 3/100 - Loss: 0.0197, Val Loss: 0.8351, Val Accuracy: 0.7055\nEpoch 4/100 - Loss: 0.0194, Val Loss: 0.8782, Val Accuracy: 0.6890\nEpoch 5/100 - Loss: 0.0190, Val Loss: 0.7941, Val Accuracy: 0.6991\nEpoch 6/100 - Loss: 0.0192, Val Loss: 0.9430, Val Accuracy: 0.6506\nEpoch 7/100 - Loss: 0.0195, Val Loss: 0.8411, Val Accuracy: 0.7061\nEpoch 8/100 - Loss: 0.0198, Val Loss: 0.9245, Val Accuracy: 0.6631\nEpoch 9/100 - Loss: 0.0197, Val Loss: 0.9370, Val Accuracy: 0.6711\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0267, Val Loss: 1.1478, Val Accuracy: 0.5735\nEpoch 2/100 - Loss: 0.0241, Val Loss: 1.2049, Val Accuracy: 0.5738\nEpoch 3/100 - Loss: 0.0241, Val Loss: 1.1905, Val Accuracy: 0.5226\nEpoch 4/100 - Loss: 0.0239, Val Loss: 1.1609, Val Accuracy: 0.5009\nEpoch 5/100 - Loss: 0.0238, Val Loss: 1.1418, Val Accuracy: 0.5225\nEpoch 6/100 - Loss: 0.0238, Val Loss: 1.2251, Val Accuracy: 0.4369\nEpoch 7/100 - Loss: 0.0237, Val Loss: 1.1442, Val Accuracy: 0.5824\nEpoch 8/100 - Loss: 0.0239, Val Loss: 1.2387, Val Accuracy: 0.4830\nEpoch 9/100 - Loss: 0.0240, Val Loss: 1.1191, Val Accuracy: 0.5680\nEpoch 10/100 - Loss: 0.0242, Val Loss: 1.2084, Val Accuracy: 0.5136\nEpoch 11/100 - Loss: 0.0245, Val Loss: 1.1913, Val Accuracy: 0.4669\nEpoch 12/100 - Loss: 0.0255, Val Loss: 1.2048, Val Accuracy: 0.4927\nEpoch 13/100 - Loss: 0.0249, Val Loss: 1.1901, Val Accuracy: 0.4983\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0248, Val Loss: 1.1363, Val Accuracy: 0.5461\nEpoch 2/100 - Loss: 0.0234, Val Loss: 1.0632, Val Accuracy: 0.6339\nEpoch 3/100 - Loss: 0.0232, Val Loss: 1.1166, Val Accuracy: 0.5908\nEpoch 4/100 - Loss: 0.0231, Val Loss: 1.0342, Val Accuracy: 0.5953\nEpoch 5/100 - Loss: 0.0235, Val Loss: 1.0623, Val Accuracy: 0.5959\nEpoch 6/100 - Loss: 0.0233, Val Loss: 1.1010, Val Accuracy: 0.5499\nEpoch 7/100 - Loss: 0.0234, Val Loss: 1.1036, Val Accuracy: 0.5804\nEpoch 8/100 - Loss: 0.0234, Val Loss: 1.1087, Val Accuracy: 0.5259\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.1890143785070866, Validation Accuracy = 0.5364\nFold 1: Validation Loss = 0.7943653591135715, Validation Accuracy = 0.6990666666666666\nFold 2: Validation Loss = 1.1188485064405076, Validation Accuracy = 0.568\nFold 3: Validation Loss = 1.0350262459288253, Validation Accuracy = 0.5952666666666667\nAverage Validation Loss across folds: 1.0343136224974978\nAverage Validation Accuracy across folds: 0.5996833333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with learning rate: 0.001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0219, Val Loss: 0.7690, Val Accuracy: 0.7498\nEpoch 2/100 - Loss: 0.0162, Val Loss: 0.6561, Val Accuracy: 0.7862\nEpoch 3/100 - Loss: 0.0145, Val Loss: 0.5931, Val Accuracy: 0.7746\nEpoch 4/100 - Loss: 0.0137, Val Loss: 0.5575, Val Accuracy: 0.8181\nEpoch 5/100 - Loss: 0.0133, Val Loss: 0.5379, Val Accuracy: 0.8130\nEpoch 6/100 - Loss: 0.0130, Val Loss: 0.5280, Val Accuracy: 0.8316\nEpoch 7/100 - Loss: 0.0129, Val Loss: 0.5334, Val Accuracy: 0.8199\nEpoch 8/100 - Loss: 0.0126, Val Loss: 0.5245, Val Accuracy: 0.8323\nEpoch 9/100 - Loss: 0.0125, Val Loss: 0.5097, Val Accuracy: 0.8277\nEpoch 10/100 - Loss: 0.0124, Val Loss: 0.5209, Val Accuracy: 0.8250\nEpoch 11/100 - Loss: 0.0122, Val Loss: 0.5212, Val Accuracy: 0.8207\nEpoch 12/100 - Loss: 0.0122, Val Loss: 0.5024, Val Accuracy: 0.8366\nEpoch 13/100 - Loss: 0.0120, Val Loss: 0.5132, Val Accuracy: 0.8293\nEpoch 14/100 - Loss: 0.0119, Val Loss: 0.4909, Val Accuracy: 0.8427\nEpoch 15/100 - Loss: 0.0120, Val Loss: 0.4795, Val Accuracy: 0.8416\nEpoch 16/100 - Loss: 0.0118, Val Loss: 0.4961, Val Accuracy: 0.8362\nEpoch 17/100 - Loss: 0.0118, Val Loss: 0.4867, Val Accuracy: 0.8391\nEpoch 18/100 - Loss: 0.0118, Val Loss: 0.4960, Val Accuracy: 0.8406\nEpoch 19/100 - Loss: 0.0118, Val Loss: 0.4903, Val Accuracy: 0.8371\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0212, Val Loss: 0.7023, Val Accuracy: 0.7710\nEpoch 2/100 - Loss: 0.0148, Val Loss: 0.6000, Val Accuracy: 0.7988\nEpoch 3/100 - Loss: 0.0136, Val Loss: 0.5647, Val Accuracy: 0.8114\nEpoch 4/100 - Loss: 0.0128, Val Loss: 0.5440, Val Accuracy: 0.8191\nEpoch 5/100 - Loss: 0.0126, Val Loss: 0.5415, Val Accuracy: 0.8229\nEpoch 6/100 - Loss: 0.0123, Val Loss: 0.5195, Val Accuracy: 0.8282\nEpoch 7/100 - Loss: 0.0122, Val Loss: 0.5167, Val Accuracy: 0.8262\nEpoch 8/100 - Loss: 0.0118, Val Loss: 0.5181, Val Accuracy: 0.8315\nEpoch 9/100 - Loss: 0.0117, Val Loss: 0.5085, Val Accuracy: 0.8348\nEpoch 10/100 - Loss: 0.0117, Val Loss: 0.5120, Val Accuracy: 0.8226\nEpoch 11/100 - Loss: 0.0115, Val Loss: 0.5054, Val Accuracy: 0.8397\nEpoch 12/100 - Loss: 0.0115, Val Loss: 0.5103, Val Accuracy: 0.8407\nEpoch 13/100 - Loss: 0.0113, Val Loss: 0.5074, Val Accuracy: 0.8384\nEpoch 14/100 - Loss: 0.0112, Val Loss: 0.5168, Val Accuracy: 0.8376\nEpoch 15/100 - Loss: 0.0111, Val Loss: 0.4758, Val Accuracy: 0.8450\nEpoch 16/100 - Loss: 0.0110, Val Loss: 0.4839, Val Accuracy: 0.8459\nEpoch 17/100 - Loss: 0.0110, Val Loss: 0.4874, Val Accuracy: 0.8403\nEpoch 18/100 - Loss: 0.0109, Val Loss: 0.4852, Val Accuracy: 0.8481\nEpoch 19/100 - Loss: 0.0109, Val Loss: 0.4789, Val Accuracy: 0.8421\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0205, Val Loss: 0.6842, Val Accuracy: 0.7673\nEpoch 2/100 - Loss: 0.0147, Val Loss: 0.6239, Val Accuracy: 0.7797\nEpoch 3/100 - Loss: 0.0136, Val Loss: 0.5803, Val Accuracy: 0.7841\nEpoch 4/100 - Loss: 0.0130, Val Loss: 0.5655, Val Accuracy: 0.7883\nEpoch 5/100 - Loss: 0.0127, Val Loss: 0.5443, Val Accuracy: 0.8005\nEpoch 6/100 - Loss: 0.0124, Val Loss: 0.5437, Val Accuracy: 0.8035\nEpoch 7/100 - Loss: 0.0122, Val Loss: 0.5526, Val Accuracy: 0.8227\nEpoch 8/100 - Loss: 0.0121, Val Loss: 0.5198, Val Accuracy: 0.8071\nEpoch 9/100 - Loss: 0.0119, Val Loss: 0.5320, Val Accuracy: 0.8066\nEpoch 10/100 - Loss: 0.0118, Val Loss: 0.5368, Val Accuracy: 0.8047\nEpoch 11/100 - Loss: 0.0117, Val Loss: 0.5429, Val Accuracy: 0.7940\nEpoch 12/100 - Loss: 0.0116, Val Loss: 0.5239, Val Accuracy: 0.8072\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0219, Val Loss: 0.7404, Val Accuracy: 0.7503\nEpoch 2/100 - Loss: 0.0160, Val Loss: 0.6427, Val Accuracy: 0.7707\nEpoch 3/100 - Loss: 0.0145, Val Loss: 0.6098, Val Accuracy: 0.7887\nEpoch 4/100 - Loss: 0.0138, Val Loss: 0.5662, Val Accuracy: 0.8077\nEpoch 5/100 - Loss: 0.0134, Val Loss: 0.5703, Val Accuracy: 0.7955\nEpoch 6/100 - Loss: 0.0132, Val Loss: 0.5480, Val Accuracy: 0.8167\nEpoch 7/100 - Loss: 0.0130, Val Loss: 0.5384, Val Accuracy: 0.8149\nEpoch 8/100 - Loss: 0.0129, Val Loss: 0.5336, Val Accuracy: 0.8256\nEpoch 9/100 - Loss: 0.0127, Val Loss: 0.5299, Val Accuracy: 0.8171\nEpoch 10/100 - Loss: 0.0126, Val Loss: 0.5287, Val Accuracy: 0.8226\nEpoch 11/100 - Loss: 0.0125, Val Loss: 0.5314, Val Accuracy: 0.8158\nEpoch 12/100 - Loss: 0.0123, Val Loss: 0.5215, Val Accuracy: 0.8193\nEpoch 13/100 - Loss: 0.0123, Val Loss: 0.5284, Val Accuracy: 0.8198\nEpoch 14/100 - Loss: 0.0122, Val Loss: 0.5113, Val Accuracy: 0.8218\nEpoch 15/100 - Loss: 0.0123, Val Loss: 0.5142, Val Accuracy: 0.8226\nEpoch 16/100 - Loss: 0.0122, Val Loss: 0.5308, Val Accuracy: 0.8192\nEpoch 17/100 - Loss: 0.0122, Val Loss: 0.5192, Val Accuracy: 0.8275\nEpoch 18/100 - Loss: 0.0120, Val Loss: 0.5101, Val Accuracy: 0.8285\nEpoch 19/100 - Loss: 0.0120, Val Loss: 0.5422, Val Accuracy: 0.7837\nEpoch 20/100 - Loss: 0.0120, Val Loss: 0.5013, Val Accuracy: 0.8239\nEpoch 21/100 - Loss: 0.0121, Val Loss: 0.5119, Val Accuracy: 0.8227\nEpoch 22/100 - Loss: 0.0119, Val Loss: 0.5096, Val Accuracy: 0.8211\nEpoch 23/100 - Loss: 0.0119, Val Loss: 0.5044, Val Accuracy: 0.8197\nEpoch 24/100 - Loss: 0.0120, Val Loss: 0.4971, Val Accuracy: 0.8251\nEpoch 25/100 - Loss: 0.0118, Val Loss: 0.4978, Val Accuracy: 0.8190\nEpoch 26/100 - Loss: 0.0119, Val Loss: 0.4999, Val Accuracy: 0.8263\nEpoch 27/100 - Loss: 0.0118, Val Loss: 0.5073, Val Accuracy: 0.8205\nEpoch 28/100 - Loss: 0.0118, Val Loss: 0.5056, Val Accuracy: 0.8229\nStopping early at epoch 29\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.47926961602048673, Validation Accuracy = 0.8416\nFold 1: Validation Loss = 0.4755426121518967, Validation Accuracy = 0.845\nFold 2: Validation Loss = 0.5202207646471388, Validation Accuracy = 0.8070666666666667\nFold 3: Validation Loss = 0.49710745380279864, Validation Accuracy = 0.8250666666666666\nAverage Validation Loss across folds: 0.4930351116555802\nAverage Validation Accuracy across folds: 0.8296833333333333\n--------------------------------\nK-fold Cross-validation completed."
  },
  {
    "objectID": "ANN.html#choice-1",
    "href": "ANN.html#choice-1",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "for hidden activation=relu, hidden layers = [64,64], optimizer=adam\nVary L1 and L2 in a 10x10 grid search (without dropout)\nPlot validation and training error as a function of L1 and L2 regularization in a 2D heatmap\nPlot the ratio (or difference) of validation to training error as a function of L1 and L2 regularization in a 2D heatmap\n\n\n\nI will do a 5 by 5 grid search instead of a 10 by 10 grid search as the runtimes are very long and the results are similar. Professor James said that it was okay to do so. The cell not ran shows the code for a 10 by 10 grid search and the second one shows the code for a 5 by 5 grid search.\n\n# 10 by 10 grid search (not run)\n# ranges for L1 and L2\nl1_range = np.logspace(-4, -1, 10)\nl2_range = np.logspace(-4, -1, 10)\n\n# store results\nresults1 = []\n# loop through the grid\nfor l1 in l1_range:\n    for l2 in l2_range:\n        print(f\"Training with L1: {l1}, L2: {l2}\")\n        # train model\n        avg_train_loss, avg_val_loss, avg_val_accuracy = training_model_kfold(\n            model_fn=model_fn,\n            k_folds=4,\n            num_epochs=100,\n            dataset=training_data,  \n            loss_fn=nn.CrossEntropyLoss(),\n            optimizer_name=\"Adam\",\n            lr=0.01,  \n            l1_reg=l1,\n            l2_reg=l2,\n            device=device,\n            patience=4,\n            plot=False)\n        \n        # store results\n        results1.append({\n            'l1': l1,\n            'l2': l2,\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n            'val_accuracy': avg_val_accuracy})\n\n\n# 5 by 5 grid search \n# Define ranges for L1 and L2 \nl1_range = np.logspace(-4, -1, 5) \nl2_range = np.logspace(-4, -1, 5) \n\nresults1 = []\n\nfor l1 in l1_range:\n    for l2 in l2_range:\n        print(f\"Training with L1: {l1}, L2: {l2}\")\n        \n        avg_train_loss, avg_val_loss, avg_val_accuracy = training_model_kfold(\n            model_fn=model_fn,\n            k_folds=2,\n            num_epochs=100,\n            dataset=training_data,\n            loss_fn=nn.CrossEntropyLoss(),\n            optimizer_name=\"Adam\",\n            lr=0.01,  \n            l1_reg=l1,\n            l2_reg=l2,\n            num_epochs=100,\n            device=device,\n            patience=4,\n            plot=False)\n        \n        results1.append({\n            'l1': l1,\n            'l2': l2,\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n            'val_accuracy': avg_val_accuracy})\n\nresults_choice_1 = pd.DataFrame(results1)\nresults_choice_1.to_csv('results1.csv', index=False)\nprint(results_choice_1)\n\nTraining with L1: 0.0001, L2: 0.0001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0380, Val Loss: 0.7429, Val Accuracy: 0.7243\nEpoch 2/100 - Loss: 0.0431, Val Loss: 0.7548, Val Accuracy: 0.6858\nEpoch 3/100 - Loss: 0.0430, Val Loss: 0.7459, Val Accuracy: 0.6887\nEpoch 4/100 - Loss: 0.0460, Val Loss: 0.6938, Val Accuracy: 0.7083\nEpoch 5/100 - Loss: 0.0443, Val Loss: 0.7786, Val Accuracy: 0.6976\nEpoch 6/100 - Loss: 0.0438, Val Loss: 0.9246, Val Accuracy: 0.6295\nEpoch 7/100 - Loss: 0.0459, Val Loss: 0.7116, Val Accuracy: 0.7470\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0377, Val Loss: 0.7237, Val Accuracy: 0.6817\nEpoch 2/100 - Loss: 0.0434, Val Loss: 0.6957, Val Accuracy: 0.7262\nEpoch 3/100 - Loss: 0.0443, Val Loss: 0.7128, Val Accuracy: 0.7239\nEpoch 4/100 - Loss: 0.0447, Val Loss: 0.7870, Val Accuracy: 0.7103\nEpoch 5/100 - Loss: 0.0448, Val Loss: 0.7433, Val Accuracy: 0.7018\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.6938231032667384, Validation Accuracy = 0.7082666666666667\nFold 1: Validation Loss = 0.6957771871516953, Validation Accuracy = 0.7262333333333333\nAverage Validation Loss across folds: 0.6948001452092168\nAverage Validation Accuracy across folds: 0.7172499999999999\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0001, L2: 0.0005623413251903491\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0373, Val Loss: 0.8293, Val Accuracy: 0.6851\nEpoch 2/100 - Loss: 0.0409, Val Loss: 0.8119, Val Accuracy: 0.6841\nEpoch 3/100 - Loss: 0.0417, Val Loss: 0.7265, Val Accuracy: 0.7159\nEpoch 4/100 - Loss: 0.0390, Val Loss: 0.7791, Val Accuracy: 0.6632\nEpoch 5/100 - Loss: 0.0413, Val Loss: 0.7154, Val Accuracy: 0.7070\nEpoch 6/100 - Loss: 0.0405, Val Loss: 0.7622, Val Accuracy: 0.6663\nEpoch 7/100 - Loss: 0.0400, Val Loss: 0.7384, Val Accuracy: 0.7042\nEpoch 8/100 - Loss: 0.0393, Val Loss: 0.7669, Val Accuracy: 0.6933\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0361, Val Loss: 0.8180, Val Accuracy: 0.6528\nEpoch 2/100 - Loss: 0.0394, Val Loss: 0.8629, Val Accuracy: 0.6975\nEpoch 3/100 - Loss: 0.0401, Val Loss: 0.7041, Val Accuracy: 0.7210\nEpoch 4/100 - Loss: 0.0399, Val Loss: 0.8094, Val Accuracy: 0.6811\nEpoch 5/100 - Loss: 0.0393, Val Loss: 0.8211, Val Accuracy: 0.6783\nEpoch 6/100 - Loss: 0.0402, Val Loss: 0.7743, Val Accuracy: 0.6977\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.7153623624524074, Validation Accuracy = 0.7070333333333333\nFold 1: Validation Loss = 0.7041597517568674, Validation Accuracy = 0.7210333333333333\nAverage Validation Loss across folds: 0.7097610571046373\nAverage Validation Accuracy across folds: 0.7140333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0001, L2: 0.0031622776601683794\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0319, Val Loss: 0.8192, Val Accuracy: 0.6723\nEpoch 2/100 - Loss: 0.0319, Val Loss: 0.7745, Val Accuracy: 0.7118\nEpoch 3/100 - Loss: 0.0313, Val Loss: 0.8971, Val Accuracy: 0.6288\nEpoch 4/100 - Loss: 0.0319, Val Loss: 0.8457, Val Accuracy: 0.6531\nEpoch 5/100 - Loss: 0.0304, Val Loss: 0.9356, Val Accuracy: 0.6112\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0318, Val Loss: 0.8225, Val Accuracy: 0.6679\nEpoch 2/100 - Loss: 0.0319, Val Loss: 0.8575, Val Accuracy: 0.6860\nEpoch 3/100 - Loss: 0.0317, Val Loss: 0.9248, Val Accuracy: 0.6869\nEpoch 4/100 - Loss: 0.0310, Val Loss: 0.7672, Val Accuracy: 0.7025\nEpoch 5/100 - Loss: 0.0301, Val Loss: 0.8730, Val Accuracy: 0.6903\nEpoch 6/100 - Loss: 0.0306, Val Loss: 0.8144, Val Accuracy: 0.7002\nEpoch 7/100 - Loss: 0.0315, Val Loss: 0.8718, Val Accuracy: 0.6770\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.7742582643464175, Validation Accuracy = 0.7118\nFold 1: Validation Loss = 0.7671754582604365, Validation Accuracy = 0.7025\nAverage Validation Loss across folds: 0.770716861303427\nAverage Validation Accuracy across folds: 0.70715\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0001, L2: 0.01778279410038923\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0255, Val Loss: 1.0673, Val Accuracy: 0.5599\nEpoch 2/100 - Loss: 0.0242, Val Loss: 0.9324, Val Accuracy: 0.6386\nEpoch 3/100 - Loss: 0.0241, Val Loss: 1.0444, Val Accuracy: 0.5678\nEpoch 4/100 - Loss: 0.0246, Val Loss: 1.0938, Val Accuracy: 0.6010\nEpoch 5/100 - Loss: 0.0236, Val Loss: 1.0484, Val Accuracy: 0.6161\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0257, Val Loss: 1.0057, Val Accuracy: 0.5963\nEpoch 2/100 - Loss: 0.0242, Val Loss: 1.0704, Val Accuracy: 0.5991\nEpoch 3/100 - Loss: 0.0241, Val Loss: 0.9964, Val Accuracy: 0.6420\nEpoch 4/100 - Loss: 0.0237, Val Loss: 0.9338, Val Accuracy: 0.6547\nEpoch 5/100 - Loss: 0.0240, Val Loss: 1.0051, Val Accuracy: 0.6275\nEpoch 6/100 - Loss: 0.0239, Val Loss: 0.9682, Val Accuracy: 0.6570\nEpoch 7/100 - Loss: 0.0235, Val Loss: 0.9504, Val Accuracy: 0.6461\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.9324265236793551, Validation Accuracy = 0.6385666666666666\nFold 1: Validation Loss = 0.9338125421294271, Validation Accuracy = 0.6547\nAverage Validation Loss across folds: 0.9331195329043911\nAverage Validation Accuracy across folds: 0.6466333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0001, L2: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0272, Val Loss: 1.5092, Val Accuracy: 0.3771\nEpoch 2/100 - Loss: 0.0260, Val Loss: 1.5169, Val Accuracy: 0.3841\nEpoch 3/100 - Loss: 0.0263, Val Loss: 1.4177, Val Accuracy: 0.4147\nEpoch 4/100 - Loss: 0.0262, Val Loss: 1.6608, Val Accuracy: 0.2949\nEpoch 5/100 - Loss: 0.0264, Val Loss: 1.6067, Val Accuracy: 0.2931\nEpoch 6/100 - Loss: 0.0262, Val Loss: 1.4282, Val Accuracy: 0.4366\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0268, Val Loss: 1.3539, Val Accuracy: 0.4802\nEpoch 2/100 - Loss: 0.0262, Val Loss: 1.4364, Val Accuracy: 0.4230\nEpoch 3/100 - Loss: 0.0261, Val Loss: 1.4150, Val Accuracy: 0.3777\nEpoch 4/100 - Loss: 0.0261, Val Loss: 1.4633, Val Accuracy: 0.3899\nStopping early at epoch 5\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.417715992500533, Validation Accuracy = 0.4147\nFold 1: Validation Loss = 1.354012314444666, Validation Accuracy = 0.4801666666666667\nAverage Validation Loss across folds: 1.3858641534725995\nAverage Validation Accuracy across folds: 0.44743333333333335\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0005623413251903491, L2: 0.0001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0597, Val Loss: 0.7166, Val Accuracy: 0.7466\nEpoch 2/100 - Loss: 0.0573, Val Loss: 0.8189, Val Accuracy: 0.6860\nEpoch 3/100 - Loss: 0.0552, Val Loss: 0.6643, Val Accuracy: 0.7468\nEpoch 4/100 - Loss: 0.0552, Val Loss: 0.9084, Val Accuracy: 0.6860\nEpoch 5/100 - Loss: 0.0542, Val Loss: 0.7774, Val Accuracy: 0.6888\nEpoch 6/100 - Loss: 0.0571, Val Loss: 0.7181, Val Accuracy: 0.7370\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0587, Val Loss: 0.8142, Val Accuracy: 0.6640\nEpoch 2/100 - Loss: 0.0577, Val Loss: 0.7875, Val Accuracy: 0.7033\nEpoch 3/100 - Loss: 0.0524, Val Loss: 0.8557, Val Accuracy: 0.6715\nEpoch 4/100 - Loss: 0.0533, Val Loss: 0.7621, Val Accuracy: 0.6897\nEpoch 5/100 - Loss: 0.0539, Val Loss: 0.7908, Val Accuracy: 0.6875\nEpoch 6/100 - Loss: 0.0550, Val Loss: 0.7250, Val Accuracy: 0.7192\nEpoch 7/100 - Loss: 0.0562, Val Loss: 0.7617, Val Accuracy: 0.6960\nEpoch 8/100 - Loss: 0.0572, Val Loss: 0.9011, Val Accuracy: 0.6383\nEpoch 9/100 - Loss: 0.0581, Val Loss: 0.7589, Val Accuracy: 0.7078\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.6643354269360174, Validation Accuracy = 0.7468\nFold 1: Validation Loss = 0.7251428825768835, Validation Accuracy = 0.7192\nAverage Validation Loss across folds: 0.6947391547564505\nAverage Validation Accuracy across folds: 0.733\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0005623413251903491, L2: 0.0005623413251903491\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0573, Val Loss: 0.8700, Val Accuracy: 0.6609\nEpoch 2/100 - Loss: 0.0578, Val Loss: 0.8070, Val Accuracy: 0.6868\nEpoch 3/100 - Loss: 0.0540, Val Loss: 0.7440, Val Accuracy: 0.7206\nEpoch 4/100 - Loss: 0.0515, Val Loss: 0.7448, Val Accuracy: 0.7279\nEpoch 5/100 - Loss: 0.0483, Val Loss: 0.8255, Val Accuracy: 0.6714\nEpoch 6/100 - Loss: 0.0518, Val Loss: 0.7378, Val Accuracy: 0.7190\nEpoch 7/100 - Loss: 0.0561, Val Loss: 0.7983, Val Accuracy: 0.7122\nEpoch 8/100 - Loss: 0.0553, Val Loss: 0.8474, Val Accuracy: 0.6588\nEpoch 9/100 - Loss: 0.0552, Val Loss: 0.8446, Val Accuracy: 0.6913\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0569, Val Loss: 0.8322, Val Accuracy: 0.7007\nEpoch 2/100 - Loss: 0.0569, Val Loss: 0.8508, Val Accuracy: 0.6539\nEpoch 3/100 - Loss: 0.0540, Val Loss: 0.7898, Val Accuracy: 0.6883\nEpoch 4/100 - Loss: 0.0533, Val Loss: 0.8368, Val Accuracy: 0.6910\nEpoch 5/100 - Loss: 0.0488, Val Loss: 0.8664, Val Accuracy: 0.6568\nEpoch 6/100 - Loss: 0.0487, Val Loss: 0.8323, Val Accuracy: 0.6581\nEpoch 7/100 - Loss: 0.0538, Val Loss: 0.7611, Val Accuracy: 0.7088\nEpoch 8/100 - Loss: 0.0530, Val Loss: 0.7929, Val Accuracy: 0.6826\nEpoch 9/100 - Loss: 0.0543, Val Loss: 0.8721, Val Accuracy: 0.6600\nEpoch 10/100 - Loss: 0.0525, Val Loss: 0.7877, Val Accuracy: 0.6895\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.7377686382356737, Validation Accuracy = 0.7190333333333333\nFold 1: Validation Loss = 0.7610122365102585, Validation Accuracy = 0.7087666666666667\nAverage Validation Loss across folds: 0.7493904373729661\nAverage Validation Accuracy across folds: 0.7139\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0005623413251903491, L2: 0.0031622776601683794\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0510, Val Loss: 0.8487, Val Accuracy: 0.6952\nEpoch 2/100 - Loss: 0.0450, Val Loss: 0.7704, Val Accuracy: 0.7096\nEpoch 3/100 - Loss: 0.0460, Val Loss: 0.8479, Val Accuracy: 0.6694\nEpoch 4/100 - Loss: 0.0459, Val Loss: 0.9400, Val Accuracy: 0.6460\nEpoch 5/100 - Loss: 0.0461, Val Loss: 0.8966, Val Accuracy: 0.6914\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0504, Val Loss: 0.8173, Val Accuracy: 0.6899\nEpoch 2/100 - Loss: 0.0467, Val Loss: 0.9908, Val Accuracy: 0.6157\nEpoch 3/100 - Loss: 0.0453, Val Loss: 0.7845, Val Accuracy: 0.7304\nEpoch 4/100 - Loss: 0.0449, Val Loss: 0.8695, Val Accuracy: 0.6960\nEpoch 5/100 - Loss: 0.0445, Val Loss: 0.9049, Val Accuracy: 0.6566\nEpoch 6/100 - Loss: 0.0500, Val Loss: 0.8826, Val Accuracy: 0.6867\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.7703188340038633, Validation Accuracy = 0.7096\nFold 1: Validation Loss = 0.7843411015167928, Validation Accuracy = 0.7304\nAverage Validation Loss across folds: 0.777329967760328\nAverage Validation Accuracy across folds: 0.72\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0005623413251903491, L2: 0.01778279410038923\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0399, Val Loss: 1.0123, Val Accuracy: 0.6276\nEpoch 2/100 - Loss: 0.0360, Val Loss: 0.9127, Val Accuracy: 0.6788\nEpoch 3/100 - Loss: 0.0361, Val Loss: 1.0770, Val Accuracy: 0.5827\nEpoch 4/100 - Loss: 0.0353, Val Loss: 1.0203, Val Accuracy: 0.6184\nEpoch 5/100 - Loss: 0.0338, Val Loss: 0.9278, Val Accuracy: 0.6785\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0408, Val Loss: 1.0140, Val Accuracy: 0.6322\nEpoch 2/100 - Loss: 0.0354, Val Loss: 0.9142, Val Accuracy: 0.6569\nEpoch 3/100 - Loss: 0.0365, Val Loss: 1.0175, Val Accuracy: 0.6321\nEpoch 4/100 - Loss: 0.0353, Val Loss: 0.9401, Val Accuracy: 0.6525\nEpoch 5/100 - Loss: 0.0354, Val Loss: 0.9571, Val Accuracy: 0.6831\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.9127078578670396, Validation Accuracy = 0.6787666666666666\nFold 1: Validation Loss = 0.9142541471066505, Validation Accuracy = 0.6569\nAverage Validation Loss across folds: 0.913481002486845\nAverage Validation Accuracy across folds: 0.6678333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0005623413251903491, L2: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0354, Val Loss: 1.5898, Val Accuracy: 0.4001\nEpoch 2/100 - Loss: 0.0336, Val Loss: 1.4679, Val Accuracy: 0.3868\nEpoch 3/100 - Loss: 0.0330, Val Loss: 1.4997, Val Accuracy: 0.4280\nEpoch 4/100 - Loss: 0.0335, Val Loss: 1.6485, Val Accuracy: 0.2848\nEpoch 5/100 - Loss: 0.0338, Val Loss: 1.5371, Val Accuracy: 0.4092\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0350, Val Loss: 1.5837, Val Accuracy: 0.3383\nEpoch 2/100 - Loss: 0.0340, Val Loss: 1.4663, Val Accuracy: 0.4791\nEpoch 3/100 - Loss: 0.0333, Val Loss: 1.4617, Val Accuracy: 0.4708\nEpoch 4/100 - Loss: 0.0333, Val Loss: 1.4582, Val Accuracy: 0.4978\nEpoch 5/100 - Loss: 0.0329, Val Loss: 1.4466, Val Accuracy: 0.4689\nEpoch 6/100 - Loss: 0.0330, Val Loss: 1.5447, Val Accuracy: 0.3295\nEpoch 7/100 - Loss: 0.0338, Val Loss: 1.5587, Val Accuracy: 0.3723\nEpoch 8/100 - Loss: 0.0330, Val Loss: 1.6625, Val Accuracy: 0.2749\nEpoch 9/100 - Loss: 0.0334, Val Loss: 1.4336, Val Accuracy: 0.3806\nEpoch 10/100 - Loss: 0.0335, Val Loss: 1.6100, Val Accuracy: 0.3103\nEpoch 11/100 - Loss: 0.0330, Val Loss: 1.4960, Val Accuracy: 0.3970\nEpoch 12/100 - Loss: 0.0328, Val Loss: 1.4443, Val Accuracy: 0.5305\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.4679549767264426, Validation Accuracy = 0.38676666666666665\nFold 1: Validation Loss = 1.4335429940396534, Validation Accuracy = 0.38056666666666666\nAverage Validation Loss across folds: 1.450748985383048\nAverage Validation Accuracy across folds: 0.38366666666666666\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0031622776601683794, L2: 0.0001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0702, Val Loss: 1.2142, Val Accuracy: 0.5484\nEpoch 2/100 - Loss: 0.0628, Val Loss: 1.0471, Val Accuracy: 0.5998\nEpoch 3/100 - Loss: 0.0623, Val Loss: 1.2113, Val Accuracy: 0.5436\nEpoch 4/100 - Loss: 0.0619, Val Loss: 1.1946, Val Accuracy: 0.6082\nEpoch 5/100 - Loss: 0.0617, Val Loss: 1.0882, Val Accuracy: 0.6289\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0703, Val Loss: 1.0566, Val Accuracy: 0.6086\nEpoch 2/100 - Loss: 0.0624, Val Loss: 1.0639, Val Accuracy: 0.6287\nEpoch 3/100 - Loss: 0.0621, Val Loss: 1.1023, Val Accuracy: 0.6211\nEpoch 4/100 - Loss: 0.0624, Val Loss: 1.1183, Val Accuracy: 0.6349\nEpoch 5/100 - Loss: 0.0617, Val Loss: 1.0396, Val Accuracy: 0.5908\nEpoch 6/100 - Loss: 0.0619, Val Loss: 1.1077, Val Accuracy: 0.6046\nEpoch 7/100 - Loss: 0.0619, Val Loss: 1.0303, Val Accuracy: 0.6632\nEpoch 8/100 - Loss: 0.0618, Val Loss: 1.0857, Val Accuracy: 0.6275\nEpoch 9/100 - Loss: 0.0615, Val Loss: 1.0177, Val Accuracy: 0.6452\nEpoch 10/100 - Loss: 0.0623, Val Loss: 1.0869, Val Accuracy: 0.6293\nEpoch 11/100 - Loss: 0.0616, Val Loss: 1.1041, Val Accuracy: 0.6326\nEpoch 12/100 - Loss: 0.0613, Val Loss: 1.2221, Val Accuracy: 0.5665\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.047046583979877, Validation Accuracy = 0.5998333333333333\nFold 1: Validation Loss = 1.0176388470094595, Validation Accuracy = 0.6452333333333333\nAverage Validation Loss across folds: 1.0323427154946683\nAverage Validation Accuracy across folds: 0.6225333333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0031622776601683794, L2: 0.0005623413251903491\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0697, Val Loss: 1.3432, Val Accuracy: 0.4925\nEpoch 2/100 - Loss: 0.0621, Val Loss: 1.3640, Val Accuracy: 0.4989\nEpoch 3/100 - Loss: 0.0619, Val Loss: 1.3231, Val Accuracy: 0.4818\nEpoch 4/100 - Loss: 0.0616, Val Loss: 1.2866, Val Accuracy: 0.5160\nEpoch 5/100 - Loss: 0.0616, Val Loss: 1.2246, Val Accuracy: 0.5135\nEpoch 6/100 - Loss: 0.0616, Val Loss: 1.2663, Val Accuracy: 0.5260\nEpoch 7/100 - Loss: 0.0618, Val Loss: 1.2851, Val Accuracy: 0.4676\nEpoch 8/100 - Loss: 0.0616, Val Loss: 1.2655, Val Accuracy: 0.4668\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0700, Val Loss: 1.1976, Val Accuracy: 0.5388\nEpoch 2/100 - Loss: 0.0621, Val Loss: 1.1500, Val Accuracy: 0.6100\nEpoch 3/100 - Loss: 0.0620, Val Loss: 1.1957, Val Accuracy: 0.5632\nEpoch 4/100 - Loss: 0.0616, Val Loss: 1.1593, Val Accuracy: 0.6115\nEpoch 5/100 - Loss: 0.0619, Val Loss: 1.1667, Val Accuracy: 0.5423\nEpoch 6/100 - Loss: 0.0615, Val Loss: 1.1338, Val Accuracy: 0.5216\nEpoch 7/100 - Loss: 0.0620, Val Loss: 1.1180, Val Accuracy: 0.5622\nEpoch 8/100 - Loss: 0.0620, Val Loss: 1.1217, Val Accuracy: 0.5223\nEpoch 9/100 - Loss: 0.0617, Val Loss: 1.0985, Val Accuracy: 0.5538\nEpoch 10/100 - Loss: 0.0617, Val Loss: 1.1757, Val Accuracy: 0.5917\nEpoch 11/100 - Loss: 0.0620, Val Loss: 1.2339, Val Accuracy: 0.5466\nEpoch 12/100 - Loss: 0.0618, Val Loss: 1.2514, Val Accuracy: 0.5252\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.2245664738897066, Validation Accuracy = 0.5134666666666666\nFold 1: Validation Loss = 1.098504531866452, Validation Accuracy = 0.5538333333333333\nAverage Validation Loss across folds: 1.1615355028780794\nAverage Validation Accuracy across folds: 0.53365\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0031622776601683794, L2: 0.0031622776601683794\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0698, Val Loss: 1.1210, Val Accuracy: 0.5800\nEpoch 2/100 - Loss: 0.0626, Val Loss: 1.1171, Val Accuracy: 0.6018\nEpoch 3/100 - Loss: 0.0624, Val Loss: 1.2142, Val Accuracy: 0.5945\nEpoch 4/100 - Loss: 0.0625, Val Loss: 1.1116, Val Accuracy: 0.5954\nEpoch 5/100 - Loss: 0.0625, Val Loss: 1.0580, Val Accuracy: 0.6436\nEpoch 6/100 - Loss: 0.0623, Val Loss: 1.0520, Val Accuracy: 0.6532\nEpoch 7/100 - Loss: 0.0631, Val Loss: 1.1405, Val Accuracy: 0.6034\nEpoch 8/100 - Loss: 0.0626, Val Loss: 1.0393, Val Accuracy: 0.6611\nEpoch 9/100 - Loss: 0.0627, Val Loss: 1.0927, Val Accuracy: 0.5875\nEpoch 10/100 - Loss: 0.0626, Val Loss: 1.1462, Val Accuracy: 0.5700\nEpoch 11/100 - Loss: 0.0626, Val Loss: 1.1107, Val Accuracy: 0.6031\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0695, Val Loss: 1.3060, Val Accuracy: 0.5113\nEpoch 2/100 - Loss: 0.0621, Val Loss: 1.3661, Val Accuracy: 0.5119\nEpoch 3/100 - Loss: 0.0619, Val Loss: 1.1826, Val Accuracy: 0.6169\nEpoch 4/100 - Loss: 0.0622, Val Loss: 1.2362, Val Accuracy: 0.5092\nEpoch 5/100 - Loss: 0.0618, Val Loss: 1.1930, Val Accuracy: 0.4931\nEpoch 6/100 - Loss: 0.0621, Val Loss: 1.1818, Val Accuracy: 0.5320\nEpoch 7/100 - Loss: 0.0621, Val Loss: 1.2270, Val Accuracy: 0.5668\nEpoch 8/100 - Loss: 0.0619, Val Loss: 1.2743, Val Accuracy: 0.4967\nEpoch 9/100 - Loss: 0.0623, Val Loss: 1.2049, Val Accuracy: 0.5150\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.0391970632681207, Validation Accuracy = 0.6611333333333334\nFold 1: Validation Loss = 1.1817621540413228, Validation Accuracy = 0.5319666666666667\nAverage Validation Loss across folds: 1.1104796086547217\nAverage Validation Accuracy across folds: 0.59655\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0031622776601683794, L2: 0.01778279410038923\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0717, Val Loss: 1.8977, Val Accuracy: 0.2290\nEpoch 2/100 - Loss: 0.0650, Val Loss: 1.8449, Val Accuracy: 0.2247\nEpoch 3/100 - Loss: 0.0647, Val Loss: 1.8484, Val Accuracy: 0.2828\nEpoch 4/100 - Loss: 0.0646, Val Loss: 1.8296, Val Accuracy: 0.2428\nEpoch 5/100 - Loss: 0.0646, Val Loss: 1.8246, Val Accuracy: 0.2562\nEpoch 6/100 - Loss: 0.0644, Val Loss: 1.8353, Val Accuracy: 0.2701\nEpoch 7/100 - Loss: 0.0643, Val Loss: 1.8368, Val Accuracy: 0.2284\nEpoch 8/100 - Loss: 0.0643, Val Loss: 1.8305, Val Accuracy: 0.2974\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0706, Val Loss: 1.5530, Val Accuracy: 0.4141\nEpoch 2/100 - Loss: 0.0636, Val Loss: 1.6296, Val Accuracy: 0.3513\nEpoch 3/100 - Loss: 0.0633, Val Loss: 1.6140, Val Accuracy: 0.4232\nEpoch 4/100 - Loss: 0.0635, Val Loss: 1.5355, Val Accuracy: 0.4204\nEpoch 5/100 - Loss: 0.0632, Val Loss: 1.5616, Val Accuracy: 0.3841\nEpoch 6/100 - Loss: 0.0632, Val Loss: 1.5469, Val Accuracy: 0.4184\nEpoch 7/100 - Loss: 0.0633, Val Loss: 1.6163, Val Accuracy: 0.4525\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.8245379136823643, Validation Accuracy = 0.2562\nFold 1: Validation Loss = 1.5354848561256424, Validation Accuracy = 0.4204\nAverage Validation Loss across folds: 1.6800113849040033\nAverage Validation Accuracy across folds: 0.3383\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.0031622776601683794, L2: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0753, Val Loss: 2.3030, Val Accuracy: 0.1013\nEpoch 2/100 - Loss: 0.0701, Val Loss: 2.3027, Val Accuracy: 0.1016\nEpoch 3/100 - Loss: 0.0702, Val Loss: 2.3028, Val Accuracy: 0.0989\nEpoch 4/100 - Loss: 0.0702, Val Loss: 2.3029, Val Accuracy: 0.1016\nEpoch 5/100 - Loss: 0.0702, Val Loss: 2.3028, Val Accuracy: 0.0977\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0752, Val Loss: 2.3030, Val Accuracy: 0.0984\nEpoch 2/100 - Loss: 0.0700, Val Loss: 2.3028, Val Accuracy: 0.0990\nEpoch 3/100 - Loss: 0.0701, Val Loss: 2.3029, Val Accuracy: 0.0992\nEpoch 4/100 - Loss: 0.0701, Val Loss: 2.3034, Val Accuracy: 0.0987\nEpoch 5/100 - Loss: 0.0701, Val Loss: 2.3028, Val Accuracy: 0.0987\nEpoch 6/100 - Loss: 0.0701, Val Loss: 2.3031, Val Accuracy: 0.0992\nEpoch 7/100 - Loss: 0.0701, Val Loss: 2.3028, Val Accuracy: 0.1011\nEpoch 8/100 - Loss: 0.0701, Val Loss: 2.3028, Val Accuracy: 0.0990\nEpoch 9/100 - Loss: 0.0701, Val Loss: 2.3027, Val Accuracy: 0.0992\nEpoch 10/100 - Loss: 0.0701, Val Loss: 2.3028, Val Accuracy: 0.0992\nEpoch 11/100 - Loss: 0.0701, Val Loss: 2.3031, Val Accuracy: 0.0984\nEpoch 12/100 - Loss: 0.0701, Val Loss: 2.3030, Val Accuracy: 0.0990\nEpoch 13/100 - Loss: 0.0701, Val Loss: 2.3026, Val Accuracy: 0.1023\nEpoch 14/100 - Loss: 0.0701, Val Loss: 2.3031, Val Accuracy: 0.0992\nEpoch 15/100 - Loss: 0.0701, Val Loss: 2.3028, Val Accuracy: 0.0984\nEpoch 16/100 - Loss: 0.0701, Val Loss: 2.3027, Val Accuracy: 0.0990\nStopping early at epoch 17\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.302656940305665, Validation Accuracy = 0.10156666666666667\nFold 1: Validation Loss = 2.3025848621498546, Validation Accuracy = 0.10233333333333333\nAverage Validation Loss across folds: 2.30262090122776\nAverage Validation Accuracy across folds: 0.10195\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.01778279410038923, L2: 0.0001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.2639, Val Loss: 2.3026, Val Accuracy: 0.1012\nEpoch 2/100 - Loss: 0.2326, Val Loss: 2.3028, Val Accuracy: 0.0977\nEpoch 3/100 - Loss: 0.2318, Val Loss: 2.3026, Val Accuracy: 0.0977\nEpoch 4/100 - Loss: 0.2313, Val Loss: 2.3025, Val Accuracy: 0.1017\nEpoch 5/100 - Loss: 0.2309, Val Loss: 2.3028, Val Accuracy: 0.0977\nEpoch 6/100 - Loss: 0.2306, Val Loss: 2.3028, Val Accuracy: 0.0977\nEpoch 7/100 - Loss: 0.2303, Val Loss: 2.3026, Val Accuracy: 0.0984\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.2627, Val Loss: 2.3028, Val Accuracy: 0.0977\nEpoch 2/100 - Loss: 0.2313, Val Loss: 2.3029, Val Accuracy: 0.0988\nEpoch 3/100 - Loss: 0.2305, Val Loss: 2.3028, Val Accuracy: 0.0988\nEpoch 4/100 - Loss: 0.2301, Val Loss: 2.3026, Val Accuracy: 0.0988\nEpoch 5/100 - Loss: 0.2296, Val Loss: 2.3028, Val Accuracy: 0.0977\nEpoch 6/100 - Loss: 0.2293, Val Loss: 2.3027, Val Accuracy: 0.0997\nEpoch 7/100 - Loss: 0.2291, Val Loss: 2.3027, Val Accuracy: 0.0983\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.302532252472347, Validation Accuracy = 0.1017\nFold 1: Validation Loss = 2.3025761972358234, Validation Accuracy = 0.09876666666666667\nAverage Validation Loss across folds: 2.3025542248540853\nAverage Validation Accuracy across folds: 0.10023333333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.01778279410038923, L2: 0.0005623413251903491\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.2640, Val Loss: 2.3027, Val Accuracy: 0.0979\nEpoch 2/100 - Loss: 0.2330, Val Loss: 2.3028, Val Accuracy: 0.0997\nEpoch 3/100 - Loss: 0.2320, Val Loss: 2.3026, Val Accuracy: 0.0989\nEpoch 4/100 - Loss: 0.2312, Val Loss: 2.3027, Val Accuracy: 0.0997\nEpoch 5/100 - Loss: 0.2304, Val Loss: 2.3028, Val Accuracy: 0.0979\nEpoch 6/100 - Loss: 0.2297, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 7/100 - Loss: 0.2293, Val Loss: 2.3026, Val Accuracy: 0.0993\nEpoch 8/100 - Loss: 0.2289, Val Loss: 2.3026, Val Accuracy: 0.1004\nEpoch 9/100 - Loss: 0.2285, Val Loss: 2.3028, Val Accuracy: 0.0979\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.2640, Val Loss: 2.3026, Val Accuracy: 0.0993\nEpoch 2/100 - Loss: 0.2331, Val Loss: 2.3027, Val Accuracy: 0.0996\nEpoch 3/100 - Loss: 0.2320, Val Loss: 2.3028, Val Accuracy: 0.0989\nEpoch 4/100 - Loss: 0.2312, Val Loss: 2.3026, Val Accuracy: 0.1017\nStopping early at epoch 5\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3025770975328457, Validation Accuracy = 0.10113333333333334\nFold 1: Validation Loss = 2.302582888206693, Validation Accuracy = 0.09926666666666667\nAverage Validation Loss across folds: 2.302579992869769\nAverage Validation Accuracy across folds: 0.10020000000000001\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.01778279410038923, L2: 0.0031622776601683794\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.2647, Val Loss: 2.3026, Val Accuracy: 0.1008\nEpoch 2/100 - Loss: 0.2309, Val Loss: 2.3026, Val Accuracy: 0.0989\nEpoch 3/100 - Loss: 0.2293, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 4/100 - Loss: 0.2307, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 5/100 - Loss: 0.2326, Val Loss: 2.3027, Val Accuracy: 0.0990\nEpoch 6/100 - Loss: 0.2348, Val Loss: 2.3026, Val Accuracy: 0.0976\nEpoch 7/100 - Loss: 0.2359, Val Loss: 2.3027, Val Accuracy: 0.0976\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.2652, Val Loss: 2.3027, Val Accuracy: 0.1005\nEpoch 2/100 - Loss: 0.2314, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 3/100 - Loss: 0.2299, Val Loss: 2.3028, Val Accuracy: 0.0989\nEpoch 4/100 - Loss: 0.2312, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 5/100 - Loss: 0.2331, Val Loss: 2.3027, Val Accuracy: 0.0982\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.302550287897399, Validation Accuracy = 0.1011\nFold 1: Validation Loss = 2.3026224865333864, Validation Accuracy = 0.10113333333333334\nAverage Validation Loss across folds: 2.302586387215393\nAverage Validation Accuracy across folds: 0.10111666666666666\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.01778279410038923, L2: 0.01778279410038923\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.2681, Val Loss: 2.3026, Val Accuracy: 0.0991\nEpoch 2/100 - Loss: 0.2416, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 3/100 - Loss: 0.2420, Val Loss: 2.3027, Val Accuracy: 0.1014\nEpoch 4/100 - Loss: 0.2423, Val Loss: 2.3028, Val Accuracy: 0.0988\nEpoch 5/100 - Loss: 0.2426, Val Loss: 2.3027, Val Accuracy: 0.0998\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.2681, Val Loss: 2.3026, Val Accuracy: 0.1012\nEpoch 2/100 - Loss: 0.2417, Val Loss: 2.3026, Val Accuracy: 0.1002\nEpoch 3/100 - Loss: 0.2421, Val Loss: 2.3026, Val Accuracy: 0.1009\nEpoch 4/100 - Loss: 0.2424, Val Loss: 2.3026, Val Accuracy: 0.1009\nEpoch 5/100 - Loss: 0.2426, Val Loss: 2.3027, Val Accuracy: 0.0990\nEpoch 6/100 - Loss: 0.2428, Val Loss: 2.3027, Val Accuracy: 0.0990\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3026089129417437, Validation Accuracy = 0.0998\nFold 1: Validation Loss = 2.3026048130572225, Validation Accuracy = 0.1009\nAverage Validation Loss across folds: 2.3026068629994834\nAverage Validation Accuracy across folds: 0.10035\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.01778279410038923, L2: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.2619, Val Loss: 2.3026, Val Accuracy: 0.1005\nEpoch 2/100 - Loss: 0.2333, Val Loss: 2.3028, Val Accuracy: 0.1000\nEpoch 3/100 - Loss: 0.2334, Val Loss: 2.3025, Val Accuracy: 0.1018\nEpoch 4/100 - Loss: 0.2334, Val Loss: 2.3027, Val Accuracy: 0.0971\nEpoch 5/100 - Loss: 0.2333, Val Loss: 2.3026, Val Accuracy: 0.1018\nEpoch 6/100 - Loss: 0.2333, Val Loss: 2.3026, Val Accuracy: 0.1024\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.2616, Val Loss: 2.3027, Val Accuracy: 0.0976\nEpoch 2/100 - Loss: 0.2331, Val Loss: 2.3026, Val Accuracy: 0.0990\nEpoch 3/100 - Loss: 0.2332, Val Loss: 2.3028, Val Accuracy: 0.0995\nEpoch 4/100 - Loss: 0.2332, Val Loss: 2.3027, Val Accuracy: 0.1000\nEpoch 5/100 - Loss: 0.2332, Val Loss: 2.3026, Val Accuracy: 0.1019\nEpoch 6/100 - Loss: 0.2331, Val Loss: 2.3027, Val Accuracy: 0.0995\nEpoch 7/100 - Loss: 0.2331, Val Loss: 2.3027, Val Accuracy: 0.0982\nEpoch 8/100 - Loss: 0.2331, Val Loss: 2.3027, Val Accuracy: 0.0995\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3025161579473696, Validation Accuracy = 0.1018\nFold 1: Validation Loss = 2.302603925469079, Validation Accuracy = 0.1019\nAverage Validation Loss across folds: 2.302560041708224\nAverage Validation Accuracy across folds: 0.10185\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.1, L2: 0.0001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 1.3447, Val Loss: 2.3026, Val Accuracy: 0.0996\nEpoch 2/100 - Loss: 1.1775, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 3/100 - Loss: 1.1746, Val Loss: 2.3026, Val Accuracy: 0.1003\nEpoch 4/100 - Loss: 1.1726, Val Loss: 2.3026, Val Accuracy: 0.0969\nEpoch 5/100 - Loss: 1.1704, Val Loss: 2.3026, Val Accuracy: 0.1009\nEpoch 6/100 - Loss: 1.1689, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 7/100 - Loss: 1.1674, Val Loss: 2.3026, Val Accuracy: 0.0992\nEpoch 8/100 - Loss: 1.1664, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 9/100 - Loss: 1.1652, Val Loss: 2.3026, Val Accuracy: 0.0977\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 1.3472, Val Loss: 2.3026, Val Accuracy: 0.0976\nEpoch 2/100 - Loss: 1.1804, Val Loss: 2.3026, Val Accuracy: 0.0976\nEpoch 3/100 - Loss: 1.1776, Val Loss: 2.3026, Val Accuracy: 0.0991\nEpoch 4/100 - Loss: 1.1756, Val Loss: 2.3026, Val Accuracy: 0.0983\nEpoch 5/100 - Loss: 1.1734, Val Loss: 2.3026, Val Accuracy: 0.0991\nEpoch 6/100 - Loss: 1.1718, Val Loss: 2.3026, Val Accuracy: 0.1031\nEpoch 7/100 - Loss: 1.1704, Val Loss: 2.3026, Val Accuracy: 0.0976\nEpoch 8/100 - Loss: 1.1692, Val Loss: 2.3026, Val Accuracy: 0.0992\nEpoch 9/100 - Loss: 1.1681, Val Loss: 2.3026, Val Accuracy: 0.0983\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.302571329734981, Validation Accuracy = 0.1017\nFold 1: Validation Loss = 2.302554980015704, Validation Accuracy = 0.1031\nAverage Validation Loss across folds: 2.3025631548753425\nAverage Validation Accuracy across folds: 0.10239999999999999\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.1, L2: 0.0005623413251903491\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 1.3479, Val Loss: 2.3026, Val Accuracy: 0.0992\nEpoch 2/100 - Loss: 1.1824, Val Loss: 2.3026, Val Accuracy: 0.1000\nEpoch 3/100 - Loss: 1.1804, Val Loss: 2.3026, Val Accuracy: 0.0987\nEpoch 4/100 - Loss: 1.1789, Val Loss: 2.3026, Val Accuracy: 0.1000\nEpoch 5/100 - Loss: 1.1768, Val Loss: 2.3026, Val Accuracy: 0.0987\nEpoch 6/100 - Loss: 1.1752, Val Loss: 2.3026, Val Accuracy: 0.1004\nEpoch 7/100 - Loss: 1.1737, Val Loss: 2.3026, Val Accuracy: 0.1000\nEpoch 8/100 - Loss: 1.1724, Val Loss: 2.3026, Val Accuracy: 0.1007\nEpoch 9/100 - Loss: 1.1711, Val Loss: 2.3026, Val Accuracy: 0.1000\nEpoch 10/100 - Loss: 1.1694, Val Loss: 2.3026, Val Accuracy: 0.1007\nEpoch 11/100 - Loss: 1.1684, Val Loss: 2.3026, Val Accuracy: 0.0993\nEpoch 12/100 - Loss: 1.1675, Val Loss: 2.3026, Val Accuracy: 0.1004\nEpoch 13/100 - Loss: 1.1664, Val Loss: 2.3026, Val Accuracy: 0.1000\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 1.3469, Val Loss: 2.3026, Val Accuracy: 0.1008\nEpoch 2/100 - Loss: 1.1808, Val Loss: 2.3026, Val Accuracy: 0.0988\nEpoch 3/100 - Loss: 1.1786, Val Loss: 2.3026, Val Accuracy: 0.0988\nEpoch 4/100 - Loss: 1.1771, Val Loss: 2.3026, Val Accuracy: 0.0996\nEpoch 5/100 - Loss: 1.1750, Val Loss: 2.3026, Val Accuracy: 0.1000\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3025767330421822, Validation Accuracy = 0.10073333333333333\nFold 1: Validation Loss = 2.302581841503379, Validation Accuracy = 0.09883333333333333\nAverage Validation Loss across folds: 2.3025792872727804\nAverage Validation Accuracy across folds: 0.09978333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.1, L2: 0.0031622776601683794\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 1.3499, Val Loss: 2.3026, Val Accuracy: 0.1001\nEpoch 2/100 - Loss: 1.1794, Val Loss: 2.3026, Val Accuracy: 0.1001\nEpoch 3/100 - Loss: 1.1719, Val Loss: 2.3026, Val Accuracy: 0.1001\nEpoch 4/100 - Loss: 1.1663, Val Loss: 2.3026, Val Accuracy: 0.1004\nEpoch 5/100 - Loss: 1.1602, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 6/100 - Loss: 1.1550, Val Loss: 2.3026, Val Accuracy: 0.0997\nEpoch 7/100 - Loss: 1.1506, Val Loss: 2.3026, Val Accuracy: 0.1001\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 1.3511, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 2/100 - Loss: 1.1807, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 3/100 - Loss: 1.1734, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 4/100 - Loss: 1.1676, Val Loss: 2.3026, Val Accuracy: 0.0999\nEpoch 5/100 - Loss: 1.1613, Val Loss: 2.3026, Val Accuracy: 0.1003\nEpoch 6/100 - Loss: 1.1559, Val Loss: 2.3026, Val Accuracy: 0.0996\nEpoch 7/100 - Loss: 1.1516, Val Loss: 2.3026, Val Accuracy: 0.1003\nEpoch 8/100 - Loss: 1.1482, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 9/100 - Loss: 1.1445, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 10/100 - Loss: 1.1411, Val Loss: 2.3026, Val Accuracy: 0.1003\nEpoch 11/100 - Loss: 1.1386, Val Loss: 2.3026, Val Accuracy: 0.1017\nEpoch 12/100 - Loss: 1.1367, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 13/100 - Loss: 1.1362, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 14/100 - Loss: 1.1365, Val Loss: 2.3026, Val Accuracy: 0.0992\nStopping early at epoch 15\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.302580474027947, Validation Accuracy = 0.10036666666666667\nFold 1: Validation Loss = 2.3025680162759223, Validation Accuracy = 0.10173333333333333\nAverage Validation Loss across folds: 2.3025742451519347\nAverage Validation Accuracy across folds: 0.10105\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.1, L2: 0.01778279410038923\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 1.3508, Val Loss: 2.3026, Val Accuracy: 0.1016\nEpoch 2/100 - Loss: 1.1591, Val Loss: 2.3026, Val Accuracy: 0.0989\nEpoch 3/100 - Loss: 1.1487, Val Loss: 2.3026, Val Accuracy: 0.0984\nEpoch 4/100 - Loss: 1.1580, Val Loss: 2.3026, Val Accuracy: 0.0990\nEpoch 5/100 - Loss: 1.1679, Val Loss: 2.3026, Val Accuracy: 0.1022\nEpoch 6/100 - Loss: 1.1835, Val Loss: 2.3026, Val Accuracy: 0.0984\nEpoch 7/100 - Loss: 1.1895, Val Loss: 2.3026, Val Accuracy: 0.0989\nEpoch 8/100 - Loss: 1.1911, Val Loss: 2.3026, Val Accuracy: 0.0999\nEpoch 9/100 - Loss: 1.1917, Val Loss: 2.3026, Val Accuracy: 0.0999\nEpoch 10/100 - Loss: 1.1920, Val Loss: 2.3026, Val Accuracy: 0.0984\nEpoch 11/100 - Loss: 1.1926, Val Loss: 2.3026, Val Accuracy: 0.1013\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 1.3498, Val Loss: 2.3026, Val Accuracy: 0.0978\nEpoch 2/100 - Loss: 1.1583, Val Loss: 2.3026, Val Accuracy: 0.1011\nEpoch 3/100 - Loss: 1.1479, Val Loss: 2.3026, Val Accuracy: 0.0987\nEpoch 4/100 - Loss: 1.1572, Val Loss: 2.3026, Val Accuracy: 0.1191\nEpoch 5/100 - Loss: 1.1673, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 6/100 - Loss: 1.1829, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 7/100 - Loss: 1.1889, Val Loss: 2.3026, Val Accuracy: 0.0984\nEpoch 8/100 - Loss: 1.1904, Val Loss: 2.3026, Val Accuracy: 0.0998\nEpoch 9/100 - Loss: 1.1911, Val Loss: 2.3026, Val Accuracy: 0.0987\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3025647897455994, Validation Accuracy = 0.09993333333333333\nFold 1: Validation Loss = 2.3025678800367344, Validation Accuracy = 0.09976666666666667\nAverage Validation Loss across folds: 2.302566334891167\nAverage Validation Accuracy across folds: 0.09985\n--------------------------------\nK-fold Cross-validation completed.\nTraining with L1: 0.1, L2: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 1.3712, Val Loss: 2.3026, Val Accuracy: 0.1001\nEpoch 2/100 - Loss: 1.2216, Val Loss: 2.3026, Val Accuracy: 0.1005\nEpoch 3/100 - Loss: 1.2224, Val Loss: 2.3026, Val Accuracy: 0.1005\nEpoch 4/100 - Loss: 1.2226, Val Loss: 2.3026, Val Accuracy: 0.0984\nEpoch 5/100 - Loss: 1.2228, Val Loss: 2.3026, Val Accuracy: 0.0975\nEpoch 6/100 - Loss: 1.2226, Val Loss: 2.3026, Val Accuracy: 0.0993\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 1.3718, Val Loss: 2.3026, Val Accuracy: 0.1310\nEpoch 2/100 - Loss: 1.2222, Val Loss: 2.3026, Val Accuracy: 0.1025\nEpoch 3/100 - Loss: 1.2231, Val Loss: 2.3026, Val Accuracy: 0.1010\nEpoch 4/100 - Loss: 1.2233, Val Loss: 2.3026, Val Accuracy: 0.0986\nEpoch 5/100 - Loss: 1.2235, Val Loss: 2.3026, Val Accuracy: 0.0974\nEpoch 6/100 - Loss: 1.2233, Val Loss: 2.3026, Val Accuracy: 0.0980\nEpoch 7/100 - Loss: 1.2234, Val Loss: 2.3026, Val Accuracy: 0.1016\nEpoch 8/100 - Loss: 1.2235, Val Loss: 2.3026, Val Accuracy: 0.1007\nEpoch 9/100 - Loss: 1.2235, Val Loss: 2.3026, Val Accuracy: 0.1025\nEpoch 10/100 - Loss: 1.2235, Val Loss: 2.3026, Val Accuracy: 0.1016\nEpoch 11/100 - Loss: 1.2236, Val Loss: 2.3026, Val Accuracy: 0.1007\nEpoch 12/100 - Loss: 1.2236, Val Loss: 2.3026, Val Accuracy: 0.0995\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3025793963149668, Validation Accuracy = 0.1005\nFold 1: Validation Loss = 2.3025585674782043, Validation Accuracy = 0.1025\nAverage Validation Loss across folds: 2.3025689818965853\nAverage Validation Accuracy across folds: 0.1015\n--------------------------------\nK-fold Cross-validation completed.\n          l1        l2  train_loss  val_loss  val_accuracy\n0   0.000100  0.000100    0.694800  0.717250      0.043452\n1   0.000100  0.000562    0.709761  0.714033      0.039795\n2   0.000100  0.003162    0.770717  0.707150      0.031289\n3   0.000100  0.017783    0.933120  0.646633      0.024251\n4   0.000100  0.100000    1.385864  0.447433      0.026379\n5   0.000562  0.000100    0.694739  0.733000      0.056297\n6   0.000562  0.000562    0.749390  0.713900      0.053505\n7   0.000562  0.003162    0.777330  0.720000      0.046358\n8   0.000562  0.017783    0.913481  0.667833      0.036259\n9   0.000562  0.100000    1.450749  0.383667      0.033569\n10  0.003162  0.000100    1.032343  0.622533      0.062833\n11  0.003162  0.000562    1.161536  0.533650      0.062517\n12  0.003162  0.003162    1.110480  0.596550      0.062991\n13  0.003162  0.017783    1.680011  0.338300      0.064812\n14  0.003162  0.100000    2.302621  0.101950      0.070583\n15  0.017783  0.000100    2.302554  0.100233      0.234562\n16  0.017783  0.000562    2.302580  0.100200      0.235061\n17  0.017783  0.003162    2.302586  0.101117      0.237253\n18  0.017783  0.017783    2.302607  0.100350      0.246316\n19  0.017783  0.100000    2.302560  0.101850      0.236775\n20  0.100000  0.000100    2.302563  0.102400      1.188609\n21  0.100000  0.000562    2.302579  0.099783      1.191367\n22  0.100000  0.003162    2.302574  0.101050      1.171356\n23  0.100000  0.017783    2.302566  0.099850      1.192878\n24  0.100000  0.100000    2.302569  0.101500      1.237895\n\n\n\n# ordered them wrong, so fixing the column names\nresults_choice_1.rename(columns={'train_loss': 'avg_val_loss'}, inplace=True)\nresults_choice_1.rename(columns={'val_loss': 'average_val_accuracy'}, inplace=True)\nresults_choice_1.rename(columns={'val_accuracy': 'average_train_loss'}, inplace=True)\n\n# save results_df1 to csv\nresults_choice_1.to_csv('results/results_choice_1.csv')\n\n\n# Display results\nresults_choice_1\n\n\n\n\n\n\n\n\nl1\nl2\naverage_val_loss\naverage_val_accuracy\naverage_train_loss\n\n\n\n\n0\n0.000100\n0.000100\n0.694800\n0.717250\n0.043452\n\n\n1\n0.000100\n0.000562\n0.709761\n0.714033\n0.039795\n\n\n2\n0.000100\n0.003162\n0.770717\n0.707150\n0.031289\n\n\n3\n0.000100\n0.017783\n0.933120\n0.646633\n0.024251\n\n\n4\n0.000100\n0.100000\n1.385864\n0.447433\n0.026379\n\n\n5\n0.000562\n0.000100\n0.694739\n0.733000\n0.056297\n\n\n6\n0.000562\n0.000562\n0.749390\n0.713900\n0.053505\n\n\n7\n0.000562\n0.003162\n0.777330\n0.720000\n0.046358\n\n\n8\n0.000562\n0.017783\n0.913481\n0.667833\n0.036259\n\n\n9\n0.000562\n0.100000\n1.450749\n0.383667\n0.033569\n\n\n10\n0.003162\n0.000100\n1.032343\n0.622533\n0.062833\n\n\n11\n0.003162\n0.000562\n1.161536\n0.533650\n0.062517\n\n\n12\n0.003162\n0.003162\n1.110480\n0.596550\n0.062991\n\n\n13\n0.003162\n0.017783\n1.680011\n0.338300\n0.064812\n\n\n14\n0.003162\n0.100000\n2.302621\n0.101950\n0.070583\n\n\n15\n0.017783\n0.000100\n2.302554\n0.100233\n0.234562\n\n\n16\n0.017783\n0.000562\n2.302580\n0.100200\n0.235061\n\n\n17\n0.017783\n0.003162\n2.302586\n0.101117\n0.237253\n\n\n18\n0.017783\n0.017783\n2.302607\n0.100350\n0.246316\n\n\n19\n0.017783\n0.100000\n2.302560\n0.101850\n0.236775\n\n\n20\n0.100000\n0.000100\n2.302563\n0.102400\n1.188609\n\n\n21\n0.100000\n0.000562\n2.302579\n0.099783\n1.191367\n\n\n22\n0.100000\n0.003162\n2.302574\n0.101050\n1.171356\n\n\n23\n0.100000\n0.017783\n2.302566\n0.099850\n1.192878\n\n\n24\n0.100000\n0.100000\n2.302569\n0.101500\n1.237895\n\n\n\n\n\n\n\n\nresults_choice_1 = pd.read_csv('results/results_choice_1.csv')\n\n# pivot the df for heatmap\ntrain_loss_choice_1_data = results_choice_1.pivot(index='l1', columns='l2', values='average_train_loss')\nval_loss_choice_1_data = results_choice_1.pivot(index='l1', columns='l2', values='average_val_loss')\n# error ratio\nerror_ratio_choice_1_data = val_loss_choice_1_data / train_loss_choice_1_data\n\n# Plot heatmaps\nplt.figure(figsize=(14, 12))\n\n# Training Error\nplt.subplot(2, 1, 1)\nsns.heatmap(train_loss_choice_1_data, annot=True, cmap='mako', fmt=\".3f\")\nplt.title('Training Error by L1 and L2 Reg')\nplt.xlabel('L2 Reg')\nplt.ylabel('L1 Reg')\n\n# Validation Error\nplt.subplot(2, 1, 2)\nsns.heatmap(val_loss_choice_1_data, annot=True, cmap='mako', fmt=\".3f\")\nplt.title('Validation Error by L1 and L2 Reg')\nplt.xlabel('L2 Reg')\nplt.ylabel('L1 Reg')\n\n# Show heatmaps\nplt.tight_layout()\nplt.show()\n\n# Error Ratio\nplt.figure(figsize=(7, 6))\nsns.heatmap(error_ratio_choice_1_data, annot=True, cmap='mako', fmt=\".2f\")\nplt.title('Error Ratio by L1 and L2 Reg')\nplt.xlabel('L2 Reg')\nplt.ylabel('L1 Reg')\n\n# Save figure just in case\nplt.savefig('error_ratio.png')\nplt.show()"
  },
  {
    "objectID": "ANN.html#choice-2",
    "href": "ANN.html#choice-2",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "for hidden activation=sigmoid, hidden layers = [96,96,96], optimizer=rmsprop\nVary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization)\nPlot training and validation error as a function of dropout rate\n\nPlot the ratio (or difference) of validation to training error as a function of dropout rate\n\n\n# model_fn2\ndef model_fn2(dropout_rate=0.0):\n    return NeuralNetwork(input_size=28*28, layer_sizes=[96, 96, 96], activation_funcs=['sigmoid', 'sigmoid', 'sigmoid'], dropout_rate=dropout_rate)\n\ndropout_rates = np.linspace(0.0, 0.9, 10) \n\nresults2 = []\n\nfor dropout_rate in dropout_rates:\n    print(f\"Training with Dropout Rate: {dropout_rate}\")\n    avg_train_loss, avg_val_loss, avg_val_accuracy = training_model_kfold(\n        k_folds=4,\n        num_epochs=100,\n        dataset=training_data,\n        model_fn=lambda: model_fn2(dropout_rate=dropout_rate), \n        loss_fn=nn.CrossEntropyLoss(),\n        optimizer_name=\"RMSProp\",  # \n        lr=0.01,\n        l1_reg=0, \n        l2_reg=0,\n        device=device,\n        patience=4,\n        plot=False)\n    \n    results2.append({\n        'dropout_rate': dropout_rate,\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n        'val_accuracy': avg_val_accuracy})\n\nresults_choice_2 = pd.DataFrame(results2)\nprint(results_choice_2)\n\nTraining with Dropout Rate: 0.0\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0149, Val Loss: 0.6797, Val Accuracy: 0.7635\nEpoch 2/100 - Loss: 0.0080, Val Loss: 0.6904, Val Accuracy: 0.7749\nEpoch 3/100 - Loss: 0.0071, Val Loss: 0.5336, Val Accuracy: 0.8151\nEpoch 4/100 - Loss: 0.0067, Val Loss: 0.4747, Val Accuracy: 0.8267\nEpoch 5/100 - Loss: 0.0065, Val Loss: 0.4366, Val Accuracy: 0.8459\nEpoch 6/100 - Loss: 0.0063, Val Loss: 0.4410, Val Accuracy: 0.8434\nEpoch 7/100 - Loss: 0.0061, Val Loss: 0.4566, Val Accuracy: 0.8116\nEpoch 8/100 - Loss: 0.0060, Val Loss: 0.4628, Val Accuracy: 0.8339\nEpoch 9/100 - Loss: 0.0059, Val Loss: 0.4293, Val Accuracy: 0.8459\nEpoch 10/100 - Loss: 0.0058, Val Loss: 0.5254, Val Accuracy: 0.8048\nEpoch 11/100 - Loss: 0.0057, Val Loss: 0.3938, Val Accuracy: 0.8599\nEpoch 12/100 - Loss: 0.0056, Val Loss: 0.4080, Val Accuracy: 0.8571\nEpoch 13/100 - Loss: 0.0056, Val Loss: 0.4058, Val Accuracy: 0.8571\nEpoch 14/100 - Loss: 0.0055, Val Loss: 0.4593, Val Accuracy: 0.8443\nStopping early at epoch 15\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0145, Val Loss: 0.5871, Val Accuracy: 0.7849\nEpoch 2/100 - Loss: 0.0078, Val Loss: 0.6239, Val Accuracy: 0.7758\nEpoch 3/100 - Loss: 0.0070, Val Loss: 0.5172, Val Accuracy: 0.7965\nEpoch 4/100 - Loss: 0.0066, Val Loss: 0.5219, Val Accuracy: 0.7971\nEpoch 5/100 - Loss: 0.0064, Val Loss: 0.4407, Val Accuracy: 0.8289\nEpoch 6/100 - Loss: 0.0062, Val Loss: 0.4542, Val Accuracy: 0.8287\nEpoch 7/100 - Loss: 0.0061, Val Loss: 0.4589, Val Accuracy: 0.8413\nEpoch 8/100 - Loss: 0.0060, Val Loss: 0.3975, Val Accuracy: 0.8610\nEpoch 9/100 - Loss: 0.0058, Val Loss: 0.4417, Val Accuracy: 0.8220\nEpoch 10/100 - Loss: 0.0058, Val Loss: 0.4756, Val Accuracy: 0.8326\nEpoch 11/100 - Loss: 0.0056, Val Loss: 0.4003, Val Accuracy: 0.8596\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0131, Val Loss: 0.8770, Val Accuracy: 0.6934\nEpoch 2/100 - Loss: 0.0080, Val Loss: 0.6589, Val Accuracy: 0.7642\nEpoch 3/100 - Loss: 0.0071, Val Loss: 0.4513, Val Accuracy: 0.8383\nEpoch 4/100 - Loss: 0.0067, Val Loss: 0.4299, Val Accuracy: 0.8513\nEpoch 5/100 - Loss: 0.0064, Val Loss: 0.8011, Val Accuracy: 0.6885\nEpoch 6/100 - Loss: 0.0062, Val Loss: 0.4721, Val Accuracy: 0.8265\nEpoch 7/100 - Loss: 0.0060, Val Loss: 0.4446, Val Accuracy: 0.8345\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0145, Val Loss: 1.0461, Val Accuracy: 0.6389\nEpoch 2/100 - Loss: 0.0080, Val Loss: 0.6594, Val Accuracy: 0.7727\nEpoch 3/100 - Loss: 0.0071, Val Loss: 0.7239, Val Accuracy: 0.7329\nEpoch 4/100 - Loss: 0.0068, Val Loss: 0.5455, Val Accuracy: 0.7991\nEpoch 5/100 - Loss: 0.0065, Val Loss: 0.4989, Val Accuracy: 0.8243\nEpoch 6/100 - Loss: 0.0062, Val Loss: 0.4405, Val Accuracy: 0.8375\nEpoch 7/100 - Loss: 0.0060, Val Loss: 0.4514, Val Accuracy: 0.8415\nEpoch 8/100 - Loss: 0.0059, Val Loss: 0.6356, Val Accuracy: 0.7794\nEpoch 9/100 - Loss: 0.0058, Val Loss: 0.4439, Val Accuracy: 0.8289\nEpoch 10/100 - Loss: 0.0058, Val Loss: 0.4396, Val Accuracy: 0.8473\nEpoch 11/100 - Loss: 0.0057, Val Loss: 0.4292, Val Accuracy: 0.8465\nEpoch 12/100 - Loss: 0.0057, Val Loss: 0.4844, Val Accuracy: 0.8323\nEpoch 13/100 - Loss: 0.0056, Val Loss: 0.4105, Val Accuracy: 0.8589\nEpoch 14/100 - Loss: 0.0055, Val Loss: 0.4215, Val Accuracy: 0.8504\nEpoch 15/100 - Loss: 0.0055, Val Loss: 0.4017, Val Accuracy: 0.8615\nEpoch 16/100 - Loss: 0.0053, Val Loss: 0.4581, Val Accuracy: 0.8407\nEpoch 17/100 - Loss: 0.0053, Val Loss: 0.3965, Val Accuracy: 0.8608\nEpoch 18/100 - Loss: 0.0052, Val Loss: 0.5352, Val Accuracy: 0.8010\nEpoch 19/100 - Loss: 0.0053, Val Loss: 0.4783, Val Accuracy: 0.8354\nEpoch 20/100 - Loss: 0.0052, Val Loss: 0.4401, Val Accuracy: 0.8643\nEpoch 21/100 - Loss: 0.0052, Val Loss: 0.3765, Val Accuracy: 0.8673\nEpoch 22/100 - Loss: 0.0051, Val Loss: 0.4208, Val Accuracy: 0.8460\nEpoch 23/100 - Loss: 0.0051, Val Loss: 0.3926, Val Accuracy: 0.8627\nEpoch 24/100 - Loss: 0.0050, Val Loss: 0.4151, Val Accuracy: 0.8583\nStopping early at epoch 25\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.39338877023534574, Validation Accuracy = 0.8599333333333333\nFold 1: Validation Loss = 0.398093506376794, Validation Accuracy = 0.861\nFold 2: Validation Loss = 0.43015647904669985, Validation Accuracy = 0.8513333333333334\nFold 3: Validation Loss = 0.37609021834870604, Validation Accuracy = 0.8672666666666666\nAverage Validation Loss across folds: 0.3994322435018864\nAverage Validation Accuracy across folds: 0.8598833333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0135, Val Loss: 0.7462, Val Accuracy: 0.7168\nEpoch 2/100 - Loss: 0.0085, Val Loss: 0.5816, Val Accuracy: 0.8049\nEpoch 3/100 - Loss: 0.0077, Val Loss: 0.4228, Val Accuracy: 0.8482\nEpoch 4/100 - Loss: 0.0073, Val Loss: 0.4355, Val Accuracy: 0.8460\nEpoch 5/100 - Loss: 0.0070, Val Loss: 0.4290, Val Accuracy: 0.8447\nEpoch 6/100 - Loss: 0.0068, Val Loss: 0.4655, Val Accuracy: 0.8275\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0139, Val Loss: 0.5745, Val Accuracy: 0.7755\nEpoch 2/100 - Loss: 0.0085, Val Loss: 0.5323, Val Accuracy: 0.8029\nEpoch 3/100 - Loss: 0.0076, Val Loss: 0.4416, Val Accuracy: 0.8413\nEpoch 4/100 - Loss: 0.0072, Val Loss: 0.4608, Val Accuracy: 0.8331\nEpoch 5/100 - Loss: 0.0070, Val Loss: 0.4440, Val Accuracy: 0.8362\nEpoch 6/100 - Loss: 0.0068, Val Loss: 0.4572, Val Accuracy: 0.8438\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0147, Val Loss: 0.6639, Val Accuracy: 0.7465\nEpoch 2/100 - Loss: 0.0087, Val Loss: 0.5118, Val Accuracy: 0.8269\nEpoch 3/100 - Loss: 0.0076, Val Loss: 0.4950, Val Accuracy: 0.8260\nEpoch 4/100 - Loss: 0.0073, Val Loss: 0.7773, Val Accuracy: 0.7218\nEpoch 5/100 - Loss: 0.0070, Val Loss: 0.4587, Val Accuracy: 0.8229\nEpoch 6/100 - Loss: 0.0069, Val Loss: 0.4402, Val Accuracy: 0.8394\nEpoch 7/100 - Loss: 0.0067, Val Loss: 0.4286, Val Accuracy: 0.8413\nEpoch 8/100 - Loss: 0.0065, Val Loss: 0.4064, Val Accuracy: 0.8576\nEpoch 9/100 - Loss: 0.0064, Val Loss: 0.4158, Val Accuracy: 0.8523\nEpoch 10/100 - Loss: 0.0062, Val Loss: 0.3926, Val Accuracy: 0.8607\nEpoch 11/100 - Loss: 0.0061, Val Loss: 0.4085, Val Accuracy: 0.8601\nEpoch 12/100 - Loss: 0.0062, Val Loss: 0.4265, Val Accuracy: 0.8271\nEpoch 13/100 - Loss: 0.0061, Val Loss: 0.5793, Val Accuracy: 0.7595\nEpoch 14/100 - Loss: 0.0061, Val Loss: 0.3794, Val Accuracy: 0.8639\nEpoch 15/100 - Loss: 0.0059, Val Loss: 0.3819, Val Accuracy: 0.8651\nEpoch 16/100 - Loss: 0.0059, Val Loss: 0.4307, Val Accuracy: 0.8443\nEpoch 17/100 - Loss: 0.0059, Val Loss: 0.4406, Val Accuracy: 0.8427\nEpoch 18/100 - Loss: 0.0057, Val Loss: 0.3604, Val Accuracy: 0.8716\nEpoch 19/100 - Loss: 0.0057, Val Loss: 0.3998, Val Accuracy: 0.8584\nEpoch 20/100 - Loss: 0.0058, Val Loss: 0.3878, Val Accuracy: 0.8625\nEpoch 21/100 - Loss: 0.0057, Val Loss: 0.3930, Val Accuracy: 0.8699\nStopping early at epoch 22\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0157, Val Loss: 0.5807, Val Accuracy: 0.7693\nEpoch 2/100 - Loss: 0.0084, Val Loss: 0.4969, Val Accuracy: 0.8166\nEpoch 3/100 - Loss: 0.0075, Val Loss: 0.4678, Val Accuracy: 0.8332\nEpoch 4/100 - Loss: 0.0070, Val Loss: 0.4963, Val Accuracy: 0.8165\nEpoch 5/100 - Loss: 0.0068, Val Loss: 0.4965, Val Accuracy: 0.8181\nEpoch 6/100 - Loss: 0.0066, Val Loss: 0.4813, Val Accuracy: 0.8362\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.42180526782857614, Validation Accuracy = 0.8482\nFold 1: Validation Loss = 0.4418250153673456, Validation Accuracy = 0.8412666666666667\nFold 2: Validation Loss = 0.3601872922258174, Validation Accuracy = 0.8716\nFold 3: Validation Loss = 0.466986808814901, Validation Accuracy = 0.8332\nAverage Validation Loss across folds: 0.42270109605916\nAverage Validation Accuracy across folds: 0.8485666666666667\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.2\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0145, Val Loss: 0.9972, Val Accuracy: 0.6769\nEpoch 2/100 - Loss: 0.0087, Val Loss: 0.5679, Val Accuracy: 0.7798\nEpoch 3/100 - Loss: 0.0079, Val Loss: 0.4856, Val Accuracy: 0.8160\nEpoch 4/100 - Loss: 0.0076, Val Loss: 0.4528, Val Accuracy: 0.8340\nEpoch 5/100 - Loss: 0.0073, Val Loss: 0.4050, Val Accuracy: 0.8471\nEpoch 6/100 - Loss: 0.0072, Val Loss: 0.4403, Val Accuracy: 0.8381\nEpoch 7/100 - Loss: 0.0071, Val Loss: 0.3981, Val Accuracy: 0.8523\nEpoch 8/100 - Loss: 0.0069, Val Loss: 0.4267, Val Accuracy: 0.8419\nEpoch 9/100 - Loss: 0.0067, Val Loss: 0.4342, Val Accuracy: 0.8389\nEpoch 10/100 - Loss: 0.0067, Val Loss: 0.3797, Val Accuracy: 0.8573\nEpoch 11/100 - Loss: 0.0067, Val Loss: 0.4323, Val Accuracy: 0.8329\nEpoch 12/100 - Loss: 0.0065, Val Loss: 0.3933, Val Accuracy: 0.8516\nEpoch 13/100 - Loss: 0.0065, Val Loss: 0.3756, Val Accuracy: 0.8634\nEpoch 14/100 - Loss: 0.0065, Val Loss: 0.3918, Val Accuracy: 0.8588\nEpoch 15/100 - Loss: 0.0064, Val Loss: 0.3983, Val Accuracy: 0.8555\nEpoch 16/100 - Loss: 0.0065, Val Loss: 0.4101, Val Accuracy: 0.8415\nStopping early at epoch 17\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0141, Val Loss: 0.6304, Val Accuracy: 0.7747\nEpoch 2/100 - Loss: 0.0094, Val Loss: 0.5978, Val Accuracy: 0.7621\nEpoch 3/100 - Loss: 0.0083, Val Loss: 0.5297, Val Accuracy: 0.8133\nEpoch 4/100 - Loss: 0.0080, Val Loss: 0.4793, Val Accuracy: 0.8308\nEpoch 5/100 - Loss: 0.0077, Val Loss: 0.4409, Val Accuracy: 0.8490\nEpoch 6/100 - Loss: 0.0074, Val Loss: 0.4881, Val Accuracy: 0.8229\nEpoch 7/100 - Loss: 0.0072, Val Loss: 0.4389, Val Accuracy: 0.8446\nEpoch 8/100 - Loss: 0.0071, Val Loss: 0.4334, Val Accuracy: 0.8492\nEpoch 9/100 - Loss: 0.0070, Val Loss: 0.3999, Val Accuracy: 0.8573\nEpoch 10/100 - Loss: 0.0069, Val Loss: 0.4424, Val Accuracy: 0.8345\nEpoch 11/100 - Loss: 0.0068, Val Loss: 0.4030, Val Accuracy: 0.8537\nEpoch 12/100 - Loss: 0.0066, Val Loss: 0.3908, Val Accuracy: 0.8545\nEpoch 13/100 - Loss: 0.0066, Val Loss: 0.3930, Val Accuracy: 0.8592\nEpoch 14/100 - Loss: 0.0065, Val Loss: 0.4348, Val Accuracy: 0.8431\nEpoch 15/100 - Loss: 0.0066, Val Loss: 0.4543, Val Accuracy: 0.8357\nStopping early at epoch 16\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0136, Val Loss: 0.7337, Val Accuracy: 0.7360\nEpoch 2/100 - Loss: 0.0090, Val Loss: 0.5160, Val Accuracy: 0.8143\nEpoch 3/100 - Loss: 0.0082, Val Loss: 0.4720, Val Accuracy: 0.8340\nEpoch 4/100 - Loss: 0.0077, Val Loss: 0.5342, Val Accuracy: 0.7985\nEpoch 5/100 - Loss: 0.0076, Val Loss: 0.5448, Val Accuracy: 0.7973\nEpoch 6/100 - Loss: 0.0074, Val Loss: 0.4398, Val Accuracy: 0.8393\nEpoch 7/100 - Loss: 0.0071, Val Loss: 0.4392, Val Accuracy: 0.8351\nEpoch 8/100 - Loss: 0.0070, Val Loss: 0.4351, Val Accuracy: 0.8500\nEpoch 9/100 - Loss: 0.0070, Val Loss: 0.4117, Val Accuracy: 0.8502\nEpoch 10/100 - Loss: 0.0068, Val Loss: 0.4026, Val Accuracy: 0.8550\nEpoch 11/100 - Loss: 0.0068, Val Loss: 0.3963, Val Accuracy: 0.8544\nEpoch 12/100 - Loss: 0.0067, Val Loss: 0.4500, Val Accuracy: 0.8415\nEpoch 13/100 - Loss: 0.0066, Val Loss: 0.3979, Val Accuracy: 0.8616\nEpoch 14/100 - Loss: 0.0066, Val Loss: 0.4363, Val Accuracy: 0.8342\nStopping early at epoch 15\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0135, Val Loss: 0.6715, Val Accuracy: 0.7301\nEpoch 2/100 - Loss: 0.0090, Val Loss: 0.5065, Val Accuracy: 0.8096\nEpoch 3/100 - Loss: 0.0080, Val Loss: 0.4929, Val Accuracy: 0.8122\nEpoch 4/100 - Loss: 0.0076, Val Loss: 0.4666, Val Accuracy: 0.8142\nEpoch 5/100 - Loss: 0.0074, Val Loss: 0.4799, Val Accuracy: 0.7929\nEpoch 6/100 - Loss: 0.0072, Val Loss: 0.4252, Val Accuracy: 0.8414\nEpoch 7/100 - Loss: 0.0070, Val Loss: 0.4292, Val Accuracy: 0.8344\nEpoch 8/100 - Loss: 0.0070, Val Loss: 0.4283, Val Accuracy: 0.8416\nEpoch 9/100 - Loss: 0.0068, Val Loss: 0.4498, Val Accuracy: 0.8333\nEpoch 10/100 - Loss: 0.0068, Val Loss: 0.4025, Val Accuracy: 0.8515\nEpoch 11/100 - Loss: 0.0067, Val Loss: 0.3929, Val Accuracy: 0.8620\nEpoch 12/100 - Loss: 0.0066, Val Loss: 0.3781, Val Accuracy: 0.8650\nEpoch 13/100 - Loss: 0.0066, Val Loss: 0.3897, Val Accuracy: 0.8559\nEpoch 14/100 - Loss: 0.0065, Val Loss: 0.3854, Val Accuracy: 0.8645\nEpoch 15/100 - Loss: 0.0065, Val Loss: 0.4201, Val Accuracy: 0.8487\nEpoch 16/100 - Loss: 0.0064, Val Loss: 0.3704, Val Accuracy: 0.8595\nEpoch 17/100 - Loss: 0.0064, Val Loss: 0.3893, Val Accuracy: 0.8619\nEpoch 18/100 - Loss: 0.0064, Val Loss: 0.4822, Val Accuracy: 0.8149\nEpoch 19/100 - Loss: 0.0063, Val Loss: 0.3814, Val Accuracy: 0.8629\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.3761543355723645, Validation Accuracy = 0.8634\nFold 1: Validation Loss = 0.3920463411731923, Validation Accuracy = 0.8544666666666667\nFold 2: Validation Loss = 0.39532898876895295, Validation Accuracy = 0.8544\nFold 3: Validation Loss = 0.36997933406779104, Validation Accuracy = 0.8594666666666667\nAverage Validation Loss across folds: 0.3833772498955752\nAverage Validation Accuracy across folds: 0.8579333333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.30000000000000004\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0145, Val Loss: 0.7604, Val Accuracy: 0.7199\nEpoch 2/100 - Loss: 0.0099, Val Loss: 0.6103, Val Accuracy: 0.7728\nEpoch 3/100 - Loss: 0.0089, Val Loss: 0.4969, Val Accuracy: 0.8169\nEpoch 4/100 - Loss: 0.0085, Val Loss: 0.6252, Val Accuracy: 0.8087\nEpoch 5/100 - Loss: 0.0081, Val Loss: 0.5061, Val Accuracy: 0.8245\nEpoch 6/100 - Loss: 0.0078, Val Loss: 0.4784, Val Accuracy: 0.8311\nEpoch 7/100 - Loss: 0.0077, Val Loss: 0.4669, Val Accuracy: 0.8312\nEpoch 8/100 - Loss: 0.0076, Val Loss: 0.5277, Val Accuracy: 0.8097\nEpoch 9/100 - Loss: 0.0075, Val Loss: 0.4571, Val Accuracy: 0.8392\nEpoch 10/100 - Loss: 0.0075, Val Loss: 0.4316, Val Accuracy: 0.8407\nEpoch 11/100 - Loss: 0.0073, Val Loss: 0.4328, Val Accuracy: 0.8416\nEpoch 12/100 - Loss: 0.0072, Val Loss: 0.4703, Val Accuracy: 0.8289\nEpoch 13/100 - Loss: 0.0071, Val Loss: 0.4525, Val Accuracy: 0.8372\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0146, Val Loss: 0.7173, Val Accuracy: 0.7061\nEpoch 2/100 - Loss: 0.0099, Val Loss: 0.5725, Val Accuracy: 0.7883\nEpoch 3/100 - Loss: 0.0090, Val Loss: 0.5208, Val Accuracy: 0.8136\nEpoch 4/100 - Loss: 0.0085, Val Loss: 0.6026, Val Accuracy: 0.7719\nEpoch 5/100 - Loss: 0.0083, Val Loss: 0.4891, Val Accuracy: 0.8248\nEpoch 6/100 - Loss: 0.0080, Val Loss: 0.4617, Val Accuracy: 0.8351\nEpoch 7/100 - Loss: 0.0078, Val Loss: 0.4208, Val Accuracy: 0.8467\nEpoch 8/100 - Loss: 0.0076, Val Loss: 0.4270, Val Accuracy: 0.8395\nEpoch 9/100 - Loss: 0.0075, Val Loss: 0.4294, Val Accuracy: 0.8485\nEpoch 10/100 - Loss: 0.0076, Val Loss: 0.4374, Val Accuracy: 0.8425\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0151, Val Loss: 0.6268, Val Accuracy: 0.7647\nEpoch 2/100 - Loss: 0.0100, Val Loss: 0.8888, Val Accuracy: 0.7232\nEpoch 3/100 - Loss: 0.0090, Val Loss: 0.5041, Val Accuracy: 0.8021\nEpoch 4/100 - Loss: 0.0085, Val Loss: 0.5040, Val Accuracy: 0.8046\nEpoch 5/100 - Loss: 0.0080, Val Loss: 0.4505, Val Accuracy: 0.8421\nEpoch 6/100 - Loss: 0.0079, Val Loss: 0.4365, Val Accuracy: 0.8399\nEpoch 7/100 - Loss: 0.0078, Val Loss: 0.4232, Val Accuracy: 0.8467\nEpoch 8/100 - Loss: 0.0076, Val Loss: 0.4475, Val Accuracy: 0.8415\nEpoch 9/100 - Loss: 0.0075, Val Loss: 0.4436, Val Accuracy: 0.8397\nEpoch 10/100 - Loss: 0.0075, Val Loss: 0.4116, Val Accuracy: 0.8541\nEpoch 11/100 - Loss: 0.0075, Val Loss: 0.4117, Val Accuracy: 0.8524\nEpoch 12/100 - Loss: 0.0073, Val Loss: 0.4015, Val Accuracy: 0.8603\nEpoch 13/100 - Loss: 0.0073, Val Loss: 0.4200, Val Accuracy: 0.8531\nEpoch 14/100 - Loss: 0.0072, Val Loss: 0.4216, Val Accuracy: 0.8502\nEpoch 15/100 - Loss: 0.0071, Val Loss: 0.4207, Val Accuracy: 0.8537\nStopping early at epoch 16\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0155, Val Loss: 0.6625, Val Accuracy: 0.7335\nEpoch 2/100 - Loss: 0.0100, Val Loss: 0.6379, Val Accuracy: 0.7475\nEpoch 3/100 - Loss: 0.0091, Val Loss: 0.5072, Val Accuracy: 0.8157\nEpoch 4/100 - Loss: 0.0086, Val Loss: 0.4262, Val Accuracy: 0.8412\nEpoch 5/100 - Loss: 0.0083, Val Loss: 0.4551, Val Accuracy: 0.8338\nEpoch 6/100 - Loss: 0.0080, Val Loss: 0.4318, Val Accuracy: 0.8457\nEpoch 7/100 - Loss: 0.0079, Val Loss: 0.4059, Val Accuracy: 0.8505\nEpoch 8/100 - Loss: 0.0078, Val Loss: 0.4441, Val Accuracy: 0.8333\nEpoch 9/100 - Loss: 0.0077, Val Loss: 0.4579, Val Accuracy: 0.8307\nEpoch 10/100 - Loss: 0.0076, Val Loss: 0.4033, Val Accuracy: 0.8495\nEpoch 11/100 - Loss: 0.0075, Val Loss: 0.4089, Val Accuracy: 0.8547\nEpoch 12/100 - Loss: 0.0075, Val Loss: 0.3896, Val Accuracy: 0.8595\nEpoch 13/100 - Loss: 0.0073, Val Loss: 0.3829, Val Accuracy: 0.8601\nEpoch 14/100 - Loss: 0.0073, Val Loss: 0.3951, Val Accuracy: 0.8599\nEpoch 15/100 - Loss: 0.0073, Val Loss: 0.4325, Val Accuracy: 0.8383\nEpoch 16/100 - Loss: 0.0071, Val Loss: 0.3805, Val Accuracy: 0.8570\nEpoch 17/100 - Loss: 0.0070, Val Loss: 0.3995, Val Accuracy: 0.8573\nEpoch 18/100 - Loss: 0.0070, Val Loss: 0.4048, Val Accuracy: 0.8478\nEpoch 19/100 - Loss: 0.0070, Val Loss: 0.3824, Val Accuracy: 0.8611\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.43241417205080074, Validation Accuracy = 0.8406666666666667\nFold 1: Validation Loss = 0.4199806311663161, Validation Accuracy = 0.8466666666666667\nFold 2: Validation Loss = 0.4011681196537424, Validation Accuracy = 0.8602666666666666\nFold 3: Validation Loss = 0.380200864501456, Validation Accuracy = 0.857\nAverage Validation Loss across folds: 0.40844094684307886\nAverage Validation Accuracy across folds: 0.8511500000000001\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.4\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0153, Val Loss: 0.6760, Val Accuracy: 0.7079\nEpoch 2/100 - Loss: 0.0108, Val Loss: 0.6038, Val Accuracy: 0.7746\nEpoch 3/100 - Loss: 0.0099, Val Loss: 0.5495, Val Accuracy: 0.7869\nEpoch 4/100 - Loss: 0.0095, Val Loss: 0.4979, Val Accuracy: 0.8145\nEpoch 5/100 - Loss: 0.0092, Val Loss: 0.4765, Val Accuracy: 0.8208\nEpoch 6/100 - Loss: 0.0089, Val Loss: 0.4627, Val Accuracy: 0.8386\nEpoch 7/100 - Loss: 0.0087, Val Loss: 0.4653, Val Accuracy: 0.8333\nEpoch 8/100 - Loss: 0.0085, Val Loss: 0.4544, Val Accuracy: 0.8399\nEpoch 9/100 - Loss: 0.0084, Val Loss: 0.4359, Val Accuracy: 0.8389\nEpoch 10/100 - Loss: 0.0083, Val Loss: 0.4355, Val Accuracy: 0.8407\nEpoch 11/100 - Loss: 0.0082, Val Loss: 0.4437, Val Accuracy: 0.8448\nEpoch 12/100 - Loss: 0.0082, Val Loss: 0.4295, Val Accuracy: 0.8441\nEpoch 13/100 - Loss: 0.0080, Val Loss: 0.4146, Val Accuracy: 0.8519\nEpoch 14/100 - Loss: 0.0080, Val Loss: 0.4315, Val Accuracy: 0.8463\nEpoch 15/100 - Loss: 0.0079, Val Loss: 0.4251, Val Accuracy: 0.8465\nEpoch 16/100 - Loss: 0.0078, Val Loss: 0.4364, Val Accuracy: 0.8479\nStopping early at epoch 17\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0167, Val Loss: 0.6957, Val Accuracy: 0.7241\nEpoch 2/100 - Loss: 0.0111, Val Loss: 0.5813, Val Accuracy: 0.7495\nEpoch 3/100 - Loss: 0.0101, Val Loss: 0.5659, Val Accuracy: 0.7595\nEpoch 4/100 - Loss: 0.0097, Val Loss: 0.5177, Val Accuracy: 0.8009\nEpoch 5/100 - Loss: 0.0093, Val Loss: 0.5031, Val Accuracy: 0.8104\nEpoch 6/100 - Loss: 0.0089, Val Loss: 0.5451, Val Accuracy: 0.7700\nEpoch 7/100 - Loss: 0.0088, Val Loss: 0.4642, Val Accuracy: 0.8327\nEpoch 8/100 - Loss: 0.0086, Val Loss: 0.4353, Val Accuracy: 0.8429\nEpoch 9/100 - Loss: 0.0084, Val Loss: 0.4951, Val Accuracy: 0.8268\nEpoch 10/100 - Loss: 0.0083, Val Loss: 0.4353, Val Accuracy: 0.8495\nEpoch 11/100 - Loss: 0.0081, Val Loss: 0.4275, Val Accuracy: 0.8480\nEpoch 12/100 - Loss: 0.0081, Val Loss: 0.4725, Val Accuracy: 0.8117\nEpoch 13/100 - Loss: 0.0081, Val Loss: 0.4456, Val Accuracy: 0.8431\nEpoch 14/100 - Loss: 0.0080, Val Loss: 0.4491, Val Accuracy: 0.8455\nEpoch 15/100 - Loss: 0.0080, Val Loss: 0.4125, Val Accuracy: 0.8562\nEpoch 16/100 - Loss: 0.0078, Val Loss: 0.4524, Val Accuracy: 0.8457\nEpoch 17/100 - Loss: 0.0078, Val Loss: 0.4148, Val Accuracy: 0.8519\nEpoch 18/100 - Loss: 0.0078, Val Loss: 0.4027, Val Accuracy: 0.8559\nEpoch 19/100 - Loss: 0.0077, Val Loss: 0.4282, Val Accuracy: 0.8490\nEpoch 20/100 - Loss: 0.0077, Val Loss: 0.4249, Val Accuracy: 0.8473\nEpoch 21/100 - Loss: 0.0077, Val Loss: 0.4167, Val Accuracy: 0.8483\nStopping early at epoch 22\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0168, Val Loss: 0.6948, Val Accuracy: 0.7217\nEpoch 2/100 - Loss: 0.0112, Val Loss: 0.5786, Val Accuracy: 0.7732\nEpoch 3/100 - Loss: 0.0100, Val Loss: 0.5121, Val Accuracy: 0.8015\nEpoch 4/100 - Loss: 0.0093, Val Loss: 0.4931, Val Accuracy: 0.8148\nEpoch 5/100 - Loss: 0.0090, Val Loss: 0.4446, Val Accuracy: 0.8429\nEpoch 6/100 - Loss: 0.0088, Val Loss: 0.4467, Val Accuracy: 0.8433\nEpoch 7/100 - Loss: 0.0085, Val Loss: 0.4317, Val Accuracy: 0.8423\nEpoch 8/100 - Loss: 0.0084, Val Loss: 0.4536, Val Accuracy: 0.8423\nEpoch 9/100 - Loss: 0.0084, Val Loss: 0.4484, Val Accuracy: 0.8391\nEpoch 10/100 - Loss: 0.0082, Val Loss: 0.4386, Val Accuracy: 0.8431\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0156, Val Loss: 0.6936, Val Accuracy: 0.7159\nEpoch 2/100 - Loss: 0.0107, Val Loss: 0.5412, Val Accuracy: 0.7902\nEpoch 3/100 - Loss: 0.0099, Val Loss: 0.5445, Val Accuracy: 0.7990\nEpoch 4/100 - Loss: 0.0093, Val Loss: 0.5067, Val Accuracy: 0.8263\nEpoch 5/100 - Loss: 0.0089, Val Loss: 0.4585, Val Accuracy: 0.8359\nEpoch 6/100 - Loss: 0.0087, Val Loss: 0.4565, Val Accuracy: 0.8376\nEpoch 7/100 - Loss: 0.0085, Val Loss: 0.4395, Val Accuracy: 0.8419\nEpoch 8/100 - Loss: 0.0083, Val Loss: 0.4266, Val Accuracy: 0.8489\nEpoch 9/100 - Loss: 0.0083, Val Loss: 0.4452, Val Accuracy: 0.8443\nEpoch 10/100 - Loss: 0.0081, Val Loss: 0.4447, Val Accuracy: 0.8338\nEpoch 11/100 - Loss: 0.0080, Val Loss: 0.4164, Val Accuracy: 0.8524\nEpoch 12/100 - Loss: 0.0079, Val Loss: 0.4426, Val Accuracy: 0.8486\nEpoch 13/100 - Loss: 0.0078, Val Loss: 0.4136, Val Accuracy: 0.8509\nEpoch 14/100 - Loss: 0.0078, Val Loss: 0.4215, Val Accuracy: 0.8563\nEpoch 15/100 - Loss: 0.0078, Val Loss: 0.4173, Val Accuracy: 0.8515\nEpoch 16/100 - Loss: 0.0077, Val Loss: 0.3996, Val Accuracy: 0.8597\nEpoch 17/100 - Loss: 0.0076, Val Loss: 0.4078, Val Accuracy: 0.8525\nEpoch 18/100 - Loss: 0.0076, Val Loss: 0.4071, Val Accuracy: 0.8529\nEpoch 19/100 - Loss: 0.0077, Val Loss: 0.4061, Val Accuracy: 0.8573\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.41576223373413085, Validation Accuracy = 0.8519333333333333\nFold 1: Validation Loss = 0.4040565474236265, Validation Accuracy = 0.8558666666666667\nFold 2: Validation Loss = 0.4315893105369933, Validation Accuracy = 0.8423333333333334\nFold 3: Validation Loss = 0.3999258263314024, Validation Accuracy = 0.8596666666666667\nAverage Validation Loss across folds: 0.41283347950653826\nAverage Validation Accuracy across folds: 0.8524499999999999\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.5\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0175, Val Loss: 0.7656, Val Accuracy: 0.6767\nEpoch 2/100 - Loss: 0.0130, Val Loss: 0.6674, Val Accuracy: 0.7293\nEpoch 3/100 - Loss: 0.0116, Val Loss: 0.6054, Val Accuracy: 0.7841\nEpoch 4/100 - Loss: 0.0108, Val Loss: 0.5525, Val Accuracy: 0.7873\nEpoch 5/100 - Loss: 0.0106, Val Loss: 0.5251, Val Accuracy: 0.8097\nEpoch 6/100 - Loss: 0.0102, Val Loss: 0.5064, Val Accuracy: 0.8169\nEpoch 7/100 - Loss: 0.0099, Val Loss: 0.4917, Val Accuracy: 0.8321\nEpoch 8/100 - Loss: 0.0097, Val Loss: 0.4967, Val Accuracy: 0.8239\nEpoch 9/100 - Loss: 0.0096, Val Loss: 0.4738, Val Accuracy: 0.8328\nEpoch 10/100 - Loss: 0.0095, Val Loss: 0.4716, Val Accuracy: 0.8326\nEpoch 11/100 - Loss: 0.0094, Val Loss: 0.4636, Val Accuracy: 0.8410\nEpoch 12/100 - Loss: 0.0093, Val Loss: 0.4510, Val Accuracy: 0.8393\nEpoch 13/100 - Loss: 0.0093, Val Loss: 0.4557, Val Accuracy: 0.8436\nEpoch 14/100 - Loss: 0.0091, Val Loss: 0.4700, Val Accuracy: 0.8338\nEpoch 15/100 - Loss: 0.0091, Val Loss: 0.4545, Val Accuracy: 0.8435\nStopping early at epoch 16\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0184, Val Loss: 0.7466, Val Accuracy: 0.6954\nEpoch 2/100 - Loss: 0.0124, Val Loss: 0.6205, Val Accuracy: 0.7365\nEpoch 3/100 - Loss: 0.0115, Val Loss: 0.6061, Val Accuracy: 0.7668\nEpoch 4/100 - Loss: 0.0109, Val Loss: 0.5617, Val Accuracy: 0.7899\nEpoch 5/100 - Loss: 0.0106, Val Loss: 0.5498, Val Accuracy: 0.7934\nEpoch 6/100 - Loss: 0.0103, Val Loss: 0.5692, Val Accuracy: 0.7789\nEpoch 7/100 - Loss: 0.0102, Val Loss: 0.5273, Val Accuracy: 0.7999\nEpoch 8/100 - Loss: 0.0099, Val Loss: 0.5096, Val Accuracy: 0.8085\nEpoch 9/100 - Loss: 0.0097, Val Loss: 0.4935, Val Accuracy: 0.8235\nEpoch 10/100 - Loss: 0.0097, Val Loss: 0.4856, Val Accuracy: 0.8311\nEpoch 11/100 - Loss: 0.0095, Val Loss: 0.4764, Val Accuracy: 0.8202\nEpoch 12/100 - Loss: 0.0093, Val Loss: 0.4819, Val Accuracy: 0.8256\nEpoch 13/100 - Loss: 0.0092, Val Loss: 0.4633, Val Accuracy: 0.8303\nEpoch 14/100 - Loss: 0.0091, Val Loss: 0.4575, Val Accuracy: 0.8283\nEpoch 15/100 - Loss: 0.0091, Val Loss: 0.4476, Val Accuracy: 0.8419\nEpoch 16/100 - Loss: 0.0091, Val Loss: 0.4482, Val Accuracy: 0.8422\nEpoch 17/100 - Loss: 0.0089, Val Loss: 0.4538, Val Accuracy: 0.8375\nEpoch 18/100 - Loss: 0.0090, Val Loss: 0.4364, Val Accuracy: 0.8424\nEpoch 19/100 - Loss: 0.0089, Val Loss: 0.4638, Val Accuracy: 0.8327\nEpoch 20/100 - Loss: 0.0089, Val Loss: 0.4531, Val Accuracy: 0.8388\nEpoch 21/100 - Loss: 0.0088, Val Loss: 0.4259, Val Accuracy: 0.8503\nEpoch 22/100 - Loss: 0.0088, Val Loss: 0.4318, Val Accuracy: 0.8502\nEpoch 23/100 - Loss: 0.0088, Val Loss: 0.4354, Val Accuracy: 0.8402\nEpoch 24/100 - Loss: 0.0087, Val Loss: 0.4456, Val Accuracy: 0.8425\nStopping early at epoch 25\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0180, Val Loss: 0.7047, Val Accuracy: 0.7449\nEpoch 2/100 - Loss: 0.0121, Val Loss: 0.5850, Val Accuracy: 0.7670\nEpoch 3/100 - Loss: 0.0112, Val Loss: 0.5353, Val Accuracy: 0.8126\nEpoch 4/100 - Loss: 0.0105, Val Loss: 0.5054, Val Accuracy: 0.8247\nEpoch 5/100 - Loss: 0.0100, Val Loss: 0.5010, Val Accuracy: 0.8250\nEpoch 6/100 - Loss: 0.0098, Val Loss: 0.5026, Val Accuracy: 0.8209\nEpoch 7/100 - Loss: 0.0098, Val Loss: 0.4767, Val Accuracy: 0.8270\nEpoch 8/100 - Loss: 0.0095, Val Loss: 0.4943, Val Accuracy: 0.8290\nEpoch 9/100 - Loss: 0.0093, Val Loss: 0.4559, Val Accuracy: 0.8340\nEpoch 10/100 - Loss: 0.0093, Val Loss: 0.4568, Val Accuracy: 0.8385\nEpoch 11/100 - Loss: 0.0092, Val Loss: 0.4523, Val Accuracy: 0.8361\nEpoch 12/100 - Loss: 0.0092, Val Loss: 0.4675, Val Accuracy: 0.8364\nEpoch 13/100 - Loss: 0.0092, Val Loss: 0.4494, Val Accuracy: 0.8427\nEpoch 14/100 - Loss: 0.0089, Val Loss: 0.4560, Val Accuracy: 0.8419\nEpoch 15/100 - Loss: 0.0090, Val Loss: 0.4678, Val Accuracy: 0.8383\nEpoch 16/100 - Loss: 0.0089, Val Loss: 0.4512, Val Accuracy: 0.8443\nStopping early at epoch 17\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0188, Val Loss: 0.6506, Val Accuracy: 0.7356\nEpoch 2/100 - Loss: 0.0123, Val Loss: 0.6610, Val Accuracy: 0.7343\nEpoch 3/100 - Loss: 0.0112, Val Loss: 0.5919, Val Accuracy: 0.7856\nEpoch 4/100 - Loss: 0.0107, Val Loss: 0.5375, Val Accuracy: 0.7927\nEpoch 5/100 - Loss: 0.0104, Val Loss: 0.5000, Val Accuracy: 0.8313\nEpoch 6/100 - Loss: 0.0100, Val Loss: 0.5263, Val Accuracy: 0.8089\nEpoch 7/100 - Loss: 0.0098, Val Loss: 0.4842, Val Accuracy: 0.8309\nEpoch 8/100 - Loss: 0.0097, Val Loss: 0.4689, Val Accuracy: 0.8343\nEpoch 9/100 - Loss: 0.0095, Val Loss: 0.4838, Val Accuracy: 0.8173\nEpoch 10/100 - Loss: 0.0093, Val Loss: 0.4604, Val Accuracy: 0.8421\nEpoch 11/100 - Loss: 0.0092, Val Loss: 0.4474, Val Accuracy: 0.8471\nEpoch 12/100 - Loss: 0.0092, Val Loss: 0.4432, Val Accuracy: 0.8411\nEpoch 13/100 - Loss: 0.0090, Val Loss: 0.4515, Val Accuracy: 0.8437\nEpoch 14/100 - Loss: 0.0089, Val Loss: 0.4485, Val Accuracy: 0.8464\nEpoch 15/100 - Loss: 0.0089, Val Loss: 0.4421, Val Accuracy: 0.8453\nEpoch 16/100 - Loss: 0.0089, Val Loss: 0.4485, Val Accuracy: 0.8485\nEpoch 17/100 - Loss: 0.0088, Val Loss: 0.4394, Val Accuracy: 0.8518\nEpoch 18/100 - Loss: 0.0087, Val Loss: 0.4254, Val Accuracy: 0.8537\nEpoch 19/100 - Loss: 0.0088, Val Loss: 0.4405, Val Accuracy: 0.8529\nEpoch 20/100 - Loss: 0.0088, Val Loss: 0.4372, Val Accuracy: 0.8555\nEpoch 21/100 - Loss: 0.0087, Val Loss: 0.4322, Val Accuracy: 0.8559\nStopping early at epoch 22\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.4509390001601361, Validation Accuracy = 0.8393333333333334\nFold 1: Validation Loss = 0.4266947881338444, Validation Accuracy = 0.8502666666666666\nFold 2: Validation Loss = 0.45053254492739414, Validation Accuracy = 0.8426666666666667\nFold 3: Validation Loss = 0.4256088764743602, Validation Accuracy = 0.8536666666666667\nAverage Validation Loss across folds: 0.4384438024239337\nAverage Validation Accuracy across folds: 0.8464833333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.6000000000000001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0206, Val Loss: 0.8309, Val Accuracy: 0.6485\nEpoch 2/100 - Loss: 0.0144, Val Loss: 0.6799, Val Accuracy: 0.7097\nEpoch 3/100 - Loss: 0.0130, Val Loss: 0.6096, Val Accuracy: 0.7553\nEpoch 4/100 - Loss: 0.0125, Val Loss: 0.6284, Val Accuracy: 0.7533\nEpoch 5/100 - Loss: 0.0122, Val Loss: 0.6094, Val Accuracy: 0.7585\nEpoch 6/100 - Loss: 0.0119, Val Loss: 0.5651, Val Accuracy: 0.7881\nEpoch 7/100 - Loss: 0.0116, Val Loss: 0.5602, Val Accuracy: 0.7869\nEpoch 8/100 - Loss: 0.0115, Val Loss: 0.5673, Val Accuracy: 0.7821\nEpoch 9/100 - Loss: 0.0114, Val Loss: 0.5620, Val Accuracy: 0.7922\nEpoch 10/100 - Loss: 0.0111, Val Loss: 0.5416, Val Accuracy: 0.7986\nEpoch 11/100 - Loss: 0.0111, Val Loss: 0.5427, Val Accuracy: 0.7989\nEpoch 12/100 - Loss: 0.0110, Val Loss: 0.5364, Val Accuracy: 0.7983\nEpoch 13/100 - Loss: 0.0108, Val Loss: 0.5371, Val Accuracy: 0.8026\nEpoch 14/100 - Loss: 0.0109, Val Loss: 0.5343, Val Accuracy: 0.7985\nEpoch 15/100 - Loss: 0.0108, Val Loss: 0.5261, Val Accuracy: 0.8040\nEpoch 16/100 - Loss: 0.0105, Val Loss: 0.5315, Val Accuracy: 0.8008\nEpoch 17/100 - Loss: 0.0106, Val Loss: 0.5217, Val Accuracy: 0.7863\nEpoch 18/100 - Loss: 0.0107, Val Loss: 0.5213, Val Accuracy: 0.8102\nEpoch 19/100 - Loss: 0.0106, Val Loss: 0.5198, Val Accuracy: 0.8067\nEpoch 20/100 - Loss: 0.0105, Val Loss: 0.5253, Val Accuracy: 0.8099\nEpoch 21/100 - Loss: 0.0105, Val Loss: 0.5271, Val Accuracy: 0.8094\nEpoch 22/100 - Loss: 0.0104, Val Loss: 0.5082, Val Accuracy: 0.8110\nEpoch 23/100 - Loss: 0.0107, Val Loss: 0.5120, Val Accuracy: 0.8059\nEpoch 24/100 - Loss: 0.0105, Val Loss: 0.5303, Val Accuracy: 0.8137\nEpoch 25/100 - Loss: 0.0105, Val Loss: 0.5070, Val Accuracy: 0.8093\nEpoch 26/100 - Loss: 0.0106, Val Loss: 0.5147, Val Accuracy: 0.8045\nEpoch 27/100 - Loss: 0.0105, Val Loss: 0.5158, Val Accuracy: 0.8101\nEpoch 28/100 - Loss: 0.0105, Val Loss: 0.5134, Val Accuracy: 0.7991\nEpoch 29/100 - Loss: 0.0104, Val Loss: 0.4957, Val Accuracy: 0.8118\nEpoch 30/100 - Loss: 0.0103, Val Loss: 0.4990, Val Accuracy: 0.8144\nEpoch 31/100 - Loss: 0.0104, Val Loss: 0.5024, Val Accuracy: 0.8124\nEpoch 32/100 - Loss: 0.0103, Val Loss: 0.4965, Val Accuracy: 0.8189\nStopping early at epoch 33\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0238, Val Loss: 1.0427, Val Accuracy: 0.5513\nEpoch 2/100 - Loss: 0.0161, Val Loss: 0.6829, Val Accuracy: 0.7221\nEpoch 3/100 - Loss: 0.0135, Val Loss: 0.6284, Val Accuracy: 0.7762\nEpoch 4/100 - Loss: 0.0126, Val Loss: 0.6264, Val Accuracy: 0.7439\nEpoch 5/100 - Loss: 0.0122, Val Loss: 0.5886, Val Accuracy: 0.7544\nEpoch 6/100 - Loss: 0.0119, Val Loss: 0.5634, Val Accuracy: 0.7909\nEpoch 7/100 - Loss: 0.0116, Val Loss: 0.5483, Val Accuracy: 0.7979\nEpoch 8/100 - Loss: 0.0113, Val Loss: 0.5473, Val Accuracy: 0.8033\nEpoch 9/100 - Loss: 0.0114, Val Loss: 0.5347, Val Accuracy: 0.8013\nEpoch 10/100 - Loss: 0.0113, Val Loss: 0.5528, Val Accuracy: 0.7848\nEpoch 11/100 - Loss: 0.0111, Val Loss: 0.5502, Val Accuracy: 0.8207\nEpoch 12/100 - Loss: 0.0108, Val Loss: 0.5297, Val Accuracy: 0.8053\nEpoch 13/100 - Loss: 0.0108, Val Loss: 0.5309, Val Accuracy: 0.8237\nEpoch 14/100 - Loss: 0.0109, Val Loss: 0.5074, Val Accuracy: 0.8137\nEpoch 15/100 - Loss: 0.0107, Val Loss: 0.5007, Val Accuracy: 0.8302\nEpoch 16/100 - Loss: 0.0107, Val Loss: 0.5362, Val Accuracy: 0.8161\nEpoch 17/100 - Loss: 0.0107, Val Loss: 0.5073, Val Accuracy: 0.8252\nEpoch 18/100 - Loss: 0.0106, Val Loss: 0.4737, Val Accuracy: 0.8337\nEpoch 19/100 - Loss: 0.0105, Val Loss: 0.4969, Val Accuracy: 0.8369\nEpoch 20/100 - Loss: 0.0104, Val Loss: 0.4878, Val Accuracy: 0.8333\nEpoch 21/100 - Loss: 0.0104, Val Loss: 0.4896, Val Accuracy: 0.8348\nStopping early at epoch 22\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0211, Val Loss: 0.7963, Val Accuracy: 0.6729\nEpoch 2/100 - Loss: 0.0141, Val Loss: 0.6835, Val Accuracy: 0.7205\nEpoch 3/100 - Loss: 0.0130, Val Loss: 0.6544, Val Accuracy: 0.7046\nEpoch 4/100 - Loss: 0.0124, Val Loss: 0.5929, Val Accuracy: 0.7814\nEpoch 5/100 - Loss: 0.0121, Val Loss: 0.5936, Val Accuracy: 0.7783\nEpoch 6/100 - Loss: 0.0118, Val Loss: 0.5560, Val Accuracy: 0.7951\nEpoch 7/100 - Loss: 0.0117, Val Loss: 0.5614, Val Accuracy: 0.7951\nEpoch 8/100 - Loss: 0.0114, Val Loss: 0.5542, Val Accuracy: 0.7953\nEpoch 9/100 - Loss: 0.0112, Val Loss: 0.5419, Val Accuracy: 0.8123\nEpoch 10/100 - Loss: 0.0111, Val Loss: 0.5420, Val Accuracy: 0.8124\nEpoch 11/100 - Loss: 0.0110, Val Loss: 0.5309, Val Accuracy: 0.8075\nEpoch 12/100 - Loss: 0.0109, Val Loss: 0.5235, Val Accuracy: 0.8225\nEpoch 13/100 - Loss: 0.0109, Val Loss: 0.5369, Val Accuracy: 0.8235\nEpoch 14/100 - Loss: 0.0107, Val Loss: 0.5590, Val Accuracy: 0.8273\nEpoch 15/100 - Loss: 0.0106, Val Loss: 0.5153, Val Accuracy: 0.8227\nEpoch 16/100 - Loss: 0.0106, Val Loss: 0.5166, Val Accuracy: 0.8286\nEpoch 17/100 - Loss: 0.0106, Val Loss: 0.5251, Val Accuracy: 0.8216\nEpoch 18/100 - Loss: 0.0106, Val Loss: 0.4969, Val Accuracy: 0.8341\nEpoch 19/100 - Loss: 0.0104, Val Loss: 0.5054, Val Accuracy: 0.8281\nEpoch 20/100 - Loss: 0.0106, Val Loss: 0.5107, Val Accuracy: 0.8280\nEpoch 21/100 - Loss: 0.0105, Val Loss: 0.4886, Val Accuracy: 0.8349\nEpoch 22/100 - Loss: 0.0104, Val Loss: 0.4930, Val Accuracy: 0.8355\nEpoch 23/100 - Loss: 0.0104, Val Loss: 0.5051, Val Accuracy: 0.8385\nEpoch 24/100 - Loss: 0.0104, Val Loss: 0.5021, Val Accuracy: 0.8331\nStopping early at epoch 25\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0216, Val Loss: 0.9356, Val Accuracy: 0.5539\nEpoch 2/100 - Loss: 0.0156, Val Loss: 0.6541, Val Accuracy: 0.7203\nEpoch 3/100 - Loss: 0.0131, Val Loss: 0.6356, Val Accuracy: 0.7345\nEpoch 4/100 - Loss: 0.0125, Val Loss: 0.6459, Val Accuracy: 0.7319\nEpoch 5/100 - Loss: 0.0123, Val Loss: 0.5959, Val Accuracy: 0.7525\nEpoch 6/100 - Loss: 0.0120, Val Loss: 0.5971, Val Accuracy: 0.7474\nEpoch 7/100 - Loss: 0.0117, Val Loss: 0.5904, Val Accuracy: 0.7826\nEpoch 8/100 - Loss: 0.0115, Val Loss: 0.5587, Val Accuracy: 0.7902\nEpoch 9/100 - Loss: 0.0113, Val Loss: 0.5696, Val Accuracy: 0.7881\nEpoch 10/100 - Loss: 0.0113, Val Loss: 0.5298, Val Accuracy: 0.7963\nEpoch 11/100 - Loss: 0.0111, Val Loss: 0.5337, Val Accuracy: 0.7977\nEpoch 12/100 - Loss: 0.0112, Val Loss: 0.5360, Val Accuracy: 0.7968\nEpoch 13/100 - Loss: 0.0109, Val Loss: 0.5521, Val Accuracy: 0.8041\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.4960966891430794, Validation Accuracy = 0.8118\nFold 1: Validation Loss = 0.4734361027149444, Validation Accuracy = 0.8336666666666667\nFold 2: Validation Loss = 0.48792572598508066, Validation Accuracy = 0.8348666666666666\nFold 3: Validation Loss = 0.5301355687861747, Validation Accuracy = 0.7963333333333333\nAverage Validation Loss across folds: 0.49689852165731985\nAverage Validation Accuracy across folds: 0.8191666666666666\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.7000000000000001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0260, Val Loss: 1.1420, Val Accuracy: 0.4323\nEpoch 2/100 - Loss: 0.0199, Val Loss: 1.0383, Val Accuracy: 0.5223\nEpoch 3/100 - Loss: 0.0183, Val Loss: 0.9313, Val Accuracy: 0.5486\nEpoch 4/100 - Loss: 0.0172, Val Loss: 0.8829, Val Accuracy: 0.5879\nEpoch 5/100 - Loss: 0.0166, Val Loss: 0.8435, Val Accuracy: 0.6369\nEpoch 6/100 - Loss: 0.0162, Val Loss: 0.7916, Val Accuracy: 0.6543\nEpoch 7/100 - Loss: 0.0156, Val Loss: 0.7629, Val Accuracy: 0.7153\nEpoch 8/100 - Loss: 0.0146, Val Loss: 0.6714, Val Accuracy: 0.7307\nEpoch 9/100 - Loss: 0.0145, Val Loss: 0.6608, Val Accuracy: 0.7333\nEpoch 10/100 - Loss: 0.0141, Val Loss: 0.6326, Val Accuracy: 0.7320\nEpoch 11/100 - Loss: 0.0139, Val Loss: 0.6345, Val Accuracy: 0.7464\nEpoch 12/100 - Loss: 0.0138, Val Loss: 0.6517, Val Accuracy: 0.7302\nEpoch 13/100 - Loss: 0.0138, Val Loss: 0.6389, Val Accuracy: 0.7396\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0275, Val Loss: 1.2305, Val Accuracy: 0.3511\nEpoch 2/100 - Loss: 0.0203, Val Loss: 1.0595, Val Accuracy: 0.4936\nEpoch 3/100 - Loss: 0.0188, Val Loss: 0.9954, Val Accuracy: 0.5453\nEpoch 4/100 - Loss: 0.0177, Val Loss: 0.9218, Val Accuracy: 0.5529\nEpoch 5/100 - Loss: 0.0170, Val Loss: 0.8921, Val Accuracy: 0.5527\nEpoch 6/100 - Loss: 0.0164, Val Loss: 0.8090, Val Accuracy: 0.6499\nEpoch 7/100 - Loss: 0.0158, Val Loss: 0.8094, Val Accuracy: 0.6721\nEpoch 8/100 - Loss: 0.0154, Val Loss: 0.7823, Val Accuracy: 0.6353\nEpoch 9/100 - Loss: 0.0153, Val Loss: 0.7653, Val Accuracy: 0.6476\nEpoch 10/100 - Loss: 0.0149, Val Loss: 0.7560, Val Accuracy: 0.6539\nEpoch 11/100 - Loss: 0.0148, Val Loss: 0.7349, Val Accuracy: 0.6915\nEpoch 12/100 - Loss: 0.0148, Val Loss: 0.7422, Val Accuracy: 0.6555\nEpoch 13/100 - Loss: 0.0146, Val Loss: 0.6835, Val Accuracy: 0.7179\nEpoch 14/100 - Loss: 0.0145, Val Loss: 0.6873, Val Accuracy: 0.7279\nEpoch 15/100 - Loss: 0.0143, Val Loss: 0.6511, Val Accuracy: 0.7258\nEpoch 16/100 - Loss: 0.0142, Val Loss: 0.6804, Val Accuracy: 0.7720\nEpoch 17/100 - Loss: 0.0140, Val Loss: 0.6545, Val Accuracy: 0.7797\nEpoch 18/100 - Loss: 0.0139, Val Loss: 0.6449, Val Accuracy: 0.7804\nEpoch 19/100 - Loss: 0.0139, Val Loss: 0.6448, Val Accuracy: 0.7435\nEpoch 20/100 - Loss: 0.0138, Val Loss: 0.6171, Val Accuracy: 0.7743\nEpoch 21/100 - Loss: 0.0138, Val Loss: 0.6491, Val Accuracy: 0.7389\nEpoch 22/100 - Loss: 0.0138, Val Loss: 0.6361, Val Accuracy: 0.7647\nEpoch 23/100 - Loss: 0.0138, Val Loss: 0.6344, Val Accuracy: 0.7492\nStopping early at epoch 24\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0262, Val Loss: 1.1150, Val Accuracy: 0.4625\nEpoch 2/100 - Loss: 0.0197, Val Loss: 0.9466, Val Accuracy: 0.5851\nEpoch 3/100 - Loss: 0.0178, Val Loss: 0.8603, Val Accuracy: 0.6177\nEpoch 4/100 - Loss: 0.0168, Val Loss: 0.7914, Val Accuracy: 0.6750\nEpoch 5/100 - Loss: 0.0161, Val Loss: 0.8027, Val Accuracy: 0.6830\nEpoch 6/100 - Loss: 0.0156, Val Loss: 0.7118, Val Accuracy: 0.7099\nEpoch 7/100 - Loss: 0.0149, Val Loss: 0.6860, Val Accuracy: 0.7276\nEpoch 8/100 - Loss: 0.0146, Val Loss: 0.6524, Val Accuracy: 0.7403\nEpoch 9/100 - Loss: 0.0143, Val Loss: 0.6488, Val Accuracy: 0.7371\nEpoch 10/100 - Loss: 0.0140, Val Loss: 0.6323, Val Accuracy: 0.7563\nEpoch 11/100 - Loss: 0.0140, Val Loss: 0.6465, Val Accuracy: 0.6934\nEpoch 12/100 - Loss: 0.0137, Val Loss: 0.6302, Val Accuracy: 0.7175\nEpoch 13/100 - Loss: 0.0138, Val Loss: 0.6207, Val Accuracy: 0.7446\nEpoch 14/100 - Loss: 0.0137, Val Loss: 0.6163, Val Accuracy: 0.7535\nEpoch 15/100 - Loss: 0.0136, Val Loss: 0.6199, Val Accuracy: 0.7473\nEpoch 16/100 - Loss: 0.0137, Val Loss: 0.6178, Val Accuracy: 0.7413\nEpoch 17/100 - Loss: 0.0136, Val Loss: 0.6273, Val Accuracy: 0.7349\nStopping early at epoch 18\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0275, Val Loss: 1.3070, Val Accuracy: 0.3988\nEpoch 2/100 - Loss: 0.0202, Val Loss: 0.9809, Val Accuracy: 0.5804\nEpoch 3/100 - Loss: 0.0179, Val Loss: 0.8524, Val Accuracy: 0.6129\nEpoch 4/100 - Loss: 0.0165, Val Loss: 0.7646, Val Accuracy: 0.6907\nEpoch 5/100 - Loss: 0.0156, Val Loss: 0.7166, Val Accuracy: 0.7151\nEpoch 6/100 - Loss: 0.0149, Val Loss: 0.6970, Val Accuracy: 0.7266\nEpoch 7/100 - Loss: 0.0146, Val Loss: 0.6853, Val Accuracy: 0.7218\nEpoch 8/100 - Loss: 0.0140, Val Loss: 0.6594, Val Accuracy: 0.7401\nEpoch 9/100 - Loss: 0.0141, Val Loss: 0.6877, Val Accuracy: 0.7155\nEpoch 10/100 - Loss: 0.0140, Val Loss: 0.6628, Val Accuracy: 0.7270\nEpoch 11/100 - Loss: 0.0139, Val Loss: 0.6611, Val Accuracy: 0.7376\nEpoch 12/100 - Loss: 0.0138, Val Loss: 0.6569, Val Accuracy: 0.7386\nEpoch 13/100 - Loss: 0.0136, Val Loss: 0.6666, Val Accuracy: 0.7321\nEpoch 14/100 - Loss: 0.0135, Val Loss: 0.6526, Val Accuracy: 0.7351\nEpoch 15/100 - Loss: 0.0135, Val Loss: 0.6422, Val Accuracy: 0.7425\nEpoch 16/100 - Loss: 0.0135, Val Loss: 0.6343, Val Accuracy: 0.7369\nEpoch 17/100 - Loss: 0.0133, Val Loss: 0.6380, Val Accuracy: 0.7431\nEpoch 18/100 - Loss: 0.0134, Val Loss: 0.6429, Val Accuracy: 0.7346\nEpoch 19/100 - Loss: 0.0135, Val Loss: 0.6412, Val Accuracy: 0.7413\nStopping early at epoch 20\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.6331945125092852, Validation Accuracy = 0.732\nFold 1: Validation Loss = 0.6169780119936517, Validation Accuracy = 0.7743333333333333\nFold 2: Validation Loss = 0.6168175093671109, Validation Accuracy = 0.7535333333333334\nFold 3: Validation Loss = 0.6341009737329281, Validation Accuracy = 0.7368666666666667\nAverage Validation Loss across folds: 0.6252727519007439\nAverage Validation Accuracy across folds: 0.7491833333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.8\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0307, Val Loss: 1.6540, Val Accuracy: 0.2332\nEpoch 2/100 - Loss: 0.0264, Val Loss: 1.5284, Val Accuracy: 0.2943\nEpoch 3/100 - Loss: 0.0253, Val Loss: 1.4759, Val Accuracy: 0.2929\nEpoch 4/100 - Loss: 0.0248, Val Loss: 1.3515, Val Accuracy: 0.3939\nEpoch 5/100 - Loss: 0.0231, Val Loss: 1.2401, Val Accuracy: 0.3823\nEpoch 6/100 - Loss: 0.0222, Val Loss: 1.2572, Val Accuracy: 0.3895\nEpoch 7/100 - Loss: 0.0221, Val Loss: 1.2167, Val Accuracy: 0.3889\nEpoch 8/100 - Loss: 0.0219, Val Loss: 1.2111, Val Accuracy: 0.4237\nEpoch 9/100 - Loss: 0.0219, Val Loss: 1.2083, Val Accuracy: 0.4037\nEpoch 10/100 - Loss: 0.0216, Val Loss: 1.2011, Val Accuracy: 0.4153\nEpoch 11/100 - Loss: 0.0215, Val Loss: 1.1991, Val Accuracy: 0.3911\nEpoch 12/100 - Loss: 0.0214, Val Loss: 1.1966, Val Accuracy: 0.4654\nEpoch 13/100 - Loss: 0.0212, Val Loss: 1.1660, Val Accuracy: 0.4833\nEpoch 14/100 - Loss: 0.0210, Val Loss: 1.1615, Val Accuracy: 0.4685\nEpoch 15/100 - Loss: 0.0217, Val Loss: 1.1540, Val Accuracy: 0.4733\nEpoch 16/100 - Loss: 0.0208, Val Loss: 1.1322, Val Accuracy: 0.4733\nEpoch 17/100 - Loss: 0.0208, Val Loss: 1.1165, Val Accuracy: 0.4819\nEpoch 18/100 - Loss: 0.0206, Val Loss: 1.1336, Val Accuracy: 0.4611\nEpoch 19/100 - Loss: 0.0207, Val Loss: 1.1168, Val Accuracy: 0.4564\nEpoch 20/100 - Loss: 0.0206, Val Loss: 1.1375, Val Accuracy: 0.4676\nStopping early at epoch 21\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0332, Val Loss: 1.7106, Val Accuracy: 0.2039\nEpoch 2/100 - Loss: 0.0275, Val Loss: 1.4895, Val Accuracy: 0.2892\nEpoch 3/100 - Loss: 0.0255, Val Loss: 1.2730, Val Accuracy: 0.3871\nEpoch 4/100 - Loss: 0.0232, Val Loss: 1.2180, Val Accuracy: 0.3715\nEpoch 5/100 - Loss: 0.0223, Val Loss: 1.1787, Val Accuracy: 0.4028\nEpoch 6/100 - Loss: 0.0218, Val Loss: 1.1421, Val Accuracy: 0.4337\nEpoch 7/100 - Loss: 0.0211, Val Loss: 1.0887, Val Accuracy: 0.4674\nEpoch 8/100 - Loss: 0.0208, Val Loss: 1.0942, Val Accuracy: 0.4699\nEpoch 9/100 - Loss: 0.0206, Val Loss: 1.0894, Val Accuracy: 0.5023\nEpoch 10/100 - Loss: 0.0205, Val Loss: 1.1441, Val Accuracy: 0.4671\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0359, Val Loss: 1.8481, Val Accuracy: 0.2003\nEpoch 2/100 - Loss: 0.0276, Val Loss: 1.5267, Val Accuracy: 0.2979\nEpoch 3/100 - Loss: 0.0253, Val Loss: 1.4785, Val Accuracy: 0.3244\nEpoch 4/100 - Loss: 0.0244, Val Loss: 1.4147, Val Accuracy: 0.3703\nEpoch 5/100 - Loss: 0.0240, Val Loss: 1.4077, Val Accuracy: 0.3763\nEpoch 6/100 - Loss: 0.0238, Val Loss: 1.3783, Val Accuracy: 0.4177\nEpoch 7/100 - Loss: 0.0233, Val Loss: 1.2397, Val Accuracy: 0.4588\nEpoch 8/100 - Loss: 0.0223, Val Loss: 1.1800, Val Accuracy: 0.4506\nEpoch 9/100 - Loss: 0.0218, Val Loss: 1.1090, Val Accuracy: 0.5086\nEpoch 10/100 - Loss: 0.0213, Val Loss: 1.0703, Val Accuracy: 0.5340\nEpoch 11/100 - Loss: 0.0210, Val Loss: 1.0734, Val Accuracy: 0.5399\nEpoch 12/100 - Loss: 0.0209, Val Loss: 1.0547, Val Accuracy: 0.5553\nEpoch 13/100 - Loss: 0.0208, Val Loss: 1.0393, Val Accuracy: 0.5550\nEpoch 14/100 - Loss: 0.0203, Val Loss: 1.0234, Val Accuracy: 0.5852\nEpoch 15/100 - Loss: 0.0203, Val Loss: 1.0191, Val Accuracy: 0.5543\nEpoch 16/100 - Loss: 0.0201, Val Loss: 1.0264, Val Accuracy: 0.5573\nEpoch 17/100 - Loss: 0.0200, Val Loss: 1.0010, Val Accuracy: 0.5939\nEpoch 18/100 - Loss: 0.0198, Val Loss: 1.0145, Val Accuracy: 0.5493\nEpoch 19/100 - Loss: 0.0198, Val Loss: 0.9956, Val Accuracy: 0.5603\nEpoch 20/100 - Loss: 0.0197, Val Loss: 0.9581, Val Accuracy: 0.5685\nEpoch 21/100 - Loss: 0.0195, Val Loss: 0.9684, Val Accuracy: 0.5547\nEpoch 22/100 - Loss: 0.0193, Val Loss: 0.9436, Val Accuracy: 0.5665\nEpoch 23/100 - Loss: 0.0196, Val Loss: 0.9408, Val Accuracy: 0.5735\nEpoch 24/100 - Loss: 0.0194, Val Loss: 0.9579, Val Accuracy: 0.6063\nEpoch 25/100 - Loss: 0.0193, Val Loss: 0.9497, Val Accuracy: 0.5759\nEpoch 26/100 - Loss: 0.0199, Val Loss: 0.9272, Val Accuracy: 0.5573\nEpoch 27/100 - Loss: 0.0193, Val Loss: 0.9357, Val Accuracy: 0.5655\nEpoch 28/100 - Loss: 0.0192, Val Loss: 0.9495, Val Accuracy: 0.6099\nEpoch 29/100 - Loss: 0.0197, Val Loss: 0.9278, Val Accuracy: 0.5522\nStopping early at epoch 30\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0310, Val Loss: 1.5845, Val Accuracy: 0.2816\nEpoch 2/100 - Loss: 0.0261, Val Loss: 1.4850, Val Accuracy: 0.2979\nEpoch 3/100 - Loss: 0.0243, Val Loss: 1.2504, Val Accuracy: 0.3813\nEpoch 4/100 - Loss: 0.0227, Val Loss: 1.2267, Val Accuracy: 0.3814\nEpoch 5/100 - Loss: 0.0222, Val Loss: 1.2396, Val Accuracy: 0.3820\nEpoch 6/100 - Loss: 0.0219, Val Loss: 1.2430, Val Accuracy: 0.3891\nEpoch 7/100 - Loss: 0.0218, Val Loss: 1.2083, Val Accuracy: 0.3855\nEpoch 8/100 - Loss: 0.0217, Val Loss: 1.1930, Val Accuracy: 0.3881\nEpoch 9/100 - Loss: 0.0216, Val Loss: 1.1814, Val Accuracy: 0.4075\nEpoch 10/100 - Loss: 0.0212, Val Loss: 1.2066, Val Accuracy: 0.3877\nEpoch 11/100 - Loss: 0.0214, Val Loss: 1.1777, Val Accuracy: 0.3889\nEpoch 12/100 - Loss: 0.0213, Val Loss: 1.1646, Val Accuracy: 0.4226\nEpoch 13/100 - Loss: 0.0212, Val Loss: 1.1123, Val Accuracy: 0.4793\nEpoch 14/100 - Loss: 0.0210, Val Loss: 1.1011, Val Accuracy: 0.4733\nEpoch 15/100 - Loss: 0.0207, Val Loss: 1.1115, Val Accuracy: 0.5345\nEpoch 16/100 - Loss: 0.0205, Val Loss: 1.1119, Val Accuracy: 0.4851\nEpoch 17/100 - Loss: 0.0206, Val Loss: 1.0813, Val Accuracy: 0.4643\nEpoch 18/100 - Loss: 0.0204, Val Loss: 1.0627, Val Accuracy: 0.4815\nEpoch 19/100 - Loss: 0.0203, Val Loss: 1.1073, Val Accuracy: 0.4609\nEpoch 20/100 - Loss: 0.0204, Val Loss: 1.0307, Val Accuracy: 0.5469\nEpoch 21/100 - Loss: 0.0201, Val Loss: 1.0035, Val Accuracy: 0.5457\nEpoch 22/100 - Loss: 0.0200, Val Loss: 0.9902, Val Accuracy: 0.5627\nEpoch 23/100 - Loss: 0.0198, Val Loss: 0.9980, Val Accuracy: 0.5593\nEpoch 24/100 - Loss: 0.0198, Val Loss: 0.9803, Val Accuracy: 0.5489\nEpoch 25/100 - Loss: 0.0199, Val Loss: 0.9659, Val Accuracy: 0.5695\nEpoch 26/100 - Loss: 0.0199, Val Loss: 0.9451, Val Accuracy: 0.5709\nEpoch 27/100 - Loss: 0.0197, Val Loss: 0.9809, Val Accuracy: 0.5596\nEpoch 28/100 - Loss: 0.0196, Val Loss: 0.9791, Val Accuracy: 0.5423\nEpoch 29/100 - Loss: 0.0195, Val Loss: 0.9938, Val Accuracy: 0.5706\nStopping early at epoch 30\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.116874812004414, Validation Accuracy = 0.4819333333333333\nFold 1: Validation Loss = 1.0887298857912104, Validation Accuracy = 0.4674\nFold 2: Validation Loss = 0.9241859925554153, Validation Accuracy = 0.5572666666666667\nFold 3: Validation Loss = 0.9458217945504696, Validation Accuracy = 0.5708666666666666\nAverage Validation Loss across folds: 1.0189031212253774\nAverage Validation Accuracy across folds: 0.5193666666666668\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.9\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0367, Val Loss: 2.3031, Val Accuracy: 0.0993\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3048, Val Accuracy: 0.0998\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3040, Val Accuracy: 0.0993\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3056, Val Accuracy: 0.0998\nEpoch 5/100 - Loss: 0.0363, Val Loss: 2.3031, Val Accuracy: 0.0993\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3031, Val Accuracy: 0.1019\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3049, Val Accuracy: 0.0961\nEpoch 8/100 - Loss: 0.0363, Val Loss: 2.3036, Val Accuracy: 0.0997\nEpoch 9/100 - Loss: 0.0363, Val Loss: 2.3035, Val Accuracy: 0.0998\nStopping early at epoch 10\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0367, Val Loss: 2.3054, Val Accuracy: 0.0975\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3032, Val Accuracy: 0.1026\nEpoch 3/100 - Loss: 0.0363, Val Loss: 2.3037, Val Accuracy: 0.0989\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3044, Val Accuracy: 0.1005\nEpoch 5/100 - Loss: 0.0362, Val Loss: 2.3027, Val Accuracy: 0.1026\nEpoch 6/100 - Loss: 0.0362, Val Loss: 2.3047, Val Accuracy: 0.0975\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3037, Val Accuracy: 0.0971\nEpoch 8/100 - Loss: 0.0362, Val Loss: 2.3033, Val Accuracy: 0.0997\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0367, Val Loss: 2.3036, Val Accuracy: 0.0979\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3038, Val Accuracy: 0.1013\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3042, Val Accuracy: 0.0993\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3030, Val Accuracy: 0.1018\nEpoch 5/100 - Loss: 0.0363, Val Loss: 2.3033, Val Accuracy: 0.1018\nEpoch 6/100 - Loss: 0.0363, Val Loss: 2.3034, Val Accuracy: 0.0978\nEpoch 7/100 - Loss: 0.0362, Val Loss: 2.3030, Val Accuracy: 0.0978\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0367, Val Loss: 2.3040, Val Accuracy: 0.0948\nEpoch 2/100 - Loss: 0.0362, Val Loss: 2.3031, Val Accuracy: 0.1025\nEpoch 3/100 - Loss: 0.0362, Val Loss: 2.3032, Val Accuracy: 0.1015\nEpoch 4/100 - Loss: 0.0362, Val Loss: 2.3037, Val Accuracy: 0.0991\nEpoch 5/100 - Loss: 0.0363, Val Loss: 2.3035, Val Accuracy: 0.1003\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3030511450260245, Validation Accuracy = 0.10193333333333333\nFold 1: Validation Loss = 2.3027690126540814, Validation Accuracy = 0.1026\nFold 2: Validation Loss = 2.3029771409136184, Validation Accuracy = 0.1018\nFold 3: Validation Loss = 2.3031324701106293, Validation Accuracy = 0.10246666666666666\nAverage Validation Loss across folds: 2.3029824421760883\nAverage Validation Accuracy across folds: 0.1022\n--------------------------------\nK-fold Cross-validation completed.\n   dropout_rate  train_loss  val_loss  val_accuracy\n0           0.0    0.399432  0.859883      0.006611\n1           0.1    0.422701  0.848567      0.007496\n2           0.2    0.383377  0.857933      0.007475\n3           0.3    0.408441  0.851150      0.008318\n4           0.4    0.412833  0.852450      0.008976\n5           0.5    0.438444  0.846483      0.010086\n6           0.6    0.496899  0.819167      0.011739\n7           0.7    0.625273  0.749183      0.015652\n8           0.8    1.018903  0.519367      0.021895\n9           0.9    2.302982  0.102200      0.036298\n\n\n\n# rename columns as they were ordered wrong (fixing)\nresults_choice_2.rename(columns={'train_loss': 'average_val_loss'}, inplace=True)\nresults_choice_2.rename(columns={'val_loss': 'average_val_accuracy'}, inplace=True)\nresults_choice_2.rename(columns={'val_accuracy': 'average_train_loss'}, inplace=True)\n\n# save results_df2 to csv\nresults_choice_2.to_csv('results/results_choice_2.csv')\n\n\n# display results\nresults_choice_2\n\n\n\n\n\n\n\n\ndropout_rate\naverage_val_loss\naverage_val_accuracy\naverage_train_loss\n\n\n\n\n0\n0.0\n0.399432\n0.859883\n0.006611\n\n\n1\n0.1\n0.422701\n0.848567\n0.007496\n\n\n2\n0.2\n0.383377\n0.857933\n0.007475\n\n\n3\n0.3\n0.408441\n0.851150\n0.008318\n\n\n4\n0.4\n0.412833\n0.852450\n0.008976\n\n\n5\n0.5\n0.438444\n0.846483\n0.010086\n\n\n6\n0.6\n0.496899\n0.819167\n0.011739\n\n\n7\n0.7\n0.625273\n0.749183\n0.015652\n\n\n8\n0.8\n1.018903\n0.519367\n0.021895\n\n\n9\n0.9\n2.302982\n0.102200\n0.036298\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nresults_choice_2 = pd.read_csv('results/results_choice_2.csv')\n\n# Plot\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(results_choice_2['dropout_rate'], results_choice_2['average_train_loss'], label='Training Loss', marker='o', color='green')\nplt.plot(results_choice_2['dropout_rate'], results_choice_2['average_val_loss'], label='Validation Loss', marker='o', color='red')\nplt.title('Training and Validation Loss vs. Dropout Rate')\nplt.xlabel('Dropout Rate')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot\nplt.subplot(1, 2, 2)\nerror_ratio = results_choice_2['average_val_loss'] / results_choice_2['average_train_loss']\nplt.plot(results_choice_2['dropout_rate'], error_ratio, label='Val/Train Loss Ratio', marker='o')\nplt.title('Error Ratio vs. Dropout Rate')\nplt.xlabel('Dropout Rate')\nplt.ylabel('Ratio')\nplt.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ANN.html#choice-3",
    "href": "ANN.html#choice-3",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "for hidden activation=relu, hidden layers = [96,96,96], optimizer=adam\nVary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization)\nPlot training and validation as a function of dropout rate\n\nPlot the ratio (or difference) of validation to training error as a function of dropout rate\n\n\ndef model_fn3(dropout_rate=0.0):\n    return NeuralNetwork(input_size=28*28, layer_sizes=[96, 96, 96], activation_funcs=['relu', 'relu', 'relu'], dropout_rate=dropout_rate)\n\ndropout_rates = np.linspace(0.0, 0.9, 10) \n\nresults3 = []\n\nfor dropout_rate in dropout_rates:\n    print(f\"Training with Dropout Rate: {dropout_rate}\")\n    avg_train_loss, avg_val_loss, avg_val_accuracy = training_model_kfold(\n        k_folds=2,\n        num_epochs=100,\n        dataset=training_data,\n        model_fn=lambda: model_fn3(dropout_rate=dropout_rate),\n        loss_fn=nn.CrossEntropyLoss(),\n        optimizer_name=\"Adam\",\n        lr=0.01, \n        l1_reg=0,   \n        l2_reg=0,\n        device=device,\n        patience=4,\n        plot=False)\n    \n    results3.append({\n        'dropout_rate': dropout_rate,\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n        'val_accuracy': avg_val_accuracy})\n\nresults_choice_3 = pd.DataFrame(results3)\nprint(results_choice_3)\n\nTraining with Dropout Rate: 0.0\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0098, Val Loss: 0.5491, Val Accuracy: 0.8206\nEpoch 2/100 - Loss: 0.0070, Val Loss: 0.4296, Val Accuracy: 0.8517\nEpoch 3/100 - Loss: 0.0065, Val Loss: 0.4732, Val Accuracy: 0.8323\nEpoch 4/100 - Loss: 0.0062, Val Loss: 0.5201, Val Accuracy: 0.8206\nEpoch 5/100 - Loss: 0.0060, Val Loss: 0.4274, Val Accuracy: 0.8539\nEpoch 6/100 - Loss: 0.0059, Val Loss: 0.4200, Val Accuracy: 0.8551\nEpoch 7/100 - Loss: 0.0057, Val Loss: 0.4063, Val Accuracy: 0.8614\nEpoch 8/100 - Loss: 0.0057, Val Loss: 0.4160, Val Accuracy: 0.8585\nEpoch 9/100 - Loss: 0.0056, Val Loss: 0.3955, Val Accuracy: 0.8626\nEpoch 10/100 - Loss: 0.0057, Val Loss: 0.4452, Val Accuracy: 0.8545\nEpoch 11/100 - Loss: 0.0053, Val Loss: 0.4184, Val Accuracy: 0.8596\nEpoch 12/100 - Loss: 0.0053, Val Loss: 0.4096, Val Accuracy: 0.8604\nStopping early at epoch 13\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0098, Val Loss: 0.4839, Val Accuracy: 0.8247\nEpoch 2/100 - Loss: 0.0073, Val Loss: 0.4649, Val Accuracy: 0.8188\nEpoch 3/100 - Loss: 0.0067, Val Loss: 0.4727, Val Accuracy: 0.8407\nEpoch 4/100 - Loss: 0.0063, Val Loss: 0.4230, Val Accuracy: 0.8535\nEpoch 5/100 - Loss: 0.0064, Val Loss: 0.4133, Val Accuracy: 0.8573\nEpoch 6/100 - Loss: 0.0062, Val Loss: 0.4242, Val Accuracy: 0.8513\nEpoch 7/100 - Loss: 0.0061, Val Loss: 0.4338, Val Accuracy: 0.8493\nEpoch 8/100 - Loss: 0.0061, Val Loss: 0.4527, Val Accuracy: 0.8339\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.39545452480377163, Validation Accuracy = 0.8626\nFold 1: Validation Loss = 0.4133657828005138, Validation Accuracy = 0.8573333333333333\nAverage Validation Loss across folds: 0.40441015380214274\nAverage Validation Accuracy across folds: 0.8599666666666667\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.1\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0108, Val Loss: 0.5447, Val Accuracy: 0.8124\nEpoch 2/100 - Loss: 0.0087, Val Loss: 0.5180, Val Accuracy: 0.8059\nEpoch 3/100 - Loss: 0.0083, Val Loss: 0.6249, Val Accuracy: 0.7878\nEpoch 4/100 - Loss: 0.0081, Val Loss: 0.4612, Val Accuracy: 0.8344\nEpoch 5/100 - Loss: 0.0079, Val Loss: 0.4898, Val Accuracy: 0.8184\nEpoch 6/100 - Loss: 0.0077, Val Loss: 0.4968, Val Accuracy: 0.8356\nEpoch 7/100 - Loss: 0.0077, Val Loss: 0.4967, Val Accuracy: 0.8346\nEpoch 8/100 - Loss: 0.0075, Val Loss: 0.4430, Val Accuracy: 0.8398\nEpoch 9/100 - Loss: 0.0074, Val Loss: 0.4704, Val Accuracy: 0.8399\nEpoch 10/100 - Loss: 0.0076, Val Loss: 0.4747, Val Accuracy: 0.8393\nEpoch 11/100 - Loss: 0.0076, Val Loss: 0.4762, Val Accuracy: 0.8389\nStopping early at epoch 12\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0112, Val Loss: 0.5145, Val Accuracy: 0.8150\nEpoch 2/100 - Loss: 0.0089, Val Loss: 0.6359, Val Accuracy: 0.8161\nEpoch 3/100 - Loss: 0.0087, Val Loss: 0.5082, Val Accuracy: 0.8140\nEpoch 4/100 - Loss: 0.0087, Val Loss: 0.4684, Val Accuracy: 0.8407\nEpoch 5/100 - Loss: 0.0083, Val Loss: 0.4925, Val Accuracy: 0.8277\nEpoch 6/100 - Loss: 0.0082, Val Loss: 0.4487, Val Accuracy: 0.8458\nEpoch 7/100 - Loss: 0.0077, Val Loss: 0.4398, Val Accuracy: 0.8493\nEpoch 8/100 - Loss: 0.0080, Val Loss: 0.4338, Val Accuracy: 0.8541\nEpoch 9/100 - Loss: 0.0078, Val Loss: 0.4431, Val Accuracy: 0.8476\nEpoch 10/100 - Loss: 0.0077, Val Loss: 0.4262, Val Accuracy: 0.8525\nEpoch 11/100 - Loss: 0.0077, Val Loss: 0.4548, Val Accuracy: 0.8427\nEpoch 12/100 - Loss: 0.0080, Val Loss: 0.4893, Val Accuracy: 0.8350\nEpoch 13/100 - Loss: 0.0076, Val Loss: 0.4807, Val Accuracy: 0.8342\nStopping early at epoch 14\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.44300877285410345, Validation Accuracy = 0.8397666666666667\nFold 1: Validation Loss = 0.4261147111400104, Validation Accuracy = 0.8525\nAverage Validation Loss across folds: 0.43456174199705694\nAverage Validation Accuracy across folds: 0.8461333333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.2\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0121, Val Loss: 0.5563, Val Accuracy: 0.7988\nEpoch 2/100 - Loss: 0.0101, Val Loss: 0.5211, Val Accuracy: 0.8208\nEpoch 3/100 - Loss: 0.0099, Val Loss: 0.4837, Val Accuracy: 0.8216\nEpoch 4/100 - Loss: 0.0095, Val Loss: 0.4971, Val Accuracy: 0.8363\nEpoch 5/100 - Loss: 0.0093, Val Loss: 0.4973, Val Accuracy: 0.8338\nEpoch 6/100 - Loss: 0.0093, Val Loss: 0.5189, Val Accuracy: 0.8233\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0123, Val Loss: 0.5502, Val Accuracy: 0.8066\nEpoch 2/100 - Loss: 0.0103, Val Loss: 0.5314, Val Accuracy: 0.8066\nEpoch 3/100 - Loss: 0.0097, Val Loss: 0.5155, Val Accuracy: 0.8196\nEpoch 4/100 - Loss: 0.0101, Val Loss: 0.4947, Val Accuracy: 0.8238\nEpoch 5/100 - Loss: 0.0097, Val Loss: 0.4686, Val Accuracy: 0.8373\nEpoch 6/100 - Loss: 0.0093, Val Loss: 0.5167, Val Accuracy: 0.8300\nEpoch 7/100 - Loss: 0.0095, Val Loss: 0.5008, Val Accuracy: 0.8208\nEpoch 8/100 - Loss: 0.0093, Val Loss: 0.5024, Val Accuracy: 0.8314\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.4835835076662015, Validation Accuracy = 0.8215666666666667\nFold 1: Validation Loss = 0.4686416948019569, Validation Accuracy = 0.8373333333333334\nAverage Validation Loss across folds: 0.47611260123407917\nAverage Validation Accuracy across folds: 0.82945\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.30000000000000004\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0141, Val Loss: 0.6420, Val Accuracy: 0.7581\nEpoch 2/100 - Loss: 0.0120, Val Loss: 0.5877, Val Accuracy: 0.7855\nEpoch 3/100 - Loss: 0.0119, Val Loss: 0.5830, Val Accuracy: 0.7896\nEpoch 4/100 - Loss: 0.0123, Val Loss: 0.6106, Val Accuracy: 0.7738\nEpoch 5/100 - Loss: 0.0121, Val Loss: 0.5829, Val Accuracy: 0.7804\nEpoch 6/100 - Loss: 0.0119, Val Loss: 0.5796, Val Accuracy: 0.7982\nEpoch 7/100 - Loss: 0.0119, Val Loss: 0.5898, Val Accuracy: 0.7951\nEpoch 8/100 - Loss: 0.0116, Val Loss: 0.5699, Val Accuracy: 0.7919\nEpoch 9/100 - Loss: 0.0115, Val Loss: 0.6045, Val Accuracy: 0.7967\nEpoch 10/100 - Loss: 0.0121, Val Loss: 0.5799, Val Accuracy: 0.7992\nEpoch 11/100 - Loss: 0.0119, Val Loss: 0.5467, Val Accuracy: 0.8078\nEpoch 12/100 - Loss: 0.0120, Val Loss: 0.5934, Val Accuracy: 0.7433\nEpoch 13/100 - Loss: 0.0118, Val Loss: 0.5665, Val Accuracy: 0.8012\nEpoch 14/100 - Loss: 0.0113, Val Loss: 0.6123, Val Accuracy: 0.8027\nStopping early at epoch 15\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0137, Val Loss: 0.6281, Val Accuracy: 0.7791\nEpoch 2/100 - Loss: 0.0123, Val Loss: 0.6268, Val Accuracy: 0.7585\nEpoch 3/100 - Loss: 0.0118, Val Loss: 0.5643, Val Accuracy: 0.8005\nEpoch 4/100 - Loss: 0.0116, Val Loss: 0.5418, Val Accuracy: 0.7992\nEpoch 5/100 - Loss: 0.0118, Val Loss: 0.6143, Val Accuracy: 0.7992\nEpoch 6/100 - Loss: 0.0120, Val Loss: 0.5568, Val Accuracy: 0.8107\nEpoch 7/100 - Loss: 0.0115, Val Loss: 0.5466, Val Accuracy: 0.7926\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.5467079385384314, Validation Accuracy = 0.8078\nFold 1: Validation Loss = 0.5418768748164431, Validation Accuracy = 0.7991666666666667\nAverage Validation Loss across folds: 0.5442924066774373\nAverage Validation Accuracy across folds: 0.8034833333333333\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.4\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0156, Val Loss: 0.6666, Val Accuracy: 0.7227\nEpoch 2/100 - Loss: 0.0142, Val Loss: 0.6869, Val Accuracy: 0.7155\nEpoch 3/100 - Loss: 0.0144, Val Loss: 0.6780, Val Accuracy: 0.7094\nEpoch 4/100 - Loss: 0.0141, Val Loss: 0.6626, Val Accuracy: 0.7483\nEpoch 5/100 - Loss: 0.0141, Val Loss: 0.6199, Val Accuracy: 0.7611\nEpoch 6/100 - Loss: 0.0142, Val Loss: 0.6648, Val Accuracy: 0.7348\nEpoch 7/100 - Loss: 0.0141, Val Loss: 0.6324, Val Accuracy: 0.7224\nEpoch 8/100 - Loss: 0.0142, Val Loss: 0.6649, Val Accuracy: 0.7485\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0161, Val Loss: 0.6815, Val Accuracy: 0.7331\nEpoch 2/100 - Loss: 0.0144, Val Loss: 0.6863, Val Accuracy: 0.6918\nEpoch 3/100 - Loss: 0.0145, Val Loss: 0.7126, Val Accuracy: 0.7145\nEpoch 4/100 - Loss: 0.0146, Val Loss: 0.6817, Val Accuracy: 0.7148\nEpoch 5/100 - Loss: 0.0141, Val Loss: 0.6263, Val Accuracy: 0.7741\nEpoch 6/100 - Loss: 0.0152, Val Loss: 0.6770, Val Accuracy: 0.7164\nEpoch 7/100 - Loss: 0.0148, Val Loss: 0.6736, Val Accuracy: 0.7492\nEpoch 8/100 - Loss: 0.0142, Val Loss: 0.6564, Val Accuracy: 0.7215\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.6199763058853556, Validation Accuracy = 0.7610666666666667\nFold 1: Validation Loss = 0.6263655792675547, Validation Accuracy = 0.7741333333333333\nAverage Validation Loss across folds: 0.6231709425764551\nAverage Validation Accuracy across folds: 0.7676000000000001\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.5\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0189, Val Loss: 0.7519, Val Accuracy: 0.7015\nEpoch 2/100 - Loss: 0.0177, Val Loss: 0.7277, Val Accuracy: 0.6994\nEpoch 3/100 - Loss: 0.0176, Val Loss: 0.7176, Val Accuracy: 0.7089\nEpoch 4/100 - Loss: 0.0179, Val Loss: 0.7189, Val Accuracy: 0.7266\nEpoch 5/100 - Loss: 0.0180, Val Loss: 0.7946, Val Accuracy: 0.6856\nEpoch 6/100 - Loss: 0.0185, Val Loss: 0.7706, Val Accuracy: 0.7138\nEpoch 7/100 - Loss: 0.0183, Val Loss: 0.7171, Val Accuracy: 0.7007\nEpoch 8/100 - Loss: 0.0178, Val Loss: 0.7257, Val Accuracy: 0.7119\nEpoch 9/100 - Loss: 0.0186, Val Loss: 0.8717, Val Accuracy: 0.6662\nEpoch 10/100 - Loss: 0.0182, Val Loss: 0.7840, Val Accuracy: 0.6975\nStopping early at epoch 11\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0195, Val Loss: 0.7994, Val Accuracy: 0.7015\nEpoch 2/100 - Loss: 0.0175, Val Loss: 0.8095, Val Accuracy: 0.6960\nEpoch 3/100 - Loss: 0.0182, Val Loss: 0.8052, Val Accuracy: 0.6905\nEpoch 4/100 - Loss: 0.0190, Val Loss: 0.8562, Val Accuracy: 0.6962\nStopping early at epoch 5\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.7170110413514729, Validation Accuracy = 0.7007333333333333\nFold 1: Validation Loss = 0.7994299579276714, Validation Accuracy = 0.7015\nAverage Validation Loss across folds: 0.758220499639572\nAverage Validation Accuracy across folds: 0.7011166666666666\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.6000000000000001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0242, Val Loss: 1.2340, Val Accuracy: 0.4771\nEpoch 2/100 - Loss: 0.0236, Val Loss: 1.2048, Val Accuracy: 0.5119\nEpoch 3/100 - Loss: 0.0239, Val Loss: 1.2743, Val Accuracy: 0.4634\nEpoch 4/100 - Loss: 0.0246, Val Loss: 1.2974, Val Accuracy: 0.4934\nEpoch 5/100 - Loss: 0.0247, Val Loss: 1.3499, Val Accuracy: 0.4290\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0232, Val Loss: 0.9873, Val Accuracy: 0.5486\nEpoch 2/100 - Loss: 0.0221, Val Loss: 0.9696, Val Accuracy: 0.6246\nEpoch 3/100 - Loss: 0.0228, Val Loss: 0.9915, Val Accuracy: 0.6192\nEpoch 4/100 - Loss: 0.0229, Val Loss: 1.1766, Val Accuracy: 0.5793\nEpoch 5/100 - Loss: 0.0233, Val Loss: 1.1956, Val Accuracy: 0.5367\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.2048083548861017, Validation Accuracy = 0.5118666666666667\nFold 1: Validation Loss = 0.969661431653159, Validation Accuracy = 0.6246\nAverage Validation Loss across folds: 1.0872348932696303\nAverage Validation Accuracy across folds: 0.5682333333333334\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.7000000000000001\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0308, Val Loss: 1.5046, Val Accuracy: 0.4832\nEpoch 2/100 - Loss: 0.0305, Val Loss: 1.5145, Val Accuracy: 0.4152\nEpoch 3/100 - Loss: 0.0308, Val Loss: 1.5467, Val Accuracy: 0.3959\nEpoch 4/100 - Loss: 0.0318, Val Loss: 1.5415, Val Accuracy: 0.3685\nStopping early at epoch 5\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0314, Val Loss: 1.6463, Val Accuracy: 0.3461\nEpoch 2/100 - Loss: 0.0302, Val Loss: 1.7507, Val Accuracy: 0.2485\nEpoch 3/100 - Loss: 0.0309, Val Loss: 1.6355, Val Accuracy: 0.3016\nEpoch 4/100 - Loss: 0.0321, Val Loss: 1.8940, Val Accuracy: 0.2018\nEpoch 5/100 - Loss: 0.0317, Val Loss: 1.7913, Val Accuracy: 0.2018\nEpoch 6/100 - Loss: 0.0310, Val Loss: 1.8040, Val Accuracy: 0.2263\nStopping early at epoch 7\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 1.5047882640285533, Validation Accuracy = 0.4831666666666667\nFold 1: Validation Loss = 1.6354421125546192, Validation Accuracy = 0.30156666666666665\nAverage Validation Loss across folds: 1.5701151882915863\nAverage Validation Accuracy across folds: 0.39236666666666664\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.8\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0357, Val Loss: 2.1137, Val Accuracy: 0.1825\nEpoch 2/100 - Loss: 0.0355, Val Loss: 2.0820, Val Accuracy: 0.1930\nEpoch 3/100 - Loss: 0.0357, Val Loss: 2.3042, Val Accuracy: 0.0984\nEpoch 4/100 - Loss: 0.0360, Val Loss: 2.3039, Val Accuracy: 0.1003\nEpoch 5/100 - Loss: 0.0360, Val Loss: 2.3035, Val Accuracy: 0.0987\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0355, Val Loss: 2.1630, Val Accuracy: 0.1757\nEpoch 2/100 - Loss: 0.0356, Val Loss: 2.1392, Val Accuracy: 0.1745\nEpoch 3/100 - Loss: 0.0356, Val Loss: 2.2030, Val Accuracy: 0.1490\nEpoch 4/100 - Loss: 0.0358, Val Loss: 2.3031, Val Accuracy: 0.0981\nEpoch 5/100 - Loss: 0.0360, Val Loss: 2.3036, Val Accuracy: 0.0981\nStopping early at epoch 6\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.0820757237070406, Validation Accuracy = 0.193\nFold 1: Validation Loss = 2.139180318633122, Validation Accuracy = 0.1745\nAverage Validation Loss across folds: 2.1106280211700814\nAverage Validation Accuracy across folds: 0.18375\n--------------------------------\nK-fold Cross-validation completed.\nTraining with Dropout Rate: 0.9\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0362, Val Loss: 2.3032, Val Accuracy: 0.0998\nEpoch 2/100 - Loss: 0.0360, Val Loss: 2.3036, Val Accuracy: 0.0985\nEpoch 3/100 - Loss: 0.0360, Val Loss: 2.3030, Val Accuracy: 0.0993\nEpoch 4/100 - Loss: 0.0360, Val Loss: 2.3029, Val Accuracy: 0.1011\nEpoch 5/100 - Loss: 0.0360, Val Loss: 2.3035, Val Accuracy: 0.0985\nEpoch 6/100 - Loss: 0.0360, Val Loss: 2.3030, Val Accuracy: 0.0998\nEpoch 7/100 - Loss: 0.0360, Val Loss: 2.3044, Val Accuracy: 0.1011\nStopping early at epoch 8\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0363, Val Loss: 2.3038, Val Accuracy: 0.0989\nEpoch 2/100 - Loss: 0.0360, Val Loss: 2.3039, Val Accuracy: 0.0993\nEpoch 3/100 - Loss: 0.0360, Val Loss: 2.3037, Val Accuracy: 0.0999\nEpoch 4/100 - Loss: 0.0360, Val Loss: 2.3034, Val Accuracy: 0.0999\nEpoch 5/100 - Loss: 0.0360, Val Loss: 2.3031, Val Accuracy: 0.0993\nEpoch 6/100 - Loss: 0.0360, Val Loss: 2.3039, Val Accuracy: 0.0989\nEpoch 7/100 - Loss: 0.0360, Val Loss: 2.3035, Val Accuracy: 0.0989\nEpoch 8/100 - Loss: 0.0360, Val Loss: 2.3042, Val Accuracy: 0.0993\nStopping early at epoch 9\n--------------------------------\nFinished FOLD 1\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 2.3028677414729395, Validation Accuracy = 0.10113333333333334\nFold 1: Validation Loss = 2.3030834497927604, Validation Accuracy = 0.09933333333333333\nAverage Validation Loss across folds: 2.3029755956328497\nAverage Validation Accuracy across folds: 0.10023333333333334\n--------------------------------\nK-fold Cross-validation completed.\n   dropout_rate  train_loss  val_loss  val_accuracy\n0           0.0    0.404410  0.859967      0.006408\n1           0.1    0.434562  0.846133      0.008214\n2           0.2    0.476113  0.829450      0.009961\n3           0.3    0.544292  0.803483      0.012022\n4           0.4    0.623171  0.767600      0.014555\n5           0.5    0.758220  0.701117      0.018314\n6           0.6    1.087235  0.568233      0.023683\n7           0.7    1.570115  0.392367      0.031110\n8           0.8    2.110628  0.183750      0.035786\n9           0.9    2.302976  0.100233      0.036044\n\n\n\n# rename columns as they were ordered wrong (fixing)\n\nresults_choice_3.rename(columns={'train_loss': 'average_val_loss'}, inplace=True)\nresults_choice_3.rename(columns={'val_loss': 'average_val_accuracy'}, inplace=True)\nresults_choice_3.rename(columns={'val_accuracy': 'average_train_loss'}, inplace=True)\n\n# save results_df3 to csv\nresults_choice_3.to_csv('results/results_choice_3.csv')\n\n\n# display results\nresults_choice_3\n\n\n\n\n\n\n\n\ndropout_rate\naverage_val_loss\naverage_val_accuracy\naverage_train_loss\n\n\n\n\n0\n0.0\n0.404410\n0.859967\n0.006408\n\n\n1\n0.1\n0.434562\n0.846133\n0.008214\n\n\n2\n0.2\n0.476113\n0.829450\n0.009961\n\n\n3\n0.3\n0.544292\n0.803483\n0.012022\n\n\n4\n0.4\n0.623171\n0.767600\n0.014555\n\n\n5\n0.5\n0.758220\n0.701117\n0.018314\n\n\n6\n0.6\n1.087235\n0.568233\n0.023683\n\n\n7\n0.7\n1.570115\n0.392367\n0.031110\n\n\n8\n0.8\n2.110628\n0.183750\n0.035786\n\n\n9\n0.9\n2.302976\n0.100233\n0.036044\n\n\n\n\n\n\n\n\nresults_choice_3 = pd.read_csv('results/results_choice_3.csv')\n\n# Plot\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(results_choice_3['dropout_rate'], results_choice_3['average_train_loss'], label='Training Loss', marker='o', color='green')\nplt.plot(results_choice_3['dropout_rate'], results_choice_3['average_val_loss'], label='Validation Loss', marker='o', color='red')\nplt.title('Training and Validation Loss vs. Dropout Rate')\nplt.xlabel('Dropout Rate')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot\nplt.subplot(1, 2, 2)\nerror_ratio = results_choice_3['average_val_loss'] / results_choice_3['average_train_loss']\nplt.plot(results_choice_3['dropout_rate'], error_ratio, label='Val/Train Loss Ratio', marker='o')\nplt.title('Error Ratio vs. Dropout Rate')\nplt.xlabel('Dropout Rate')\nplt.ylabel('Ratio')\nplt.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ANN.html#final-fit",
    "href": "ANN.html#final-fit",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "At the very end, select a best fit model and report, training, validation, and test errors at the very end\nMake sure your “plotting variable=True” when for the final training\n\nMy best model was the one with the following parameters: * hidden activation=relu, hidden layers = [96,96,96], optimizer=adam, learning rate=0.001, L1=0, L2=0, dropout=0, weight initialization=random\n\n# Best model\ndef model_fn_final():\n    return NeuralNetwork(input_size=28*28, layer_sizes=[96, 96, 96], activation_funcs=['relu', 'relu', 'relu'], dropout_rate=0)\n\nfinal_results = []\n\navg_val_loss, avg_val_accuracy, avg_train_loss = training_model_kfold(k_folds=4, dataset=training_data, model_fn=model_fn_final, loss_fn=nn.CrossEntropyLoss(), optimizer_name=\"Adam\", lr=0.001, l1_reg=0, l2_reg=0, num_epochs=100, device='cpu', plot=True, patience=4)\n\nfinal_results.append({\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n        'val_accuracy': avg_val_accuracy\n    })\n\nfinal_results = pd.DataFrame(final_results)\nprint(final_results)\n\nFOLD 0\n--------------------------------\nEpoch 1/100 - Loss: 0.0098, Val Loss: 0.4877, Val Accuracy: 0.8215\nEpoch 2/100 - Loss: 0.0065, Val Loss: 0.4001, Val Accuracy: 0.8525\nEpoch 3/100 - Loss: 0.0058, Val Loss: 0.3777, Val Accuracy: 0.8595\nEpoch 4/100 - Loss: 0.0055, Val Loss: 0.3714, Val Accuracy: 0.8619\nEpoch 5/100 - Loss: 0.0051, Val Loss: 0.3436, Val Accuracy: 0.8764\nEpoch 6/100 - Loss: 0.0049, Val Loss: 0.3358, Val Accuracy: 0.8739\nEpoch 7/100 - Loss: 0.0046, Val Loss: 0.3054, Val Accuracy: 0.8892\nEpoch 8/100 - Loss: 0.0044, Val Loss: 0.3431, Val Accuracy: 0.8756\nEpoch 9/100 - Loss: 0.0042, Val Loss: 0.3166, Val Accuracy: 0.8860\nEpoch 10/100 - Loss: 0.0041, Val Loss: 0.3118, Val Accuracy: 0.8873\nStopping early at epoch 11\n\n\n\n\n\n\n\n\n\n--------------------------------\nFinished FOLD 0\n--------------------------------\nFOLD 1\n--------------------------------\nEpoch 1/100 - Loss: 0.0100, Val Loss: 0.4680, Val Accuracy: 0.8311\nEpoch 2/100 - Loss: 0.0065, Val Loss: 0.4125, Val Accuracy: 0.8465\nEpoch 3/100 - Loss: 0.0059, Val Loss: 0.3794, Val Accuracy: 0.8656\nEpoch 4/100 - Loss: 0.0054, Val Loss: 0.3961, Val Accuracy: 0.8559\nEpoch 5/100 - Loss: 0.0051, Val Loss: 0.3500, Val Accuracy: 0.8723\nEpoch 6/100 - Loss: 0.0048, Val Loss: 0.3646, Val Accuracy: 0.8695\nEpoch 7/100 - Loss: 0.0046, Val Loss: 0.3671, Val Accuracy: 0.8704\nEpoch 8/100 - Loss: 0.0044, Val Loss: 0.3426, Val Accuracy: 0.8803\nEpoch 9/100 - Loss: 0.0043, Val Loss: 0.3402, Val Accuracy: 0.8771\nEpoch 10/100 - Loss: 0.0041, Val Loss: 0.3702, Val Accuracy: 0.8669\nEpoch 11/100 - Loss: 0.0039, Val Loss: 0.3457, Val Accuracy: 0.8785\nEpoch 12/100 - Loss: 0.0038, Val Loss: 0.3315, Val Accuracy: 0.8853\nEpoch 13/100 - Loss: 0.0037, Val Loss: 0.3715, Val Accuracy: 0.8730\nEpoch 14/100 - Loss: 0.0035, Val Loss: 0.3392, Val Accuracy: 0.8790\nEpoch 15/100 - Loss: 0.0034, Val Loss: 0.3400, Val Accuracy: 0.8804\nStopping early at epoch 16\n\n\n\n\n\n\n\n\n\n--------------------------------\nFinished FOLD 1\n--------------------------------\nFOLD 2\n--------------------------------\nEpoch 1/100 - Loss: 0.0099, Val Loss: 0.4422, Val Accuracy: 0.8445\nEpoch 2/100 - Loss: 0.0065, Val Loss: 0.3862, Val Accuracy: 0.8597\nEpoch 3/100 - Loss: 0.0058, Val Loss: 0.3709, Val Accuracy: 0.8644\nEpoch 4/100 - Loss: 0.0054, Val Loss: 0.3443, Val Accuracy: 0.8763\nEpoch 5/100 - Loss: 0.0050, Val Loss: 0.3434, Val Accuracy: 0.8763\nEpoch 6/100 - Loss: 0.0049, Val Loss: 0.3322, Val Accuracy: 0.8795\nEpoch 7/100 - Loss: 0.0046, Val Loss: 0.3170, Val Accuracy: 0.8875\nEpoch 8/100 - Loss: 0.0044, Val Loss: 0.3164, Val Accuracy: 0.8856\nEpoch 9/100 - Loss: 0.0043, Val Loss: 0.3170, Val Accuracy: 0.8868\nEpoch 10/100 - Loss: 0.0041, Val Loss: 0.3228, Val Accuracy: 0.8808\nEpoch 11/100 - Loss: 0.0040, Val Loss: 0.3112, Val Accuracy: 0.8851\nEpoch 12/100 - Loss: 0.0039, Val Loss: 0.3166, Val Accuracy: 0.8866\nEpoch 13/100 - Loss: 0.0037, Val Loss: 0.3131, Val Accuracy: 0.8903\nEpoch 14/100 - Loss: 0.0037, Val Loss: 0.3038, Val Accuracy: 0.8893\nEpoch 15/100 - Loss: 0.0035, Val Loss: 0.3038, Val Accuracy: 0.8941\nEpoch 16/100 - Loss: 0.0034, Val Loss: 0.3144, Val Accuracy: 0.8884\nEpoch 17/100 - Loss: 0.0034, Val Loss: 0.3054, Val Accuracy: 0.8929\nStopping early at epoch 18\n\n\n\n\n\n\n\n\n\n--------------------------------\nFinished FOLD 2\n--------------------------------\nFOLD 3\n--------------------------------\nEpoch 1/100 - Loss: 0.0098, Val Loss: 0.4526, Val Accuracy: 0.8384\nEpoch 2/100 - Loss: 0.0065, Val Loss: 0.4153, Val Accuracy: 0.8541\nEpoch 3/100 - Loss: 0.0058, Val Loss: 0.3749, Val Accuracy: 0.8682\nEpoch 4/100 - Loss: 0.0054, Val Loss: 0.3635, Val Accuracy: 0.8652\nEpoch 5/100 - Loss: 0.0051, Val Loss: 0.3493, Val Accuracy: 0.8727\nEpoch 6/100 - Loss: 0.0048, Val Loss: 0.3533, Val Accuracy: 0.8706\nEpoch 7/100 - Loss: 0.0046, Val Loss: 0.3431, Val Accuracy: 0.8752\nEpoch 8/100 - Loss: 0.0044, Val Loss: 0.3582, Val Accuracy: 0.8745\nEpoch 9/100 - Loss: 0.0043, Val Loss: 0.3276, Val Accuracy: 0.8806\nEpoch 10/100 - Loss: 0.0041, Val Loss: 0.3176, Val Accuracy: 0.8866\nEpoch 11/100 - Loss: 0.0039, Val Loss: 0.3192, Val Accuracy: 0.8842\nEpoch 12/100 - Loss: 0.0038, Val Loss: 0.3209, Val Accuracy: 0.8873\nEpoch 13/100 - Loss: 0.0037, Val Loss: 0.3349, Val Accuracy: 0.8833\nEpoch 14/100 - Loss: 0.0035, Val Loss: 0.3159, Val Accuracy: 0.8897\nEpoch 15/100 - Loss: 0.0035, Val Loss: 0.3241, Val Accuracy: 0.8868\nEpoch 16/100 - Loss: 0.0034, Val Loss: 0.3303, Val Accuracy: 0.8880\nEpoch 17/100 - Loss: 0.0033, Val Loss: 0.3327, Val Accuracy: 0.8857\nStopping early at epoch 18\n\n\n\n\n\n\n\n\n\n--------------------------------\nFinished FOLD 3\n--------------------------------\nValidation Loss and Accuracy per Fold:\nFold 0: Validation Loss = 0.30621458574178373, Validation Accuracy = 0.8892\nFold 1: Validation Loss = 0.33166823387145994, Validation Accuracy = 0.8853333333333333\nFold 2: Validation Loss = 0.3046102743833623, Validation Accuracy = 0.8893333333333333\nFold 3: Validation Loss = 0.31593505687853124, Validation Accuracy = 0.8896666666666667\nAverage Validation Loss across folds: 0.3146070377187843\nAverage Validation Accuracy across folds: 0.8883833333333333\n--------------------------------\nK-fold Cross-validation completed.\n   train_loss  val_loss  val_accuracy\n0    0.004801  0.314607      0.888383\n\n\n\n# save final_results_df to csv\nfinal_results.to_csv('results/final_results.csv')"
  },
  {
    "objectID": "ANN.html#acruacy-of-my-best-model",
    "href": "ANN.html#acruacy-of-my-best-model",
    "title": "ANN training in PyTorch and hyperparameter tuning",
    "section": "",
    "text": "# Model evaluation\nmodel = NeuralNetwork(input_size=28*28, layer_sizes=[96, 96, 96], activation_funcs=['relu', 'relu', 'relu'], dropout_rate=0)\nmodel.load_state_dict(torch.load(\"best_model_fold_2.pth\"))\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\ntest(test_dataloader, model, loss_fn, device)\nprint(\"Done!\")\n\nTest Error: \n Accuracy: 88.2%, Avg loss: 0.337591 \n\nDone!"
  },
  {
    "objectID": "trees.html",
    "href": "trees.html",
    "title": "Regression trees and Random Forests",
    "section": "",
    "text": "The goal of this tab is to build a model to predict the average miles per galon (counting gasoline and diesel). We wil use the following variables as predictors: city, agency, organization type, mode (A system for carrying transit passengers described by specific right-of-way (ROW), technology and operational features), TOS (Describes how public transportation services are provided by the transit agency: directly operated (DO) or purchased transportation (PT) services), and Electric Battery (referring to the use of electric batteries to power the vehicle).\nThat way, we can not only predict the average miles per galon, but also understand which variables are more important to predict the average miles per galon. This could give us insights on what to do to improve the average miles per galon, which factors are better, and how much fossil fuels we will use in the future with different combinations of all these variables.\n\n\nDecision trees are machine learning models that try to predict the value of a target variable by learning simple decision rules inferred from the data features. They can be used for either classification or regression models. While the can handle both categorical and numerical data at once for either type of model (classification or regression), in implementation, some of the oposite variables will need to be transformed with one-hot encoding in order to run the algorithm (tipically for handling categorical variables). Each internal node in the tree represents a feature or attribute, each branch a decision rule, and each leaf a prediction. The prediction then is always done from the top of the tree to the bottom by following each decision rule.\nDecision trees are great for interpreteability and visualization. Their hierarchical structure allow us to visualize the importance of every variable in the decision making process of the tree. However, their tendency to overfit the data makes them less useful for prediction. They tend to capture the noise of the data into their model. To solve this problem, we can use methods like purning, which takes awa the “non-contributing” parts of the tree or hyperparameter tunning, which changes the parameters of the model to make it more generalizable.\nApplying decision trees requires to have a target variable (the one that we want to predict) and a set of features (the ones that we will use to predict the target variable). Classification trees focusses on predicting labels while regression trees on numerical values. Depending on your objective and your data, you can use oine or the other. For example, in this case, we want to predict the average miles per galon, which is a continuous variable, so we will use a regression tree. Nevertheless, if we wanted to take a different approach, we could always convert that variable into a categorical one (dividing the data into low mpg, medium mpg, and high mpg), and then use a classification tree instead.\n\n\n\nRandom forests are another machine learning algorithm that takes advantage of decision trees. They combine multiple number of decision trees to make a prediction. Each tree operates individually. In order to generated, random forests use the bootstrapping method to randomly select a subset of the data and also randomly select a subset of the features. This way, each tree is different from the others. this variability tries to reduce the overfitting of the model. However, they also tend to overfit, the same way as a decision tree. To solve this, we can utilize methods like hyperparameter tunning to make the model more generalizable. It is also worth notiong that, since they are built with trees, they can also handle both categorical and numerical data at once for either type of model (classification or regression) but, in implementation, the one-hot encoding needs to be utilized the same way as for a decision tree.\nFor classification, the final prediction is the one that has the majority of votes from the trees (each tree having one vote). For regression, the final prediction is the overall average of the predictions from each tree. This approach aims for a more accurate than a single decision tree (but this is not always the case). Furthermore, the differences of the features on each tree allows random forests to better handle noise and outliers.\nRandom forests can be used in many different situations. Some of the most famous ones are healthcare, finance, or image recognition. Their flexibility and high performance make them a great choice for many different problems. Furthermore, since they are built with trees, they are also great for interpreteability and visualization.\n\n\n\nFor both, decision trees and random forests, hyperparameter tunning is a great way to improve the performance of the model. Thus, we will use it for both. Hyperparameters are the con figuration of settings in order to build a model. in decision trees, we will focus on how the depth affect the model accruacy and chose the optimal one to train the model. In random forests, we will focus on the number of trees and the depth of each tree. We will use the same approach as for decision trees to find and then train our optimal model.\n\n\n\nThis model will randomly select values and apply them to our predictions. We will use this model as our baseline to compare the performance of our other models."
  },
  {
    "objectID": "trees.html#explanation-of-the-modeling-methods",
    "href": "trees.html#explanation-of-the-modeling-methods",
    "title": "Regression trees and Random Forests",
    "section": "",
    "text": "The goal of this tab is to build a model to predict the average miles per galon (counting gasoline and diesel). We wil use the following variables as predictors: city, agency, organization type, mode (A system for carrying transit passengers described by specific right-of-way (ROW), technology and operational features), TOS (Describes how public transportation services are provided by the transit agency: directly operated (DO) or purchased transportation (PT) services), and Electric Battery (referring to the use of electric batteries to power the vehicle).\nThat way, we can not only predict the average miles per galon, but also understand which variables are more important to predict the average miles per galon. This could give us insights on what to do to improve the average miles per galon, which factors are better, and how much fossil fuels we will use in the future with different combinations of all these variables.\n\n\nDecision trees are machine learning models that try to predict the value of a target variable by learning simple decision rules inferred from the data features. They can be used for either classification or regression models. While the can handle both categorical and numerical data at once for either type of model (classification or regression), in implementation, some of the oposite variables will need to be transformed with one-hot encoding in order to run the algorithm (tipically for handling categorical variables). Each internal node in the tree represents a feature or attribute, each branch a decision rule, and each leaf a prediction. The prediction then is always done from the top of the tree to the bottom by following each decision rule.\nDecision trees are great for interpreteability and visualization. Their hierarchical structure allow us to visualize the importance of every variable in the decision making process of the tree. However, their tendency to overfit the data makes them less useful for prediction. They tend to capture the noise of the data into their model. To solve this problem, we can use methods like purning, which takes awa the “non-contributing” parts of the tree or hyperparameter tunning, which changes the parameters of the model to make it more generalizable.\nApplying decision trees requires to have a target variable (the one that we want to predict) and a set of features (the ones that we will use to predict the target variable). Classification trees focusses on predicting labels while regression trees on numerical values. Depending on your objective and your data, you can use oine or the other. For example, in this case, we want to predict the average miles per galon, which is a continuous variable, so we will use a regression tree. Nevertheless, if we wanted to take a different approach, we could always convert that variable into a categorical one (dividing the data into low mpg, medium mpg, and high mpg), and then use a classification tree instead.\n\n\n\nRandom forests are another machine learning algorithm that takes advantage of decision trees. They combine multiple number of decision trees to make a prediction. Each tree operates individually. In order to generated, random forests use the bootstrapping method to randomly select a subset of the data and also randomly select a subset of the features. This way, each tree is different from the others. this variability tries to reduce the overfitting of the model. However, they also tend to overfit, the same way as a decision tree. To solve this, we can utilize methods like hyperparameter tunning to make the model more generalizable. It is also worth notiong that, since they are built with trees, they can also handle both categorical and numerical data at once for either type of model (classification or regression) but, in implementation, the one-hot encoding needs to be utilized the same way as for a decision tree.\nFor classification, the final prediction is the one that has the majority of votes from the trees (each tree having one vote). For regression, the final prediction is the overall average of the predictions from each tree. This approach aims for a more accurate than a single decision tree (but this is not always the case). Furthermore, the differences of the features on each tree allows random forests to better handle noise and outliers.\nRandom forests can be used in many different situations. Some of the most famous ones are healthcare, finance, or image recognition. Their flexibility and high performance make them a great choice for many different problems. Furthermore, since they are built with trees, they are also great for interpreteability and visualization.\n\n\n\nFor both, decision trees and random forests, hyperparameter tunning is a great way to improve the performance of the model. Thus, we will use it for both. Hyperparameters are the con figuration of settings in order to build a model. in decision trees, we will focus on how the depth affect the model accruacy and chose the optimal one to train the model. In random forests, we will focus on the number of trees and the depth of each tree. We will use the same approach as for decision trees to find and then train our optimal model.\n\n\n\nThis model will randomly select values and apply them to our predictions. We will use this model as our baseline to compare the performance of our other models."
  },
  {
    "objectID": "trees.html#data-and-libraries",
    "href": "trees.html#data-and-libraries",
    "title": "Regression trees and Random Forests",
    "section": "Data and libraries",
    "text": "Data and libraries\nThis is how the data used for creating these models looks like:\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\ndf = pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/2021_Fuel_and_Energy_mpg_fossil_fuels.csv')\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1056 entries, 0 to 1055\nData columns (total 8 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   City                    1053 non-null   object \n 1   Agency                  1056 non-null   object \n 2   Organization Type       1056 non-null   object \n 3   Primary UZA Population  1056 non-null   int64  \n 4   Mode                    1056 non-null   object \n 5   TOS                     1056 non-null   object \n 6   Electric Battery        1056 non-null   int64  \n 7   avg_mpg_fossil_fuels    1056 non-null   float64\ndtypes: float64(1), int64(2), object(5)\nmemory usage: 66.1+ KB\n\n\nIn order to view the shape of the data we want to predict, it will be useful to visualize its shape. This will be more of an EDA step, however, it is better to have it next to our model to understand it better. A violin plot allow us to visualize the distribution of the data and its probability density, which could be useful to understand better the data.\n\n\nCode\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(8, 6))\nsns.violinplot(x=df['avg_mpg_fossil_fuels'], color='skyblue')\n\nplt.xlabel('Average MPG of Fossil Fuels')\nplt.title('Violin Plot of Average MPG of Fossil Fuels')\n\nplt.show()\n\n\n/Users/jorgebrismoreno/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\n\n\n\n\nAs we can see from this graph, it seems that there is a high density of data close to 0. This is beneficial to visualize and better understand the nature of the data. Moreover, we can see that there is a wide range of possible values, which could mean that either we have a lot of heavy outliers or very few data samples on those values."
  },
  {
    "objectID": "trees.html#base-model-random-classifier",
    "href": "trees.html#base-model-random-classifier",
    "title": "Regression trees and Random Forests",
    "section": "Base model / random classifier",
    "text": "Base model / random classifier\nA random classiffier simply makes random predictions of the data. This is the most basic model we can use to predict our mpg data. We will use this model as a baseline to compare the other models.\n\n\nCode\ndef random_regressor_avg_mpg_fossil_fuels(y_data):\n    y_pred = np.random.uniform(y_data.min(), y_data.max(), size=len(y_data))\n    \n    print(\"---------RANDOM REGRESSOR---------\")\n\n    mse = mean_squared_error(y_data, y_pred)\n    print(\"Mean Squared Error:\", mse)\n    \n    rmse = np.sqrt(mse)\n    print(\"Root Mean Squared Error:\", rmse)\n\ny_avg_mpg_fossil_fuels = df['avg_mpg_fossil_fuels']\n\nrandom_regressor_avg_mpg_fossil_fuels(y_avg_mpg_fossil_fuels)\n\n\n---------RANDOM REGRESSOR---------\nMean Squared Error: 1066824.3696089713\nRoot Mean Squared Error: 1032.8719037755704\n\n\nAs we can see, it has a very low accruacy, which is expected since it is a random classifier. We will keep this values in mind to compare them with the other models."
  },
  {
    "objectID": "trees.html#decision-tree-1",
    "href": "trees.html#decision-tree-1",
    "title": "Regression trees and Random Forests",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nBase Decision Tree\nThis is the base regression tree model for our data set and its results:\n\n\nCode\ny = df['avg_mpg_fossil_fuels']\nX = df.drop(columns=['avg_mpg_fossil_fuels'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\n\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean'))\n])\n\ncategorical_features = X.select_dtypes(include=['object']).columns\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nX_train_transformed = preprocessor.fit_transform(X_train)\n\nfeature_names = list(numeric_features) + list(\n    preprocessor.named_transformers_['cat']\n    .named_steps['onehot'].get_feature_names_out(categorical_features)\n)\n\nregressor = DecisionTreeRegressor(random_state=44)\nregressor.fit(X_train_transformed, y_train)\n\nX_test_transformed = preprocessor.transform(X_test)\ny_pred = regressor.predict(X_test_transformed)\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\nprint(\"Mean Squared Error:\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\n\n\nMean Squared Error: 3464.875119590503\nRoot Mean Squared Error (RMSE): 58.863189852321995\n\n\n\n\nCode\nplt.figure(figsize=(20, 10))\nplot_tree(regressor, feature_names=feature_names, filled=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see, even though it does a much better job than the random classifier, it is still not very accurate. Furthermore, the depth of the tree is impressingly large, which could be a problem for the model. Thus, we are going to tune this model on the next step.\n\n\nHyperparameter tunning on decision tree\nIn this step, we are going to tune the hyperparameters of the regression tree model. We will vary the depth of the tree to find the optimal one to use.\n\n\nCode\ntest_results = []\ntrain_results = []\n\nfor max_depth in range(1, 34):\n    model = DecisionTreeRegressor(max_depth=max_depth, random_state=44)\n    model.fit(X_train_transformed, y_train)\n\n    y_pred_train = model.predict(X_train_transformed)\n    y_pred_test = model.predict(X_test_transformed)\n\n    train_mse = mean_squared_error(y_train, y_pred_train)\n    test_mse = mean_squared_error(y_test, y_pred_test)\n    \n    train_rmse = np.sqrt(train_mse)\n    test_rmse = np.sqrt(test_mse)\n\n    train_results.append([max_depth, train_mse, train_rmse])\n    test_results.append([max_depth, test_mse, test_rmse])\n\nmax_depth_values = [result[0] for result in test_results]\ntrain_mse_values = [result[1] for result in train_results]\ntest_mse_values = [result[1] for result in test_results]\ntrain_rmse_values = [result[2] for result in train_results]\ntest_rmse_values = [result[2] for result in test_results]\n\nplt.figure(figsize=(10, 6))\nplt.plot(max_depth_values, train_mse_values, label='Train MSE', marker='o', color='blue')\nplt.plot(max_depth_values, test_mse_values, label='Test MSE', marker='o', color='red')\nplt.xlabel('Max Depth of Decision Tree')\nplt.ylabel('Mean Squared Error')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.plot(max_depth_values, train_rmse_values, label='Train RMSE', marker='o', color='blue')\nplt.plot(max_depth_values, test_rmse_values, label='Test RMSE', marker='o', color='red')\nplt.xlabel('Max Depth of Decision Tree')\nplt.ylabel('Root Mean Squared Error')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom these graphs, we can see that the best depth for the tree is 5, since it has the lowest error for both, the training and the test data. Thus, we will use this depth for the next model.\n\n\nOptimal tree\nBased on the previous results, we will use a tree with a depth of 5 to predict the mpg data. The tree has the following structure and results:\n\n\nCode\ny = df['avg_mpg_fossil_fuels']\nX = df.drop(columns=['avg_mpg_fossil_fuels'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\n\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns\ncategorical_features = X.select_dtypes(include=['object']).columns\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean'))\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nmodel = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', DecisionTreeRegressor(max_depth=5, random_state=44))\n])\n\nmodel.fit(X_train, y_train)\n\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\ntrain_mse = mean_squared_error(y_train, y_pred_train)\ntest_mse = mean_squared_error(y_test, y_pred_test)\n\ntrain_rmse = np.sqrt(train_mse)\ntest_rmse = np.sqrt(test_mse)\n\nprint(f\"Train Mean Squared Error: {train_mse}\")\nprint(f\"Test Mean Squared Error: {test_mse}\")\nprint(f\"Train Root Mean Squared Error: {train_rmse}\")\nprint(f\"Test Root Mean Squared Error: {test_rmse}\")\n\n\nTrain Mean Squared Error: 2065.96904233018\nTest Mean Squared Error: 941.6117919314114\nTrain Root Mean Squared Error: 45.45293216427495\nTest Root Mean Squared Error: 30.685693603557528\n\n\n\n\nCode\nmodel.fit(X_train, y_train)\n\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\ncategorical_features_names = model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\nfeature_names = list(numeric_features) + list(categorical_features_names)\n\nplt.figure(figsize=(20, 10))\nplot_tree(model.named_steps['regressor'], feature_names=feature_names, filled=True, rounded=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that the MSE is much lower than the previous tree model (cut down by almost half), which means that this model is much more accurate. Furthermore, the depth of the tree is much lower than the previous one, which is good since it means that the model is simpler. Furthermore, we can see that Agency is the most important variable to predict the mpg data, followed by City, Mode, and TOS. This will allow us to make inferences about this variables in the future."
  },
  {
    "objectID": "trees.html#random-forest-1",
    "href": "trees.html#random-forest-1",
    "title": "Regression trees and Random Forests",
    "section": "Random forest",
    "text": "Random forest\n\nBase Random Forest\nThis is the base random forest model for our data set, its results, and the first three trees of the forest:\n\n\nCode\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns\ncategorical_features = X.select_dtypes(include=['object']).columns\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean'))\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nrf_regressor = RandomForestRegressor(n_estimators=100, random_state=44)\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', rf_regressor)\n])\n\npipeline.fit(X_train, y_train)\n\ny_pred_train_rf = pipeline.predict(X_train)\ny_pred_test_rf = pipeline.predict(X_test)\n\ntrain_mse_rf = mean_squared_error(y_train, y_pred_train_rf)\ntest_mse_rf = mean_squared_error(y_test, y_pred_test_rf)\ntrain_rmse_rf = np.sqrt(train_mse_rf)\ntest_rmse_rf = np.sqrt(test_mse_rf)\n\nprint(\"Random Forest Regressor Results:\")\nprint(\"Train Mean Squared Error:\", train_mse_rf)\nprint(\"Test Mean Squared Error:\", test_mse_rf)\nprint(\"Train Root Mean Squared Error:\", train_rmse_rf)\nprint(\"Test Root Mean Squared Error:\", test_rmse_rf)\n\n\nRandom Forest Regressor Results:\nTrain Mean Squared Error: 2544.263281758251\nTest Mean Squared Error: 4549.356195127014\nTrain Root Mean Squared Error: 50.4406907343491\nTest Root Mean Squared Error: 67.44891544811536\n\n\n\n\nCode\nrf_regressor = pipeline.named_steps['regressor']\n\ncategorical_features = X.select_dtypes(include=['object']).columns\nfeature_names = list(numeric_features) + list(\n    pipeline.named_steps['preprocessor'].named_transformers_['cat']\n    .named_steps['onehot'].get_feature_names_out(categorical_features)\n)\n\nfor i in range(3):\n    plt.figure(figsize=(20, 10))\n    plot_tree(rf_regressor.estimators_[i], feature_names=feature_names, filled=True, rounded=True)\n    plt.title(f\"Random Forest Tree {i+1}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, the accruacy is seems to be much worse for our test data compared to the optimal decision tree (as the MSE is much higher). However, the trees look very large and seems that this model can be improved to perform even better while being more interpretable. Thus, we are going to tune this model on the next step.\n\n\nHyperparameter tunning on random forest\nIn this step, we are going to tune the hyperparameters of the random forest model. We will vary the depth of the trees and the number of estimators to find the optimal one to use.\n\n\nCode\nparam_dist = {\n    'regressor__n_estimators': randint(44, 444),\n    'regressor__max_depth': randint(1, 24),\n}\n\nrand_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=20, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\nrand_search.fit(X_train, y_train)\n\nbest_rf_model = rand_search.best_estimator_\n\ny_pred_train_best = best_rf_model.predict(X_train)\ny_pred_test_best = best_rf_model.predict(X_test)\n\nmse_train_best = mean_squared_error(y_train, y_pred_train_best)\nmse_test_best = mean_squared_error(y_test, y_pred_test_best)\nrmse_train_best = np.sqrt(mse_train_best)\nrmse_test_best = np.sqrt(mse_test_best)\n\nprint(\"Best Model Metrics:\")\nprint(f\"Train Mean Squared Error: {mse_train_best}\")\nprint(f\"Test Mean Squared Error: {mse_test_best}\")\nprint(f\"Train Root Mean Squared Error: {rmse_train_best}\")\nprint(f\"Test Root Mean Squared Error: {rmse_test_best}\")\n\nprint(\"Best Hyperparameters:\")\nprint(rand_search.best_params_)\n\n\nBest Model Metrics:\nTrain Mean Squared Error: 6362.505730760978\nTest Mean Squared Error: 1823.3688800609457\nTrain Root Mean Squared Error: 79.76531659036387\nTest Root Mean Squared Error: 42.700923644119754\nBest Hyperparameters:\n{'regressor__max_depth': 2, 'regressor__n_estimators': 326}\n\n\nFrom this results, we can inferr that the best depth of the trees are 2 and the regressor estimators is 326. Thus, we will use these parameters for the next model.\n\n\nOptimal random forest\nBased on the previous results, we will use a random forests with trees of a depth of 3 and 140 estimators to predict the mpg data. The forest has the following results and the first three trees of the forest look like the following:\n\n\nCode\nbest_hyperparameters = {'max_depth': 2, 'n_estimators': 326}\n\npipeline.named_steps['regressor'].set_params(**best_hyperparameters)\n\npipeline.fit(X_train, y_train)\n\ny_pred_train_optimal = pipeline.predict(X_train)\ny_pred_test_optimal = pipeline.predict(X_test)\n\nmse_train_optimal = mean_squared_error(y_train, y_pred_train_optimal)\nmse_test_optimal = mean_squared_error(y_test, y_pred_test_optimal)\nrmse_train_optimal = np.sqrt(mse_train_optimal)\nrmse_test_optimal = np.sqrt(mse_test_optimal)\n\nprint(\"Optimal Model Metrics:\")\nprint(f\"Train Mean Squared Error: {mse_train_optimal}\")\nprint(f\"Test Mean Squared Error: {mse_test_optimal}\")\nprint(f\"Train Root Mean Squared Error: {rmse_train_optimal}\")\nprint(f\"Test Root Mean Squared Error: {rmse_test_optimal}\")\n\n\nOptimal Model Metrics:\nTrain Mean Squared Error: 6362.505730760978\nTest Mean Squared Error: 1823.3688800609457\nTrain Root Mean Squared Error: 79.76531659036387\nTest Root Mean Squared Error: 42.700923644119754\n\n\n\n\nCode\nrf_regressor = pipeline.named_steps['regressor']\n\ncategorical_features = X.select_dtypes(include=['object']).columns\nfeature_names = list(numeric_features) + list(\n    pipeline.named_steps['preprocessor'].named_transformers_['cat']\n    .named_steps['onehot'].get_feature_names_out(categorical_features)\n)\n\nfor i in range(min(3, len(rf_regressor.estimators_))):\n    plt.figure(figsize=(20, 10))\n    plot_tree(rf_regressor.estimators_[i], feature_names=feature_names, filled=True, rounded=True)\n    plt.title(f\"Random Forest Tree {i+1}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, the most important predictors still seem to be Agency, City and Mode. However, the MSE is greater than our optimal decision tree, which means that this model is less accurate."
  },
  {
    "objectID": "trees.html#conclusion",
    "href": "trees.html#conclusion",
    "title": "Regression trees and Random Forests",
    "section": "Conclusion",
    "text": "Conclusion\nFrom the previous results, we can conclude that the best model to predict the mpg data is our optimal decision tree with a depth of 5. This model has a MSE of for 941.61 for the test data, which seems to be a good result due to the distribution of our data as seen before. Furthermore, the most important predictors are Agency, City, TOS, and Mode, which means that these variables are the most important to predict the mpg data. This could give us insights on what to do to improve the average miles per galon, which factors are better, and how much fossil fuels we will use in the future based on different characteristics.\nNow that it seems that we have a good model to predict the mpg data, we can use it to predict the mpg data for the next year. Moreover, we can start to investigate how different agencies, cities, TOS, and Mode might affect the efficiency of the vehicles used and make recommendations for changes in the future. This will help ius visualize how much our carbon footprint will be reduced in the future with those changes and we can predict our future mpg based on those changes."
  },
  {
    "objectID": "Voterlabeling.html#overview",
    "href": "Voterlabeling.html#overview",
    "title": "Party Labeling",
    "section": "Overview",
    "text": "Overview\nThe objective of this tab is to showcase the approach taken to labeling the party based on the combination between sentiment analysis and keywords. The job run to do this can be found in the repository under spark_jobs/SecondCleaning_and_Labeling/ in the file Labeling.py.\nWe labelled our data as either “Democrat”, “Republican”, or “No Party.” The “No Party” label was used if we were not able to confidently determine which party they are supporting based on the attributes of the post. We want to highlight that this approach has been on the cautious side, meaning we have only labed as a specific party when we were very confident about it, at the risk of discarding more posts than we strictly needed to."
  },
  {
    "objectID": "Voterlabeling.html#labeling-process",
    "href": "Voterlabeling.html#labeling-process",
    "title": "Party Labeling",
    "section": "Labeling Process",
    "text": "Labeling Process\nFor our labeling process, we used keywords that heavily lean to the left (Democrat) and others that heavily lean to the right (Republican), as well as neutral keywords that are more nuanced and not as straight forward.\nBelow is our list of left leaning keywords:\n\npro-choice\nclimate justice\nminimum wage\nMedicare\ngun control\nBlack Lives Matter\nstudent loan forgiveness\nclean energy\nParis Agreement\nMedicare for All\n\nAnd here is a list of right leaning keywords:\n\nNRA lobby\nSecond Amendment rights\nlaw enforcement\nborder\nheartbeat bill\ngun rights\ntaxes\nvoter fraud\nbuild the wall\nillegal immigration\n\nThe neutral keywords are the tricky ones. We decided to use the current landscape of US politics to determine which neutral keywords could potentially lean to the left or right. Please find below our logic for selecting those keywords:\n\nLabeling Logic for left leaning neutral words:\nRoe_v_Wade: Associated with reproductive rights and access to abortion, a policy stance strongly supported by Democrats.\nclimate change: Democrats typically advocate for addressing climate change through policies like renewable energy investments and carbon emissions reductions.\nrenewable energy:  Promoting renewable energy sources such as wind and solar power is a hallmark of Democratic environmental and economic policies.\nBlack Lives Matter:  A movement associated with social justice and addressing systemic racism, which aligns with Democratic platforms.\ngun control:  Advocacy for stricter gun laws and regulations is primarily championed by Democrats.\nsolar energy:  Another renewable energy focus strongly tied to Democratic environmental initiatives.\nuniversal healthcare:  The Democratic party often campaigns for expanded healthcare access, such as Medicare for All or similar policies.\nMedicare:  While Medicare itself is non-partisan, its expansion and support for universal healthcare are commonly associated with Democrats.\nMedicaid:  Similarly, Medicaid expansion and funding align with Democratic priorities for healthcare equity.\n\n\nLabeling Logic for right leaning neutral words:\nlaw enforcement: Republicans often emphasize support for law enforcement and policies promoting law and order.\nborder patrol: Strong support for border patrol is associated with Republican efforts to secure the U.S. borders and address illegal immigration.\nvoter turnout: While non-partisan in nature, Republicans often focus on issues like voter ID laws and election integrity when discussing voter turnout.\nSecond Amendment: Republicans strongly support Second Amendment rights, focusing on protecting gun ownership and opposing restrictions.\nschool board elections: Republicans have recently emphasized local control over education and parental rights, often framing school board elections as a battleground for issues like curriculum content.\nmass shootings: While mass shootings are non-partisan, Republican responses often highlight mental health solutions or oppose gun control measures.\n** For display purposes on this tab, we will use a subset of our Reddit data from the r/politics subreddit to showcase the code and execution for labeling **"
  },
  {
    "objectID": "Voterlabeling.html#labeling-execution",
    "href": "Voterlabeling.html#labeling-execution",
    "title": "Party Labeling",
    "section": "Labeling Execution",
    "text": "Labeling Execution\n\n\nCode\nspark\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 84, 6, Finished, Available, Finished)\n\n\n\n            \n                SparkSession - hive\n                \n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1.5.2.20240522.3\n              Master\n                yarn\n              AppName\n                Azure ML Experiment\n            \n        \n        \n            \n        \n\n\n\n\nCode\nfrom azureml.core import Workspace, Dataset, Datastore\nfrom pyspark.sql import functions as F\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 84, 7, Finished, Available, Finished)\n\n\n\n\nCode\n#read data\nsubscription_id = '21ff0fc0-dd2c-450d-93b7-96eeb3699b22'\nresource_group = 'project-group-11'\nworkspace_name = 'project-group-11'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndatastore = Datastore.get(workspace, \"workspaceblobstore\")\n\ndataset = Dataset.Tabular.from_parquet_files(path=(datastore, 'sampledata/politics_both_with_sentiment_and_keywords_and_weights.parquet'))\ndf = dataset.to_spark_dataframe()\ndf.head(1)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 84, 8, Finished, Available, Finished)\n\n\nPerforming interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code FEZU8XGR8 to authenticate.\nInteractive authentication successfully completed.\n\n\n[Row(id='ih59f7l', author='JokerJangles123', body=\"Only to morons that never paid any attention to the election process until an orange moron convinced them that anything that they didn't understand was now something to be suspicious of\", created_utc=datetime.datetime(2022, 7, 22, 3, 56, 5), year=2022, month=7, subreddit='politics', lemmatized_body='moron never pay attention election process orange moron convinced anything didnt understand something suspicious', sentiment='negative', tough on crime=0, Bidenflation=0, pro-choice=0, solar energy=0, insurance=0, pro-life=0, immigration=0, border=0, public safety=0, Roe_v_Wade=0, common-sense gun laws=0, sanctuary cities=0, students=0, climate justice=0, minimum wage=0, late-term abortion=0, living wage=0, NRA lobby=0, voter fraud=0, Medicare=0, Medicaid=0, corporate greed=0, Second Amendment rights=0, background checks=0, redistricting=0, abortion access=0, Soros-funded prosecutors=0, law enforcement=0, soft on crime=0, electric vehicles=0, gas prices=0, mass migration=0, fracking=0, renewable energy=0, red flag laws=0, racial profiling=0, social programs=0, parental rights=0, election integrity=0, defund the police=0, free college=0, my body my choice=0, fetus=0, migrants=0, Voting Rights Act=0, border patrol=0, stimulus checks=0, global warming=0, police reform=0, polling=0, citizenship=0, woke indoctrination=0, illegal immigration=0, stolen election=0, Green New Deal=0, Republican=0, crime=0, mental health services=0, police accountability=0, carbon footprint=0, teacher funding=0, education=0, vaccine mandates=0, healthcare=0, wind power=0, fossil fuels=0, healthcare is a human right=0, school choice=0, tax the rich=0, protect the unborn=0, AR-15=0, public health=0, firearm=0, big government=0, cost of living=0, tuition=0, heartbeat bill=0, SCOTUS=0, energy independence=0, Republicans=0, systemic racism=0, detention centers=0, Trump=0, build the wall=0, personal responsibility=0, disenfranchisement=0, Biden=0, DACA=0, gun violence=0, standardized testing=0, overtaxed=0, Democrat=0, economic freedom=0, ACA=0, Dreamers=0, carbon emissions=0, carbon tax=0, GDP=0, Democratic=0, January 6th=0, reproductive justice=0, climate hoax=0, inclusive curriculum=0, economy=0, border crisis=0, Department of Education=0, wages=0, recession=0, green tyranny=0, coal jobs=0, violent crime=0, ballot=0, mail-in ballot abuse=0, abortion=0, Second Amendment=0, reproductive rights=0, taxes=0, critical race theory=0, Federal Reserve=0, safety=0, equity in education=0, college=0, mass shooting=0, immigrant rights=0, Medicare for All=0, mass shootings=0, pathway to citizenship=0, socialized medicine=0, drop boxes=0, student loan forgiveness=0, abortion law=0, environmental crisis=0, curriculum=0, asylum=0, ban assault weapons=0, voting=0, protect gun ownership=0, climate change=0, women’s rights=0, gerrymandering=0, book bans=0, expand voting rights=0, clean energy=0, trigger laws=0, voucher programs=0, net zero=0, gun-free zones=0, school shootings=0, Paris Agreement=0, early voting=0, midterms=0, Planned Parenthood=0, wealth inequality=0, forced birth=0, Dobbs decision=0, gun rights=0, policing=0, universal healthcare=0, absentee ballot=0, Title 42=0, public health crisis=0, voter turnout=0, gun control=0, Black Lives Matter=0, heartbeat detection=0, unemployment=0, prison reform=0, teaching=0, Democrats=0, bail reform=0, open carry=0, law and order=0, private insurance=0, Keystone pipeline=0, ICE=0, voter suppression=0, abortion ban=0, Obamacare repeal=0, election=1, pharmaceutical greed=0, COVID-19=0, interest rates=0, migrant caravans=0, inflation=0, fair wages=0, family reunification=0, border security=0, debt ceiling=0, illegal aliens=0, supply chain=0, concealed carry=0, unborn child=0, rigged election=0, schools=0, school board elections=0, ContainsLeft=0, ContainsRight=0, ContainsGeneric=1, SumLeft=0, SumRight=0, SumGeneric=1)]\n\n\n\n\nCode\n# List of left keywords for reference\nleft_keywords = [\n    \"pro-choice\", \"climate justice\", \"minimum wage\", \"Medicare\", \n    \"gun control\", \"Black Lives Matter\", \"student loan forgiveness\", \n    \"clean energy\", \"Paris Agreement\", \"Medicare for All\"\n]\n\n# List of right keywords for reference\nright_keywords = [\n    \"NRA lobby\", \"Second Amendment rights\", \"law enforcement\", \n    \"border\", \"heartbeat bill\", \"gun rights\", \"taxes\", \n    \"voter fraud\", \"build the wall\", \"illegal immigration\"\n]\n\n# list of neutral left and neutral right keywords\nneutral_left = [\n    \"Roe_v_Wade\", \"climate change\", \"renewable energy\", \"Black Lives Matter\",\n    \"gun control\", \"solar energy\", \"universal healthcare\", \"Medicare\",\n    \"Medicaid\"\n]\nneutral_right = [\n    \"law enforcement\", \"border patrol\", \"voter turnout\", \"Second Amendment\",\n    \"school board elections\", \"mass shootings\"\n]\n\n# list of pure neutral keywords\npure_neutral = [\"Roe_v_Wade\", \"climate change\", \"renewable energy\", \"Black Lives Matter\",\n    \"gun control\", \"solar energy\", \"universal healthcare\", \"Medicare\",\n    \"Medicaid\", \"law enforcement\", \"border patrol\", \"voter turnout\", \"Second Amendment\",\n    \"school board elections\", \"mass shootings\",\n    \"GDP\", \"stimulus checks\", \"public health\", \"interest rates\",\n    \"supply chain\", \"healthcare\", \"heartbeat bill\", \"drop boxes\",\n    \"Planned Parenthood\", \"Paris Agreement\", \"public safety\",\n    \"January 6th\", \"mass shooting\", \"fetus\", \"trigger laws\",\n    \"global warming\", \"background checks\", \"vaccine mandates\",\n    \"police reform\", \"environmental crisis\", \"abortion access\",\n    \"crime\", \"polling\", \"education\", \"wages\", \"recession\",\n    \"violent crime\", \"ballot\", \"safety\", \"teaching\", \"schools\",\n    \"equity in education\", \"college\", \"pathway to citizenship\",\n    \"concealed carry\", \"Title 42\", \"absentee ballot\", \"firearm\",\n    \"public health crisis\", \"carbon emissions\", \"wind power\",\n    \"early voting\", \"carbon footprint\", \"tuition\", \"school shootings\",\n    \"cost of living\", \"environmental crisis\", \"asylum\",\n    \"pharmaceutical greed\", \"electric vehicles\", \"climate justice\",\n    \"fossil fuels\", \"immigration\", \"students\"\n]\n\n# Compute the Party column\ndf = df.withColumn(\n    \"Party\",\n    F.when(\n        # Positive sentiment and left-leaning\n        (F.col(\"ContainsLeft\") == 1) & (F.col(\"sentiment\") == \"positive\"),\n        \"Democrat\"\n    ).when(\n        # Positive sentiment and right-leaning\n        (F.col(\"ContainsRight\") == 1) & (F.col(\"sentiment\") == \"positive\"),\n        \"Republican\"\n    ).when(\n        # Negative sentiment and left-leaning (flip to Republican)\n        (F.col(\"ContainsLeft\") == 1) & (F.col(\"sentiment\") == \"negative\"),\n        \"Republican\"\n    ).when(\n        # Negative sentiment and right-leaning (flip to Democrat)\n        (F.col(\"ContainsRight\") == 1) & (F.col(\"sentiment\") == \"negative\"),\n        \"Democrat\"\n    ).when(\n        # Neutral keywords with sentiment context - Positive Sentiment\n        (F.col(\"ContainsGeneric\") == 1) & (F.col(\"sentiment\") == \"positive\"),\n        F.when(\n            # Neutral Left\n            F.col(\"body\").rlike(\n                r\"(Roe_v_Wade|climate change|renewable energy|Black Lives Matter|\"\n                r\"gun control|solar energy|universal healthcare|Medicare|Medicaid)\"\n            ),\n            \"Democrat\"\n        ).when(\n            # Neutral Right\n            F.col(\"body\").rlike(\n                r\"(law enforcement|border patrol|voter turnout|Second Amendment|\"\n                r\"school board elections|mass shootings)\"\n            ),\n            \"Republican\"\n        ).otherwise(\"No Party\")\n    ).when(\n        # Neutral keywords with sentiment context - Negative Sentiment\n        (F.col(\"ContainsGeneric\") == 1) & (F.col(\"sentiment\") == \"negative\"),\n        F.when(\n            # Neutral Right (flipped to Democrat)\n            F.col(\"body\").rlike(\n                r\"(law enforcement|border patrol|mass shootings|Second Amendment|\"\n                r\"school board elections)\"\n            ),\n            \"Democrat\"\n        ).when(\n            # Neutral Left (flipped to Republican)\n            F.col(\"body\").rlike(\n                r\"(Roe_v_Wade|climate change|renewable energy|Black Lives Matter|\"\n                r\"gun control|solar energy|universal healthcare|Medicare|Medicaid)\"\n            ),\n            \"Republican\"\n        ).otherwise(\"No Party\")\n    ).otherwise(\"No Party\")  # Default case\n)\n\n# Display results\ndf.select(\"id\", \"Party\").show(truncate=False)\n\n\nStatementMeta(16fd2129-cd11-4016-9021-0a19ae621014, 84, 9, Finished, Available, Finished)\n\n\n+-------+----------+\n|id     |Party     |\n+-------+----------+\n|ih59f7l|No Party  |\n|ih59mrg|No Party  |\n|ih59xkt|Republican|\n|isiltf8|No Party  |\n|ii35276|Republican|\n|ii35o3x|Democrat  |\n|i52w72k|Republican|\n|i52wehw|Republican|\n|i52xgkk|Democrat  |\n|i955eoq|Democrat  |\n|ibofcih|No Party  |\n|hxfxmw3|Democrat  |\n|i9qqfx4|Democrat  |\n|i9qqfx4|Democrat  |\n|i7lmhfo|No Party  |\n|i7lmhfo|No Party  |\n|i7lmw6h|Democrat  |\n|i7lmw6h|Democrat  |\n|i7lnl0d|No Party  |\n|i7lnl0d|No Party  |\n+-------+----------+\nonly showing top 20 rows\n\n\n\n** The code was then executed via job against the clean Reddit dataset **"
  },
  {
    "objectID": "Voterlabeling.html#label-counts",
    "href": "Voterlabeling.html#label-counts",
    "title": "Party Labeling",
    "section": "Label Counts",
    "text": "Label Counts\nThese are the counts of each label given to our dataset. As you can see, many of them have “No Party” as the label. In future iterations of this project, we may want to revisit the labeling logic to be less aggressive. However, this conservative approach really ensures the quality of our labels.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# read data from parquet\ndf = pd.read_parquet('../data/Results_Reddit.parquet')\n\ntotal_counts = df.groupby('Party')['party_count'].sum()\ndr_counts = total_counts[total_counts.index.isin(['Democrat', 'Republican'])]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 10))\n\n# First subplot for Democrat and Republican\ndr_counts.plot(kind='bar', ax=axes[0], color=['#0015BC', '#E81B23'])\naxes[0].set_title('Count of Democrat and Republican')\naxes[0].set_xlabel(' ')\naxes[0].set_ylabel('Count in K')\naxes[0].spines['top'].set_visible(False)\naxes[0].spines['right'].set_visible(False)\naxes[0].tick_params(axis='x', rotation=0)\nformatter = ticker.FuncFormatter(lambda x, pos: f'{int(x/1e3)}K')\naxes[0].yaxis.set_major_formatter(formatter)\n\nfor p in axes[0].patches:\n    axes[0].annotate(f'{int(p.get_height()/1000)}K', (p.get_x() + p.get_width() / 2., p.get_height()),\n                     ha='center', va='center', fontsize=12, color='black', xytext=(0, 10),\n                     textcoords='offset points')\n\n\n# Second subplot for all parties\ntotal_counts.plot(kind='bar', ax=axes[1], color=['#0015BC', '#EC9A29', '#E81B23'])\naxes[1].set_title('Total Count of Each Label')\naxes[1].set_xlabel(' ')\naxes[1].set_ylabel('Count in M')\naxes[1].spines['top'].set_visible(False)\naxes[1].spines['right'].set_visible(False)\naxes[1].tick_params(axis='x', rotation=0)\nformatter = ticker.FuncFormatter(lambda x, pos: f'{int(x/1e6)}M')\naxes[1].yaxis.set_major_formatter(formatter)\n\nfor p in axes[1].patches:\n    if p.get_height() &gt;= 1e6:\n        annotation = f'{int(p.get_height()/1e6)}M'\n    else:\n        annotation = f'{int(p.get_height()/1e3)}K'\n    axes[1].annotate(annotation, (p.get_x() + p.get_width() / 2., p.get_height()),\n                     ha='center', va='center', fontsize=12, color='black', xytext=(0, 10),\n                     textcoords='offset points')\n\n\n\n# Show the plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Subset_selection.html",
    "href": "Subset_selection.html",
    "title": "Subset selection",
    "section": "",
    "text": "Assignment by: Dr. Purna Gamage\nInstructions\nSubmission:\nOptional:\nCode\n#knitr::opts_chunk$set(include = FALSE) # for making prompts\n#knitr::opts_chunk$set(echo = TRUE) # for making solutions\nlibrary(tidyverse)\nlibrary(modeldata)\nlibrary(leaps)\nlibrary(caret)\nlibrary(corrplot)\nlibrary(ggplot2)\nlibrary(viridis)"
  },
  {
    "objectID": "Subset_selection.html#hw-2.1-heating-values",
    "href": "Subset_selection.html#hw-2.1-heating-values",
    "title": "Subset selection",
    "section": "HW-2.1: Heating values",
    "text": "HW-2.1: Heating values\nUse best subset selection methods to determine the best heating values\nFor bioenergy production, the heating value is a measure of the amount of heat released during combustion. The Higher heating value (HHV) is a particular method for determining the heat released during combustion. The higher HHV the more energy released for a given amount of material. You will use the biomass dataset from the modeldata package. Run a ?biomass after importing the data to read about the domain. The response variable is HHV and the predictor variables are the percentages of different elements. Do not include the sample and dataset variables in your analysis.\nHW-2.1.a Create scatter-plots of the response and predictor variables and comment on your findings.\n\n\nCode\n# IMPORT \ndata(biomass)\n\n# EXPLORE DATA\nprint(class(biomass))\n\n\n[1] \"data.frame\"\n\n\nCode\nprint(dim(biomass))\n\n\n[1] 536   8\n\n\nCode\nprint(biomass[1:3,1:8])\n\n\n                  sample  dataset carbon hydrogen oxygen nitrogen sulfur    HHV\n1           Akhrot Shell Training  49.81     5.64  42.94     0.41   0.00 20.008\n2 Alabama Oak Wood Waste Training  49.50     5.70  41.30     0.20   0.00 19.228\n3                  Alder Training  47.82     5.80  46.25     0.11   0.02 18.299\n\n\n\n\nCode\n# INSERT CODE \n\nmy_theme &lt;- readRDS('~/.Rthemes/my_theme.rds')\n\nggplot(biomass, aes(x = carbon, y = HHV)) + \n  geom_point(aes(color = carbon)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\") +\n  labs(title = \"carbon vs HHV\", \n       x = \"carbon\", \n       y = \"HHV\") + \n  my_theme()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\nggplot(biomass, aes(x = hydrogen, y = HHV)) + \n  geom_point(aes(color = hydrogen)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\") +\n  labs(title = \"hydrogen vs HHV\", \n       x = \"hydrogen\", \n       y = \"HHV\") + \n  my_theme()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\nggplot(biomass, aes(x = oxygen, y = HHV)) + \n  geom_point(aes(color = oxygen)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\") +\n  labs(title = \"oxygen vs HHV\", \n       x = \"oxygen\", \n       y = \"HHV\") + \n  my_theme()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\nggplot(biomass, aes(x = nitrogen, y = HHV)) + \n  geom_point(aes(color = nitrogen)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\") +\n  labs(title = \"nitrogen vs HHV\", \n       x = \"nitrogen\", \n       y = \"HHV\") + \n  my_theme()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\nggplot(biomass, aes(x = sulfur, y = HHV)) + \n  geom_point(aes(color = sulfur)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\") +\n  labs(title = \"sulfur vs HHV\", \n       x = \"sulfur\", \n       y = \"HHV\") + \n  my_theme()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can see from these scatter plots that carbon has the strongest correlation with HHV, but oxygen and hydrogen seem to also have some. For Nitrogen and sulfur is hard to tell. Thus, we will also be checking this with a correlation plot for additional visualization.\n\n\nCode\n# Generate a correlation plot\ncor_matrix &lt;- cor(biomass[, sapply(biomass, is.numeric)])\n\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", order = \"hclust\", \n         tl.col = \"black\", tl.srt = 45, \n         addCoef.col = \"black\",\n         diag = FALSE)\n\n\n\n\n\n\n\n\n\nThis proves what we mention, and oxigen and hydrogen still seem to have a “strong” correlation with HHV.\nHW-2.1.b Split the dataset into an 80-20 training and test sets.\n\n\nCode\n# INSERT CODE \npaste(\"Current training labelled:\",(sum(biomass$dataset == \"Training\"))/nrow(biomass))\n\n\n[1] \"Current training labelled: 0.850746268656716\"\n\n\nCode\npaste(\"Current testing labelled:\",(sum(biomass$dataset == \"Testing\"))/nrow(biomass))\n\n\n[1] \"Current testing labelled: 0.149253731343284\"\n\n\nWe will drop the column dataset as we are not using it and it does not have the training or test quantities we want, and then split the data.\n\n\nCode\n# drop dataset column\nbiomass &lt;- subset(biomass, select = -dataset)\n# checking column names\ncolnames(biomass)\n\n\n[1] \"sample\"   \"carbon\"   \"hydrogen\" \"oxygen\"   \"nitrogen\" \"sulfur\"   \"HHV\"     \n\n\n\n\nCode\n# Partition data\npartition &lt;- createDataPartition(biomass$HHV, p = 0.8, list = FALSE)\ntraining_set &lt;- biomass[partition, ]\ntesting_set &lt;- biomass[-partition, ]\n\n\nWe will also drop column sample as it is unique for each observatioin and it is categorical.\n\n\nCode\ntraining_set &lt;- subset(biomass, select = -sample)\ntesting_set &lt;- subset(biomass, select = -sample)\n\n\nHW-2.1.c Use regsubsets() to perform best subset selection to pick the best model according to \\(C_p\\), BIC, and adjusted \\(R^2\\).\n\n\nCode\n# INSERT CODE \nset.seed(4444)\n\nresult &lt;- regsubsets(HHV ~ ., data = training_set, nvmax = 5, method = \"exhaustive\")\n\nsummary_result &lt;- summary(result)\n\n\n\n\nCode\npar(mfrow = c(2, 2))\n\nplot(summary_result$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(summary_result$cp), summary_result$cp[which.min(summary_result$cp)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary_result$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(summary_result$bic), summary_result$bic[which.min(summary_result$bic)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary_result$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\npoints(which.max(summary_result$adjr2), summary_result$adjr2[which.max(summary_result$adjr2)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\n\n\n\n\nIt seems that a three variable model is our best pick from this graphs.\nHW-2.1.d Repeat this procedure for forward stepwise selection and backward stepwise selection, compare the best models from each selection method.\n\n\nCode\n# INSERT CODE\nforward_model &lt;- regsubsets(HHV ~ ., data=training_set, method=\"forward\", nvmax = NULL)\n\nforward_summary &lt;- summary(forward_model)\n\nbackward_model &lt;- regsubsets(HHV ~ ., data=training_set, method=\"backward\", nvmax=NULL)\n\nbackward_summary &lt;- summary(backward_model)\n\n\n\n\nCode\n# INSERT CODE \n# Plot forward\npar(mfrow = c(2, 2))\n\nplot(forward_summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(forward_summary$cp), forward_summary$cp[which.min(forward_summary$cp)], col = \"red\", cex = 2, pch = 20)\n\nplot(forward_summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(forward_summary$bic), forward_summary$bic[which.min(forward_summary$bic)], col = \"red\", cex = 2, pch = 20)\n\nplot(forward_summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\npoints(which.max(forward_summary$adjr2), forward_summary$adjr2[which.max(forward_summary$adjr2)], col = \"red\", cex = 2, pch = 20)\n\n# Plot Backwards\npar(mfrow = c(2, 2))\n\n\n\n\n\n\n\n\n\nCode\nplot(backward_summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(backward_summary$cp), backward_summary$cp[which.min(backward_summary$cp)], col = \"red\", cex = 2, pch = 20)\n\nplot(backward_summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(backward_summary$bic), backward_summary$bic[which.min(backward_summary$bic)], col = \"red\", cex = 2, pch = 20)\n\nplot(backward_summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\npoints(which.max(backward_summary$adjr2), backward_summary$adjr2[which.max(backward_summary$adjr2)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\n\n\n\n\nWe can see that these methods also say that three variable model is our best option.\nHW-2.1.e Use the predict() function to investigate the test performance in RMSE using your “best model”.\n\n\nCode\n# INSERT CODE \n\n# Select the best model for each\nbackward_best_index &lt;- which.max(backward_summary$adjr2)\nforward_best_index &lt;- which.max(forward_summary$adjr2)\n\n# Best variables\nbackward_best_vars &lt;- names(coef(backward_model, id = backward_best_index))\nforward_best_vars &lt;- names(coef(forward_model, id = forward_best_index))\n# Without intercept\nbackward_best_vars &lt;- backward_best_vars[backward_best_vars != \"(Intercept)\"]\nforward_best_vars &lt;- forward_best_vars[forward_best_vars != \"(Intercept)\"]\n\n# formulas so I can fit lm\nbackward_formula_str &lt;- paste(\"HHV ~\", paste(backward_best_vars, collapse=\" + \"))\nforward_formula_str &lt;- paste(\"HHV ~\", paste(forward_best_vars, collapse=\" + \"))\n\nbackward_formula &lt;- as.formula(backward_formula_str)\nforward_formula &lt;- as.formula(forward_formula_str)\n\n# Fit model\nbackwards_best_model_fit &lt;- lm(backward_formula, data=training_set)\nforwards_best_model_fit &lt;- lm(forward_formula, data=training_set)\n\n# preds\npredictions_back &lt;- predict(backwards_best_model_fit, newdata=testing_set)\npredictions_forw &lt;- predict(forwards_best_model_fit, newdata=testing_set)\n\n# RMSE\nrmse_back &lt;- sqrt(mean((testing_set$HHV - predictions_back)^2, na.rm = TRUE))\nrmse_forw &lt;- sqrt(mean((testing_set$HHV - predictions_forw)^2, na.rm = TRUE))\n\n# RMSE\nprint(paste(\"Backward RMSE:\", rmse_back))\n\n\n[1] \"Backward RMSE: 1.43045828417102\"\n\n\nCode\nprint(paste(\"Forward RMSE:\", rmse_forw))\n\n\n[1] \"Forward RMSE: 1.43045828417102\""
  },
  {
    "objectID": "Subset_selection.html#hw-2.2-loan-applications",
    "href": "Subset_selection.html#hw-2.2-loan-applications",
    "title": "Subset selection",
    "section": "HW-2.2: Loan applications",
    "text": "HW-2.2: Loan applications\nCreate your own cross validation algorithm to predict the interest rate for loan applications\nLending club gained fame for being one of the first major players in retail lending. The dataset lending_club in the model_data package includes 9,857 loans that were provided. Interest rates of loans are often a good indicator of the level of risk associated with lending. If you are likely to pay back a loan, then you will likely be charged lower interest than someone who has a higher chance of default. Your goal is to determine the best model for predicting the interest rate charged to borrowers using best, forward, and backward subset selection within a five-fold cross-validation framework.\nPrep steps: - drop all rows with missing data in the following columns\nHW-2.2.a Create a correlation plot of all the numeric variables in the dataset using the corrplot package to create a high quality graph, then comment on your findings\n\n\nCode\n# INSERT CODE\ndata(\"lending_club\", package = \"modeldata\")\nloan &lt;- lending_club\nnumeric_vars &lt;- select_if(loan, is.numeric)\n\ncor_matrix &lt;- cor(loan[, sapply(loan, is.numeric)])\n\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", order = \"hclust\", \n         tl.col = \"black\", tl.srt = 45, \n         addCoef.col = \"black\",\n         diag = FALSE)\n\n\n\n\n\n\n\n\n\nWhile there seems to be correlation between other different variables, interest rate is correlated with many others but at a very low scale, the most being with all_util with 0.29. All the other correlations are between $$0.25 which are not very strong correlations. This may make having a good model more complicated, but it can still be done (up to a certain point).\nHW-2.2.b Run best, forward, and backward subset selection on the entire dataset comment on the findings\nWe will retrieve only the numeric variables (so taking away state) as professors said its ok since it will run for too long (even if we do one hot encoding).\n\n\nCode\nloan &lt;- na.omit(lending_club)\n\nnumeric_loan &lt;- loan %&gt;% select_if(is.numeric)\n\n\n\n\nCode\n# INSERT CODE \n# Best\nbest_subset_fit &lt;- regsubsets(int_rate ~ ., data=numeric_loan, nvmax=NULL, method=\"exhaustive\", really.big = T)\n\nsummary_best &lt;- summary(best_subset_fit)\n\n#summary_best\n\npar(mfrow = c(2, 2))\n\nplot(summary_best$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(summary_best$cp), summary_best$cp[which.min(summary_best$cp)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary_best$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(summary_best$bic), summary_best$bic[which.min(summary_best$bic)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary_best$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\npoints(which.max(summary_best$adjr2), summary_best$adjr2[which.max(summary_best$adjr2)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# INSERT CODE \n# Forward\n\nforward_fit &lt;- regsubsets(int_rate ~ ., data=numeric_loan, method=\"forward\", nvmax=NULL)\n\nsummary_forward &lt;- summary(forward_fit)\n\n#summary_forward\n\npar(mfrow = c(2, 2))\n\nplot(summary_forward$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(summary_forward$cp), summary_forward$cp[which.min(summary_forward$cp)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary_forward$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(summary_forward$bic), summary_forward$bic[which.min(summary_forward$bic)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary_forward$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\npoints(which.max(summary_forward$adjr2), summary_forward$adjr2[which.max(summary_forward$adjr2)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# INSERT CODE \n# backward\n\nbackward_fit &lt;- regsubsets(int_rate ~ ., data=numeric_loan, method=\"backward\", nvmax=NULL)\n\nsummary_backward &lt;- summary(backward_fit)\n\n#summary_backward\n\npar(mfrow = c(2, 2))\n\nplot(summary_backward$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(summary_backward$cp), summary_backward$cp[which.min(summary_backward$cp)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary_backward$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(summary_backward$bic), summary_backward$bic[which.min(summary_backward$bic)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary_backward$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\npoints(which.max(summary_backward$adjr2), summary_backward$adjr2[which.max(summary_backward$adjr2)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\n\n\n\n\nThey all seem to agree that 10 variables is the best. While some variable will add better predictions for some of the measurements, the better value is so little that we can agree on 10.\nHW-2.2.c Create a five-fold cross-validation algorithm using for loops to compare the CV mse performance of your best two models\n\n\nCode\nlibrary(boot)\n\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:lattice':\n\n    melanoma\n\n\nCode\n#| vscode: {languageId: r}\n# INSERT CODE \n\n# Selecting the model based on CP as it is between BIC and r^2adj\nbest_bic_backward_index &lt;- which.min(summary_backward$cp)\nbest_vars_backward &lt;- names(coef(backward_fit, id = best_bic_backward_index))\n\nbest_bic_best_subset_index &lt;- which.min(summary_best$cp)\nbest_vars_best_subset &lt;- names(coef(best_subset_fit, id = best_bic_best_subset_index))\n\nbackward_formula &lt;- as.formula(paste(\"int_rate ~\", paste(best_vars_backward[-1], collapse = \" + \")))\nbest_subset_formula &lt;- as.formula(paste(\"int_rate ~\", paste(best_vars_best_subset[-1], collapse = \" + \")))\n\n\n\n\nCode\n# INSERT CODE \nset.seed(4444)\n\nfolds &lt;- cut(seq(1, nrow(numeric_loan)), breaks=5, labels=FALSE)\ncv_mse_backward &lt;- numeric(5)\ncv_mse_best_subset &lt;- numeric(5)\n\nfor(i in 1:5){\n  test_indices &lt;- which(folds == i)\n  train_indices &lt;- setdiff(seq_len(nrow(numeric_loan)), test_indices)\n  \n  train_set &lt;- numeric_loan[train_indices, ]\n  test_set &lt;- numeric_loan[test_indices, ]\n  # Fit both models\n  fit_backward &lt;- glm(backward_formula, data = train_set)\n  fit_best_subset &lt;- glm(best_subset_formula, data = train_set)\n  # Preds of both\n  predictions_backward &lt;- predict(fit_backward, newdata = test_set)\n  predictions_best_subset &lt;- predict(fit_best_subset, newdata = test_set)\n  # MSE\n  cv_mse_backward[i] &lt;- mean((test_set$int_rate - predictions_backward)^2, na.rm = TRUE)\n  cv_mse_best_subset[i] &lt;- mean((test_set$int_rate - predictions_best_subset)^2, na.rm = TRUE)\n}\n\n# Avg MSE\navg_cv_mse_backward &lt;- mean(cv_mse_backward)\navg_cv_mse_best_subset &lt;- mean(cv_mse_best_subset)\n\nprint(paste(\"Average CV MSE for Backward Selection:\", avg_cv_mse_backward))\n\n\n[1] \"Average CV MSE for Backward Selection: 17.5556682537638\"\n\n\nCode\nprint(paste(\"Average CV MSE for Exhaustive Selection:\", avg_cv_mse_best_subset))\n\n\n[1] \"Average CV MSE for Exhaustive Selection: 17.5556682537638\""
  },
  {
    "objectID": "Subset_selection.html#hw-2.4-advertising-budget",
    "href": "Subset_selection.html#hw-2.4-advertising-budget",
    "title": "Subset selection",
    "section": "HW-2.4: Advertising budget",
    "text": "HW-2.4: Advertising budget\nUsing cross-validation to select best advertising budget\nIn this problem, we use the Advertising data download here. We want to predict Sales from TV, Radio and Newspaper, using multiple regression with all three predictors plus up to one interaction term of these three predictors, e.g. TV * Radio or Radio * Newspaper.\nHW-2.4.a Should such an interaction term be included? Which one? Try to answer this question by estimating the residual standard error using 10-fold cross validation for all four possible models.\nPossible models:\n\nSales \\(\\sim\\) TV + Radio + Newspaper\nSales \\(\\sim\\) TV + Radio + Newspaper + TV:Radio\nSales \\(\\sim\\) TV + Radio + Newspaper + TV:Newspaper\nSales \\(\\sim\\) TV + Radio + Newspaper + Radio:Newspaper\n\n\n\nCode\ndf &lt;- read_csv('https://www.statlearning.com/s/Advertising.csv')\n\n\nNew names:\nRows: 200 Columns: 5\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(5): ...1, TV, radio, newspaper, sales\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\n\nCode\n# list possible models\nmodels &lt;- list(\n  model1 = sales ~ TV + radio + newspaper,\n  model2 = sales ~ TV + radio + newspaper + TV:radio,\n  model3 = sales ~ TV + radio + newspaper + TV:newspaper,\n  model4 = sales ~ TV + radio + newspaper + radio:newspaper\n)\n\n# for storing results\ncv_results &lt;- vector(\"list\", length(models))\nnames(cv_results) &lt;- c(\"Base\", \"TV:Radio\", \"TV:Newspaper\", \"Radio:Newspaper\")\n\n# loop around possible models\nfor (i in seq_along(models)) {\n  glm_model_selected &lt;- glm(formula = models[[i]], data = df)\n  cv_result &lt;- cv.glm(df, glm_model_selected, K = 10)\n  \n  cv_results[[i]] &lt;- list(cv_error = cv_result$delta[1])\n}\n\n# Results fixing diplay (look like a data frame / tibble)\ncv_results_df &lt;- bind_rows(cv_results, .id = \"Model\")\n\n# Print the results\nprint(cv_results_df)\n\n\n# A tibble: 4 × 2\n  Model           cv_error\n  &lt;chr&gt;              &lt;dbl&gt;\n1 Base               2.93 \n2 TV:Radio           0.943\n3 TV:Newspaper       2.89 \n4 Radio:Newspaper    3.06 \n\n\nTV interacting with radia should be included as the error is significantly decreased.\nHW-2.4.b Create a single plot showing the return on investment of each advertising method where the y-axis is Sales and the x-axis is advertising dollars. There should be three lines, one for each method. The slope is the coefficient from you regression. What is the best advertising method to invest in based on return on investment?\n\n\nCode\n# INSERT CODE\n\n# Models\nmodel_tv &lt;- lm(sales ~ TV, data = df)\nmodel_radio &lt;- lm(sales ~ radio, data = df)\nmodel_newspaper &lt;- lm(sales ~ newspaper, data = df)\n\ncoeff_tv &lt;- coef(model_tv)\ncoeff_radio &lt;- coef(model_radio)\ncoeff_newspaper &lt;- coef(model_newspaper)\n\nad_dollars &lt;- seq(from = 0, to = max(df$TV, df$radio, df$newspaper), by = 1)\n\nsales_tv &lt;- coeff_tv[1] + coeff_tv[2] * ad_dollars\nsales_radio &lt;- coeff_radio[1] + coeff_radio[2] * ad_dollars\nsales_newspaper &lt;- coeff_newspaper[1] + coeff_newspaper[2] * ad_dollars\n\nplot_data &lt;- tibble(\n  AdvertisingDollars = c(ad_dollars, ad_dollars, ad_dollars),\n  sales = c(sales_tv, sales_radio, sales_newspaper),\n  Method = factor(rep(c(\"TV\", \"radio\", \"newspaper\"), each = length(ad_dollars)))\n)\n\n# Plot\nggplot(plot_data, aes(x = AdvertisingDollars, y = sales, color = Method)) +\n  geom_line() +\n  labs(title = \"ROI on advertisement method\",\n       x = \"Advertising $\",\n       y = \"Sales\",\n       color = \"Method\") +\n  my_theme()\n\n\n\n\n\n\n\n\n\nThe best on ROI seems to be radio, so we should recommend investing more in radio advertisements."
  },
  {
    "objectID": "Subset_selection.html#hw-2.5-islr-6.8-8a-d",
    "href": "Subset_selection.html#hw-2.5-islr-6.8-8a-d",
    "title": "Subset selection",
    "section": "HW-2.5: ISLR-6.8 #8(a-d)",
    "text": "HW-2.5: ISLR-6.8 #8(a-d)\n\nPart-A\n\n\nCode\n# INSERT CODE\nX &lt;- rnorm(100)\nnoise &lt;- rnorm(100)\n\n\n\n\nPart-B\n\n\nCode\n# INSERT CODE\nB_0 &lt;- 5\nB_1 &lt;- 5\nB_2 &lt;- 4\nB_3 &lt;- 4\n\nY &lt;- B_0 + B_1 * X + B_2 * X^2 + B_3 * X^3 + noise\n\n\n\n\nPart-C\n\n\nCode\n# INSERT CODE\n\nex_5_df &lt;- tibble(X, Y)\n\nmodel &lt;- regsubsets(Y ~ poly(X, 10, raw = TRUE), data = ex_5_df)\n\nsummary &lt;- summary(model)\n\nCP = which.min(summary$cp)\nBIC = which.min(summary$bic)\nR2 = which.max(summary$adjr2)\n\nbest_vars_CP &lt;- names(coef(model, id = CP))\nbest_vars_BIC &lt;- names(coef(model, id = BIC))\nbest_vars_R2 &lt;- names(coef(model, id = R2))\n\ncat(\"Best subets:\",\"\\n\")\n\n\nBest subets: \n\n\nCode\ncat(\"Cp:\", paste(best_vars_CP[-1], collapse = \", \"), \"\\n\",\"\\n\")\n\n\nCp: poly(X, 10, raw = TRUE)1, poly(X, 10, raw = TRUE)2, poly(X, 10, raw = TRUE)5, poly(X, 10, raw = TRUE)7, poly(X, 10, raw = TRUE)9, poly(X, 10, raw = TRUE)10 \n \n\n\nCode\ncat(\"BIC:\", paste(best_vars_BIC[-1], collapse = \", \"), \"\\n\",\"\\n\")\n\n\nBIC: poly(X, 10, raw = TRUE)1, poly(X, 10, raw = TRUE)2, poly(X, 10, raw = TRUE)3 \n \n\n\nCode\ncat(\"Adjusted R^2:\", paste(best_vars_R2[-1], collapse = \", \"), \"\\n\",\"\\n\")\n\n\nAdjusted R^2: poly(X, 10, raw = TRUE)1, poly(X, 10, raw = TRUE)2, poly(X, 10, raw = TRUE)5, poly(X, 10, raw = TRUE)7, poly(X, 10, raw = TRUE)9, poly(X, 10, raw = TRUE)10 \n \n\n\n\n\nCode\n# INSERT CODE\npar(mfrow = c(2, 2))\n\nplot(summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(summary$cp), summary$cp[which.min(summary$cp)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(summary$bic), summary$bic[which.min(summary$bic)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\npoints(which.max(summary$adjr2), summary$adjr2[which.max(summary$adjr2)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\n\n\n\n\n\n\nPart-D:\n\n\nForward selection\n\n\nCode\n# INSERT CODE\nmodel &lt;- regsubsets(Y ~ poly(X, 10, raw = TRUE), data = ex_5_df, method = \"forward\")\n\nsummary &lt;- summary(model)\n\nCP = which.min(summary$cp)\nBIC = which.min(summary$bic)\nR2 = which.max(summary$adjr2)\n\nbest_vars_CP &lt;- names(coef(model, id = CP))\nbest_vars_BIC &lt;- names(coef(model, id = BIC))\nbest_vars_R2 &lt;- names(coef(model, id = R2))\n\ncat(\"Best subets:\",\"\\n\")\n\n\nBest subets: \n\n\nCode\ncat(\"Cp:\", paste(best_vars_CP[-1], collapse = \", \"), \"\\n\",\"\\n\")\n\n\nCp: poly(X, 10, raw = TRUE)1, poly(X, 10, raw = TRUE)2, poly(X, 10, raw = TRUE)3 \n \n\n\nCode\ncat(\"BIC:\", paste(best_vars_BIC[-1], collapse = \", \"), \"\\n\",\"\\n\")\n\n\nBIC: poly(X, 10, raw = TRUE)1, poly(X, 10, raw = TRUE)2, poly(X, 10, raw = TRUE)3 \n \n\n\nCode\ncat(\"Adjusted R^2:\", paste(best_vars_R2[-1], collapse = \", \"), \"\\n\",\"\\n\")\n\n\nAdjusted R^2: poly(X, 10, raw = TRUE)1, poly(X, 10, raw = TRUE)2, poly(X, 10, raw = TRUE)3 \n \n\n\n\n\nCode\n# INSERT CODE\npar(mfrow = c(2, 2))\n\nplot(summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(summary$cp), summary$cp[which.min(summary$cp)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(summary$bic), summary$bic[which.min(summary$bic)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\npoints(which.max(summary$adjr2), summary$adjr2[which.max(summary$adjr2)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\n\n\n\n\n\n\nBackward selection\n\n\nCode\n# INSERT CODE\nmodel &lt;- regsubsets(Y ~ poly(X, 10, raw = TRUE), data = ex_5_df, method = \"backward\")\n\nsummary &lt;- summary(model)\n\nCP = which.min(summary$cp)\nBIC = which.min(summary$bic)\nR2 = which.max(summary$adjr2)\n\nbest_vars_CP &lt;- names(coef(model, id = CP))\nbest_vars_BIC &lt;- names(coef(model, id = BIC))\nbest_vars_R2 &lt;- names(coef(model, id = R2))\n\ncat(\"Best subets:\",\"\\n\")\n\n\nBest subets: \n\n\nCode\ncat(\"Cp:\", paste(best_vars_CP[-1], collapse = \", \"), \"\\n\",\"\\n\")\n\n\nCp: poly(X, 10, raw = TRUE)1, poly(X, 10, raw = TRUE)2, poly(X, 10, raw = TRUE)5, poly(X, 10, raw = TRUE)7, poly(X, 10, raw = TRUE)9, poly(X, 10, raw = TRUE)10 \n \n\n\nCode\ncat(\"BIC:\", paste(best_vars_BIC[-1], collapse = \", \"), \"\\n\",\"\\n\")\n\n\nBIC: poly(X, 10, raw = TRUE)1, poly(X, 10, raw = TRUE)2, poly(X, 10, raw = TRUE)5, poly(X, 10, raw = TRUE)7, poly(X, 10, raw = TRUE)9, poly(X, 10, raw = TRUE)10 \n \n\n\nCode\ncat(\"Adjusted R^2:\", paste(best_vars_R2[-1], collapse = \", \"), \"\\n\", \"\\n\")\n\n\nAdjusted R^2: poly(X, 10, raw = TRUE)1, poly(X, 10, raw = TRUE)2, poly(X, 10, raw = TRUE)5, poly(X, 10, raw = TRUE)7, poly(X, 10, raw = TRUE)9, poly(X, 10, raw = TRUE)10 \n \n\n\n\n\nCode\n# INSERT CODE\npar(mfrow = c(2, 2))\n\nplot(summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(summary$cp), summary$cp[which.min(summary$cp)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(summary$bic), summary$bic[which.min(summary$bic)], col = \"red\", cex = 2, pch = 20)\n\nplot(summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted R^2\", type = \"l\")\npoints(which.max(summary$adjr2), summary$adjr2[which.max(summary$adjr2)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\n\n\n\n\nWhile backward method suggests a model with more variables than the one in part c and the one using the calculated the forward method, these two actually agree with each other. Thus, I would pick three variables as it still does almost as the optimal one calculated using the backward method and the other two agree with the claim."
  },
  {
    "objectID": "Plotly.html",
    "href": "Plotly.html",
    "title": "Plotly",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "Plotly.html#r-exploration-of-data-and-cleaning",
    "href": "Plotly.html#r-exploration-of-data-and-cleaning",
    "title": "Plotly",
    "section": "R, exploration of data and cleaning",
    "text": "R, exploration of data and cleaning\n\n\nCode\nsuppressWarnings({\ndf &lt;- read_csv(\"data/chip_dataset.csv\")\n})\n\n\nThis is the head of the data:\n\n\nCode\nhead(df)\n\n\n# A tibble: 6 × 13\n  Product   Type  `Release Date` `Process Size (nm)` `TDP (W)` `Die Size (mm^2)`\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;               &lt;chr&gt;     &lt;chr&gt;            \n1 AMD Athl… CPU   6/5/00         180                 54        120              \n2 AMD Athl… CPU   10/31/00       180                 54        120              \n3 AMD Athl… CPU   8/14/00        180                 60        120              \n4 AMD Athl… CPU   10/31/00       180                 63        120              \n5 AMD Athl… CPU   10/31/00       180                 66        120              \n6 AMD Athl… CPU   10/17/00       180                 66        120              \n# ℹ 7 more variables: `Transistors (million)` &lt;chr&gt;, `Freq (GHz)` &lt;dbl&gt;,\n#   Foundry &lt;chr&gt;, Vendor &lt;chr&gt;, `FP16 GFLOPS` &lt;dbl&gt;, `FP32 GFLOPS` &lt;dbl&gt;,\n#   `FP64 GFLOPS` &lt;dbl&gt;\n\n\nData seems almost clean, we will change some of the data types and put FP GFLOPS as one column and then it will completely clean.\n\n\nCode\nsuppressWarnings({\ndf$Type &lt;- as.factor(df$Type)\ndf$`Process Size (nm)` &lt;- as.numeric(df$`Process Size (nm)`)\ndf$`TDP (W)`&lt;- as.numeric(df$`TDP (W)`)\ndf$`Die Size (mm^2)`&lt;- as.numeric(df$`Die Size (mm^2)`)\ndf$`Transistors (million)` &lt;- as.numeric(df$`Transistors (million)`)\ndf$`Release Date` &lt;- as.Date(df$`Release Date`, format = \"%m/%d/%y\")})\n\n\nThis is the new summary of the data:\n\n\nCode\nsummary(df)\n\n\n   Product           Type       Release Date        Process Size (nm)\n Length:4945        CPU:2231   Min.   :1999-03-15   Min.   :  0.0    \n Class :character   GPU:2714   1st Qu.:2007-04-01   1st Qu.: 14.0    \n Mode  :character              Median :2012-09-04   Median : 32.0    \n                               Mean   :2012-09-04   Mean   : 51.3    \n                               3rd Qu.:2018-04-11   3rd Qu.: 80.0    \n                               Max.   :2024-01-08   Max.   :250.0    \n                               NA's   :199          NA's   :58       \n    TDP (W)        Die Size (mm^2) Transistors (million)   Freq (GHz)  \n Min.   :   1.00   Min.   :   6    Min.   :     8.0      Min.   : 100  \n 1st Qu.:  35.00   1st Qu.: 114    1st Qu.:   163.5      1st Qu.: 650  \n Median :  65.00   Median : 169    Median :   950.0      Median :1400  \n Mean   :  92.54   Mean   : 213    Mean   :  4836.4      Mean   :1615  \n 3rd Qu.: 119.00   3rd Qu.: 257    3rd Qu.:  4050.0      3rd Qu.:2500  \n Max.   :2400.00   Max.   :1280    Max.   :153000.0      Max.   :4700  \n NA's   :891       NA's   :716     NA's   :806           NA's   :437   \n   Foundry             Vendor           FP16 GFLOPS      FP32 GFLOPS     \n Length:4945        Length:4945        Min.   :    10   Min.   :   12.8  \n Class :character   Class :character   1st Qu.:  1300   1st Qu.:  384.0  \n Mode  :character   Mode  :character   Median :  6136   Median : 1248.0  \n                                       Mean   : 19033   Mean   : 5403.0  \n                                       3rd Qu.: 20175   3rd Qu.: 5069.0  \n                                       Max.   :653700   Max.   :93240.0  \n                                       NA's   :4145     NA's   :3260     \n  FP64 GFLOPS      \n Min.   :    3.60  \n 1st Qu.:   59.25  \n Median :  136.35  \n Mean   : 1096.61  \n 3rd Qu.:  382.45  \n Max.   :81720.00  \n NA's   :3667      \n\n\nNow, data seems clean for our purposes. We will not discard any NA’s yet as we will do that as we plot, since we don’t want to loose any values for certain plots.\n\nR Plot 1: Do the number of Transistors over time and in terms of process size follow Moore’s Law?\n\n\nCode\nsuppressWarnings({\nfig &lt;- plot_ly(data = df, type = 'scatter', mode = 'markers',\n               x = ~`Release Date`,\n               y = ~`Transistors (million)`,\n               color = ~`Process Size (nm)`,\n               size = ~`Die Size (mm^2)`) %&gt;%\n  layout(title = 'Transistor Counts Over Time with Process Size',\n         xaxis = list(title = 'Release Date', range = c('1990-03-13','2030-01-09')),\n         yaxis = list(title = 'Transistors (million)', type = 'log'),\n         coloraxis = list(colorscale = 'inferno'),\n         hovermode = 'closest')\n\nfig\n})\n\n\n\n\n\n\n\n\nFigure 1: This plot shows a linear trend on a logarithmic scale for the number of transistors overtime. This follows Moore’s Law, which states that the number of transistors in a dense integrated circuit doubles about every two years. The color indicates that the process size has been decreasing overtime, which is also a trend that follows Moore’s Law."
  },
  {
    "objectID": "Plotly.html#plot-2-r-do-transistors-per-mm2-follow-moores-law-on-gpus-and-cpus",
    "href": "Plotly.html#plot-2-r-do-transistors-per-mm2-follow-moores-law-on-gpus-and-cpus",
    "title": "Plotly",
    "section": "Plot 2 R: Do transistors per \\(mm^2\\) follow Moore’s Law on GPUs and CPUs?",
    "text": "Plot 2 R: Do transistors per \\(mm^2\\) follow Moore’s Law on GPUs and CPUs?\n\n\nCode\nsuppressWarnings({\ndf &lt;- df %&gt;%\n  mutate(TransistorDensity = (`Transistors (million)` * 1e6) / `Die Size (mm^2)`)\n\nfig &lt;- plot_ly(data = df, x = ~`Release Date`, y = ~TransistorDensity,\n               type = 'scatter', mode = 'markers',\n               color = ~Type,\n               colors = c(\"CPU\" = \"burlywood1\", \"GPU\" = \"darkslategray2\"),\n               marker = list(\n                 size = 10,\n                 opacity = 0.7\n               )) %&gt;%\n        layout(title = 'Transistor Density Over Time',\n               xaxis = list(title = 'Release Date'),\n               yaxis = list(title = 'Transistor Density (per mm^2)', type = 'log'),\n               hovermode = 'closest',\n               legend = list(title = list(text = 'Type'))) \nfig\n})\n\n\n\n\n\n\n\n\nFigure 2: The plot shows the transistor density of CPUs and GPUs over time. Transistor density is the total number of transistors over the die size. The color indicates whether is CPU or GPU. The y axis is in log scale and we can see a linear increase in transistor density (more transistors in smaller devices) over time. This is a clear indicator of the Moore’s Law."
  },
  {
    "objectID": "Plotly.html#python-exploration-of-data-and-cleaning",
    "href": "Plotly.html#python-exploration-of-data-and-cleaning",
    "title": "Plotly",
    "section": "Python, exploration of data and cleaning",
    "text": "Python, exploration of data and cleaning\nFollows a similar procedure as before. Outputs of the cleaning process will not be printed but the code is available.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\ndf = pd.read_csv(\"data/chip_dataset.csv\")\n# print(df.info())\n\n\n\n\nCode\ndf['Type'] = df['Type'].astype('category')\ndf['Process Size (nm)'] = pd.to_numeric(df['Process Size (nm)'], errors='coerce')\ndf['TDP (W)'] = pd.to_numeric(df['TDP (W)'], errors='coerce')\ndf['Die Size (mm^2)'] = pd.to_numeric(df['Die Size (mm^2)'], errors='coerce')\ndf['Transistors (million)'] = pd.to_numeric(df['Transistors (million)'], errors='coerce')\ndf['Release Date'] = pd.to_datetime(df['Release Date'], format='%m/%d/%y')\n# df.info()\n\n\nNow, our data seems clean and we will proceed to the plots."
  },
  {
    "objectID": "Plotly.html#plot-1-python-does-computing-performance-of-gflops-fp32-follow-moores-law",
    "href": "Plotly.html#plot-1-python-does-computing-performance-of-gflops-fp32-follow-moores-law",
    "title": "Plotly",
    "section": "Plot 1 Python: Does computing performance of GFLOPS (FP32) follow Moore’s Law?",
    "text": "Plot 1 Python: Does computing performance of GFLOPS (FP32) follow Moore’s Law?\n\n\nCode\ndf_fp32 = df.dropna(subset=['FP32 GFLOPS'])\n\nmin_size, max_size = 5, 15\ndie_sizes = df_fp32['Die Size (mm^2)']\nnormalized_sizes = (die_sizes - die_sizes.min()) / (die_sizes.max() - die_sizes.min())\nnormalized_sizes = normalized_sizes * (max_size - min_size) + min_size\nnormalized_sizes = normalized_sizes.fillna(min_size)\n\ntraces = []\nfor device_type in ['CPU', 'GPU']:\n    df_filtered = df_fp32[df_fp32['Type'] == device_type]\n    sizes_for_filtered_df = normalized_sizes[df_filtered.index]\n    trace = go.Scatter(\n        x=df_filtered['Release Date'],\n        y=df_filtered['FP32 GFLOPS'],\n        mode='markers',\n        marker=dict(\n            size=sizes_for_filtered_df, \n            opacity=0.7,\n        ),\n        name=device_type\n    )\n    traces.append(trace)\n\nfig = go.Figure(\n    data=traces,\n    layout=go.Layout(\n        title='Computing Performance (FP32 GFLOPS) Over Time',\n        xaxis=dict(title='Release Year'),\n        yaxis=dict(title='FP32 GFLOPS', type='log'),\n        template='plotly_white'\n    )\n)\n\nfig.show()\n\n\n\n\n                        \n                                            \n\n\nFigure 3: This plot shows the evolution of the performance of FP32 GFLOPS overtime. The y axis is in logarithmic scale to see the increase called by the Moore’s Law. The data points are individual GPUs and CPUs and the sizes are their normalized die sizes. This plot shows roughly a linear (in the log scale) increase in performance of the GFLOPS overtime, following the Moore’s Law. However, is worth noting that this does not show the transistors as the Moore’s Law states, but rather the performance of the devices, which is still related and worth exploring."
  },
  {
    "objectID": "Plotly.html#plot-2-python-does-efficiency-gflops-per-watt-follow-moores-law",
    "href": "Plotly.html#plot-2-python-does-efficiency-gflops-per-watt-follow-moores-law",
    "title": "Plotly",
    "section": "Plot 2 Python: Does efficiency (GFLOPS per Watt) follow Moore’s Law?",
    "text": "Plot 2 Python: Does efficiency (GFLOPS per Watt) follow Moore’s Law?\n\n\nCode\ndf['Efficiency (GFLOPS/Watt)'] = df['FP32 GFLOPS'] / df['TDP (W)']\ndf['Release Year'] = df['Release Date'].dt.year\n\nprocess_size_norm = (df['Process Size (nm)'] - df['Process Size (nm)'].min()) / (df['Process Size (nm)'].max() - df['Process Size (nm)'].min())\n\ncolorscale = [[0, 'blue'], [1, 'red']]\n\ntraces = []\nfor device_type in ['CPU', 'GPU']:\n    df_filtered = df[df['Type'] == device_type]\n    trace = go.Scatter(\n        x=df_filtered['Release Year'],\n        y=df_filtered['Efficiency (GFLOPS/Watt)'],\n        mode='markers',\n        marker=dict(\n            size=10,\n            color=process_size_norm[df_filtered.index],\n            colorscale=colorscale,\n            colorbar=dict(title='Normalized Process Size'),\n            showscale=True\n        ),\n        name=device_type,\n        text=df_filtered['Process Size (nm)'] \n    )\n    traces.append(trace)\n\nfig = go.Figure(\n    data=traces,\n    layout=go.Layout(\n        title='Computational Efficiency Over Time (FP32 GFLOPS per Watt)',\n        xaxis=dict(title='Release Year'),\n        yaxis=dict(title='Efficiency (GFLOPS/Watt)', type='log'),\n        template='plotly_white'\n    )\n)\n\nfig.show()\n\n\n\n\n                        \n                                            \n\n\nFigure 4: The plot shows the efficiency (GFLOPS per Watt) of computing systems over time. Again, the y axis is in log scale to see whether the increase follows a similar patter to what is expressed in the Moore’s Law. The increase is linear (on the log scale) indicating that efficiency increase follows the Moore’s Law. This is a clear indicator of the advancement of semiconductor manufacturing and chip design."
  },
  {
    "objectID": "VoterCounts_for_polls.html",
    "href": "VoterCounts_for_polls.html",
    "title": "Reddit Results Extraction",
    "section": "",
    "text": "This section covers the methodology of how we extracted our Reddit results after all the data had been processed and labelled in previous steps. It showcases the process of aggregation to obtain the final “Reddit thermometer” results, which will be contrasted against official polls."
  },
  {
    "objectID": "VoterCounts_for_polls.html#overview",
    "href": "VoterCounts_for_polls.html#overview",
    "title": "Reddit Results Extraction",
    "section": "",
    "text": "This section covers the methodology of how we extracted our Reddit results after all the data had been processed and labelled in previous steps. It showcases the process of aggregation to obtain the final “Reddit thermometer” results, which will be contrasted against official polls."
  },
  {
    "objectID": "VoterCounts_for_polls.html#code-process",
    "href": "VoterCounts_for_polls.html#code-process",
    "title": "Reddit Results Extraction",
    "section": "Code & Process",
    "text": "Code & Process\nFor this step, we used DuckDB to read the data and perform the operations, due to its speed processing large datasets utilizing SQL queries. In the script below, you can see how we read the labelled data, aggregated by month and year, and then counted the number of time each label appears per month. This created our counts for Democrat, Republican, and No Party Preference.\n\n\nCode\nimport sys\n!{sys.executable} -m pip install duckdb\nimport duckdb\nimport pandas as pd\nfrom azureml.core import Workspace, Datastore, Dataset\n\n#workspace set up and identification\n#dummy variables for subscription_id\nsubscription_id = 'MY_SUBSCRIPTION_ID' # Removed for security concerns\nresource_group = 'project-group-11'\nworkspace_name = 'project-group-11'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\ndatastore = Datastore.get(workspace, \"workspaceblobstore\")\n\n#reading in both the comments & submissions\ndataset_submissions = Dataset.Tabular.from_parquet_files(path=(datastore, 'cleandata/submissions_labeled_Jude.parquet'))\ndataset_comments = Dataset.Tabular.from_parquet_files(path=(datastore, 'cleandata/comments_labeled_Jude.parquet'))\n\n# Convert Tabular Datasets to Pandas DataFrames\ndf_submissions = dataset_submissions.to_pandas_dataframe()\ndf_comments = dataset_comments.to_pandas_dataframe()\n\n# Use DuckDB to process data\ncon = duckdb.connect()\n\n# Query to combine and count Party occurrences by Month and Year\nquery = \"\"\"\nSELECT \n    year, \n    month, \n    Party, \n    COUNT(*) AS party_count\nFROM (\n    SELECT year, month, Party FROM df_submissions\n    UNION ALL\n    SELECT year, month, Party FROM df_comments\n)\nGROUP BY year, month, Party\nORDER BY year, month, Party;\n\"\"\"\n\n# Execute the query and get the result\nresult = con.execute(query).fetchdf()"
  },
  {
    "objectID": "VoterCounts_for_polls.html#post-processing-review",
    "href": "VoterCounts_for_polls.html#post-processing-review",
    "title": "Reddit Results Extraction",
    "section": "Post Processing Review",
    "text": "Post Processing Review\nThis is how the data looks after processing:\n\n\nCode\nfrom azureml.core import Dataset\nfrom azureml.data.datapath import DataPath\n\ntarget_path = DataPath(datastore, \"cleandata/counts_for_presentation\")\n\nresult_tabular = Dataset.Tabular.register_pandas_dataframe(\n    dataframe=result,\n    target=target_path,\n    name=\"counts_for_presentation\",\n    description=\"Party counts grouped by month and year\",\n    show_progress=True\n)\n\nresult.head()\n\n\n\n\n\n\n\n\n\nyear\nmonth\nParty\nparty_count\n\n\n\n\n0\n2022\n1\nDemocrat\n11388\n\n\n1\n2022\n1\nNo Party\n861962\n\n\n2\n2022\n1\nRepublican\n12636\n\n\n3\n2022\n2\nDemocrat\n10654\n\n\n4\n2022\n2\nNo Party\n820484"
  },
  {
    "objectID": "VoterCounts_for_polls.html#last-thoughts-before-results",
    "href": "VoterCounts_for_polls.html#last-thoughts-before-results",
    "title": "Reddit Results Extraction",
    "section": "Last Thoughts Before Results",
    "text": "Last Thoughts Before Results\nAs you can see above, our labelled dataset contains a great deal of posts categorized as “No Party” where our labelling logic could not deduce the party lean of the post in question. While this is interesting in and of itself, and was retained in the data for the purposes of training or models in during the Machine Learning section, we discarded No Party Preference posts for the purpose of analyzing our findings. These posts often fell into one of three camps: discussing the keywords in a nonpolitical way, having a neutral sentiment, or having too many keywords from both the left and right to make a determination with confidence. As such, we did not feel that they would be helpful in fully grasping the mood of those Redditors who were discussing partisan politics in a clearly identifiable way.\nNow that we have our aggregated Reddit data, we can start to plot it and draw some conclusions."
  },
  {
    "objectID": "VoterData_Transformation.html",
    "href": "VoterData_Transformation.html",
    "title": "Data Transformation",
    "section": "",
    "text": "This section covers the methodology of how we performed some simple one-hot encoding and aggregation on each row of our filtered dataset, in order to more smoothly perform the next steps."
  },
  {
    "objectID": "VoterData_Transformation.html#overview",
    "href": "VoterData_Transformation.html#overview",
    "title": "Data Transformation",
    "section": "",
    "text": "This section covers the methodology of how we performed some simple one-hot encoding and aggregation on each row of our filtered dataset, in order to more smoothly perform the next steps."
  },
  {
    "objectID": "VoterData_Transformation.html#state-of-the-data",
    "href": "VoterData_Transformation.html#state-of-the-data",
    "title": "Data Transformation",
    "section": "State of the Data",
    "text": "State of the Data\nAfter running our Job to get and clean the data from the Reddit corpus (as described in the “Initial Filtering & EDA” tab), we were still working with a significant amount of data:\n\nAbout five million submissions and just over 74 million comments, each occupying their own row in two separate data sets. We knew that all these posts had been made in our relevant time period, contained text in English, were not from our excluded subreddits, and - most importantly! - contained at least one of our targetted keywords.\n\nOur Keywords\nA quick note about our keywords! These were created by the team after several discussions on what we did or did not think would be relevant, and what attached sentiment the post surrounding the keyword might indicate. Our keywords were then broken into three categories:\n\nright-leaning, where positive sentiment containing these keywords would indicate preference towards Republicans, and negative sentiment would indicate Democrats\nleft-leaning, the opposite of right-leaning, where positive sentiment would indicate support for the Democrats, and vice versa\ngeneric, where presence of the terms themselves indicates that the posts is discussing a politically charged subject, but that may or may not have a strong correlation with a particular party\n\nThe inclusion of generic keywords was a hotly debated topic among our group, even as we progressed in our data analysis. You can see at later steps that we revisted the list of keywords in order to assign some lean to several “even more key” keywords. However, for the step below, the summation columns were calculated with the original three lists."
  },
  {
    "objectID": "VoterData_Transformation.html#one-hot-encoding-by-hand",
    "href": "VoterData_Transformation.html#one-hot-encoding-by-hand",
    "title": "Data Transformation",
    "section": "One-hot encoding (by hand!)",
    "text": "One-hot encoding (by hand!)\nNow that we had our Reddit data with relevant keywords, it was time to transform our text posts into something that could be more easily quantified, in order to allow us to label the still-massive dataset quickly. The script below was submitted as a Job in Azure ML in order to create columns containing boolean (binary) data indicating if that keyword was observed in the data. After the one hot encoding was performed concerning keywords, summation columns were added to indicate the presence of keywords from each list, and how many keywords (if any) were present. This was initially tested on our smaller r/politics dataset, then submitted as a job on the Reddit-wide scraped data. Here you can see an example of the resulting count after this was run on the (much smaller!) r/politics set:\n\nBelow is the script that performed this transformation - behold!"
  },
  {
    "objectID": "VoterData_Transformation.html#the-script",
    "href": "VoterData_Transformation.html#the-script",
    "title": "Data Transformation",
    "section": "The Script",
    "text": "The Script\n\n\n### Libraries and session start ###\n\nfrom azureml.core import Workspace, Dataset, Datastore\nimport sparknlp\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\nspark = SparkSession.builder \\\n    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12-5.0.2\") \\\n    .getOrCreate()\n\n### Initial data read ###\n\n#read data\nsubscription_id = 'TEMPLATE' #removed for security reasons\nresource_group = 'project-group-11'\nworkspace_name = 'project-group-11'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndatastore = Datastore.get(workspace, \"workspaceblobstore\")\n\n# Read data and repartition immediately if large data volumes are expected\ncomments_df = Dataset.Tabular.from_parquet_files(path=(datastore, 'sampledata/clean_comments.parquet')).to_spark_dataframe().repartition(200)\nsubmissions_df = Dataset.Tabular.from_parquet_files(path=(datastore, 'sampledata/clean_submissions.parquet')).to_spark_dataframe().repartition(200)\n\n#submissions_df.head()\n\n### Keyword futzing ###\n\n# Function to load the keywords\ndef load_keywords(file_path):\n    with open(file_path, 'r') as f:\n        return [line.strip() for line in f.readlines()]\n\n# Load all keywords, skipping lemmatization\nkeywords_left_lemmatized = load_keywords(\"Keywords_Left_Lemmatized.txt\")\nkeywords_right_lemmatized = load_keywords(\"Keywords_Right_Lemmatized.txt\")\nkeywords_non_partisan_lemmatized = load_keywords(\"Keywords_Non_Partisan_Lemmatized.txt\")\nkeywords_left_unchanged = load_keywords(\"Keywords_Left_Unchanged.txt\")\nkeywords_right_unchanged = load_keywords(\"Keywords_Right_Unchanged.txt\")\nkeywords_non_partisan_unchanged = load_keywords(\"Keywords_Non_Partisan_Unchanged.txt\")\n\n#minor tweaks\nkeywords_left_unchanged = keywords_left_unchanged + [\"Democrat\", \"Democratic\"]\nkeywords_right_unchanged = keywords_right_unchanged + [\"Republican\", \"Republicans\", \"defund the police\", \"red flag laws\", \"critical race theory\"]\n\n# Combine all keywords into a single list\nall_keywords = set(\n    keywords_left_unchanged + keywords_right_unchanged + keywords_non_partisan_unchanged +\n    keywords_left_lemmatized + keywords_right_lemmatized + keywords_non_partisan_lemmatized\n)\n\n#more minor tweaks\n#the below fields will never be found since they're part of labelling\nall_keywords.remove(\"CRT (critical race theory)\")\nall_keywords.remove(\"defund the police (criticized)\")\nall_keywords.remove(\"defund the police (advocated)\")\nall_keywords.remove(\"red flag laws (supported)\")\nall_keywords.remove(\"red flag laws (opposed)\")\n\n### The Meat - creates yes/no columns for each keyword ###\n\n#change to list so can batch this bad boy UP\nall_keywords = list(all_keywords)\n\n#function to match keywords in a batch\n#comments version\ndef process_batch_comments(keywords_batch):\n    keyword_columns = [\n        F.when(F.col('body').contains(keyword), 1).otherwise(0).alias(keyword)\n        for keyword in keywords_batch\n    ]\n    return comments_df.select('*', *keyword_columns)\n\n#submission version\ndef process_batch_submissions(keywords_batch):\n    keyword_columns = [\n        F.when(F.col('selftext').contains(keyword), 1).otherwise(0).alias(keyword)\n        for keyword in keywords_batch\n    ]\n    return submissions_df.select('*', *keyword_columns)\n\n# Split all keywords into smaller batches\n# this is why you changed it to a list!\nbatch_size = 1000\nbatches = [all_keywords[i:i + batch_size] for i in range(0, len(all_keywords), batch_size)]\n\n# Process each batch and combine results\nfor batch in batches:\n    comments_df = process_batch_comments(batch)\n    submissions_df = process_batch_submissions(batch)\n\n\n#adding summation columns\ncomments_df_weights = (\n    comments_df\n    .withColumn(\"ContainsLeft\", F.greatest(*[F.col(col) for col in left_keywords]))\n    .withColumn(\"ContainsRight\", F.greatest(*[F.col(col) for col in right_keywords]))\n    .withColumn(\"ContainsGeneric\", F.greatest(*[F.col(col) for col in generic_keywords]))\n    .withColumn(\"SumLeft\", sum(F.col(col) for col in left_keywords))\n    .withColumn(\"SumRight\", sum(F.col(col) for col in right_keywords))\n    .withColumn(\"SumGeneric\", sum(F.col(col) for col in generic_keywords))\n)\n\nsubmissions_df = submissions_df.withColumnRenamed(\"Roe v. Wade\", \"Roe_v_Wade\")\nsubmissions_df_weights = (\n    submissions_df\n    .withColumn(\"ContainsLeft\", F.greatest(*[F.col(col) for col in left_keywords]))\n    .withColumn(\"ContainsRight\", F.greatest(*[F.col(col) for col in right_keywords]))\n    .withColumn(\"ContainsGeneric\", F.greatest(*[F.col(col) for col in generic_keywords]))\n    .withColumn(\"SumLeft\", sum(F.col(col) for col in left_keywords))\n    .withColumn(\"SumRight\", sum(F.col(col) for col in right_keywords))\n    .withColumn(\"SumGeneric\", sum(F.col(col) for col in generic_keywords))\n)\n\n### Write to file ###\n\nsubmissions_df_weights.write.mode(\"overwrite\").parquet(\"azureml://datastores/workspaceblobstore/paths/cleandata/submissions_w_keywords_and_weights.parquet\")\ncomments_df_weights.write.mode(\"overwrite\").parquet(\"azureml://datastores/workspaceblobstore/paths/cleandata/comments_w_keywords_and_weights.parquet\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jorge Bris Moreno",
    "section": "",
    "text": "At the top of this page, you can find the different projects I have been working on and the relevant code. These include statistical analyses, supervised and unsupervised machine learning models, and data visualization."
  },
  {
    "objectID": "index.html#welcome-to-my-personal-website",
    "href": "index.html#welcome-to-my-personal-website",
    "title": "Jorge Bris Moreno",
    "section": "",
    "text": "At the top of this page, you can find the different projects I have been working on and the relevant code. These include statistical analyses, supervised and unsupervised machine learning models, and data visualization."
  },
  {
    "objectID": "index.html#meet-me",
    "href": "index.html#meet-me",
    "title": "Jorge Bris Moreno",
    "section": "Meet me",
    "text": "Meet me\nI am an aspiring data scientist passionate about extracting clear, actionable insights from complex datasets through machine learning and thoughtful decision-making.\nMy proficiency lies in synthesizing these insights and effectively communicating them to inform strategic choices. Growing up, I learned from my parents the importance of weighing options through a structured approach — making lists of pros and cons — which ignited my passion for data-driven decision-making. In today’s world, where vast amounts of underutilized data exist, I see every decision as an opportunity to apply rigorous data analysis.\nMy expertise in machine learning, statistical modeling, and programming has been honed through rigorous academic training and practical application across various domains, including project management, entrepreneurship, sales, and education. These experiences have not only enhanced my technical skills but also refined my soft skills, enabling me to lead projects that convert data into actionable strategies and measurable outcomes."
  },
  {
    "objectID": "index.html#growing-up",
    "href": "index.html#growing-up",
    "title": "Jorge Bris Moreno",
    "section": "Growing Up",
    "text": "Growing Up\n\n\n\n\n\nThis is my family in Madrid!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jorge Bris Moreno",
    "section": "Education",
    "text": "Education\nGeorgetown University | Washington, DC\n\nM.S in Data Science & Analytics | Jun 2023 - May 2025\nI am currently studying a MS in Data Science and Analytics at Georgetown University.\nFranklin & Marshall College | Lancaster, PA\n\nB.A in Mathematics and Business | Aug 2019 - May 2023\nI double majored in Mathematics and Business at Franklin & Marshall College. During that time, I played varsity tennis, rugby, was part of the Google Student Developer Club, and was a member of Ware Parliament.\n\nHobbies\nI love playing sports, especially tennis and rugby. However, I love any other outdoor activity such as hiking, skiing, snowboarding, surfing, and many more! I strongly believe that sports are a great way to not only learn about yourself but be more disciplined and develop a strong work ethic.\nI also love photography. I have been taking pictures for a long time now and I love it. I love capturing moments and being able to look back at them. Photography allows you to look at the world from a different perspective and appreciate the little things in life. (You can see some of my work along with the awesome photographer Raluca Rilla’s pics at @lascallesde in instagram!)\n\n\nRelevant Experience\nResearch Assistant | Massive Data Institute\n\nSep 2024 - Present\nData Analyst Intern | Alliance of Community Health Plans\n\nMay 2024 - Aug 2024\nTeaching Assistant | Network Analytics at Georgetown U.\n\nMay 2024 - Present\nTeaching Assistant | Data Storytelling at Georgetown U.\n\nJan 2024 - May 2024\nStatistician | PhD student’s thesis collaborator\n\nNov 2022 - Nov 2023\nClient Engagement & Resource Assistant | ASSETS\n\nSept 2022 - May 2023\nProject Manager Intern | Miller Mats\n\nMay 2022 - Aug 2022"
  },
  {
    "objectID": "index.html#hobbies",
    "href": "index.html#hobbies",
    "title": "Jorge Bris Moreno",
    "section": "Hobbies",
    "text": "Hobbies\nI love playing sports, especially tennis and rugby. However, I love any other outdoor activity such as hiking, skiing, snowboarding, surfing, and many more! I strongly believe that sports are a great way to not only learn about yourself but be more disciplined and develop a strong work ethic.\nI also love photography. I have been taking pictures for a long time now and I love it. I love capturing moments and being able to look back at them. Photography allows you to look at the world from a different perspective and appreciate the little things in life. (You can see some of my work along with the awesome photographer Raluca Rilla’s pics at @lascallesde in instagram!)"
  },
  {
    "objectID": "index.html#relevant-experience",
    "href": "index.html#relevant-experience",
    "title": "Jorge Bris Moreno",
    "section": "Relevant Experience",
    "text": "Relevant Experience\nResearch Assistant | Massive Data Institute\n\nSep 2024 - Present\nData Analyst Intern | Alliance of Community Health Plans\n\nMay 2024 - Aug 2024\nTeaching Assistant | Network Analytics at Georgetown U.\n\nMay 2024 - Present\nTeaching Assistant | Data Storytelling at Georgetown U.\n\nJan 2024 - May 2024\nStatistician | PhD student’s thesis collaborator\n\nNov 2022 - Nov 2023\nClient Engagement & Resource Assistant | ASSETS\n\nSept 2022 - May 2023\nProject Manager Intern | Miller Mats\n\nMay 2022 - Aug 2022"
  },
  {
    "objectID": "VoteSignals.html",
    "href": "VoteSignals.html",
    "title": "Project Overview & Introduction",
    "section": "",
    "text": "A comparison of aggregate polls, election results, and Reddit data to try to better contextualize the 2022 midterm elections."
  },
  {
    "objectID": "VoteSignals.html#footnotes",
    "href": "VoteSignals.html#footnotes",
    "title": "Project Overview & Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“2022 Election Results.” Politico. Accessed Dec 8 2024↩︎\nEnten, Harry. “A Republican wave in the House is still quite possible.” CNN, October 16 2022. Accessed Dec 8 2024↩︎\n“Latest Polls - Generic Ballot 2022.” FiveThirtyEight. Accessed Dec 2 2024↩︎\nBlake, Aaron. “How bad the 2022 election was for the GOP, historically speaking.” The Washington Post, November 10 2022. Accessed Dec 8 2024.↩︎\nPerkins, Tom. “How Michigan Democrats took control for the first time in decades.” The Guardian, November 17 2022. Accessed Dec 8 2024.↩︎\nRutenberg, Jim; Bensinger, Ken; and Eder, Steve. “The ‘Red Wave’ Washout: How Skewed Polls Fed a False Election Narrative.” The New York Times, Dec 31 2022. Accessed Dec 4 2024↩︎\nNarea, Nicole. “The guy who got the midterms right explains what the media got wrong.” Vox Media, Nov 27 2022. Accessed Dec 8 2024↩︎\nThompson, Derek. “Democrats Might Have Pulled Off the Biggest Midterm Shock in Decades.” The Atlantic, Nov 9 2022 Accessed Dec 8 2024↩︎"
  },
  {
    "objectID": "data_viz_ind.html",
    "href": "data_viz_ind.html",
    "title": "Global study of Gender Inequality in Education Access",
    "section": "",
    "text": "Abstract\nEducation is one of the key drivers in the development of society. In the global pursuit of gender equality, education stands out as both a fundamental right and a pivotal area of progress and challenge. In this article we aim to explore gender inequality in education access across the globe.\nUsing data obtained from the World Bank, our team analyzed the relationship between education attainment, education enrollment, gender, and other economic factors such as Gross Domestic Product (GDP), the Gross National Income (GNI) index, and population size. The data utilized is from the years 2010 - 2018 (with some historical data dating back to 1970). This project focuses on a larger global scale and then provides interactivity for readers to dive into specific regions, countries, and demographics in order to identify trends and patterns. We hope that you find this dataset as interesting and expository as we did - from regions with the highest diparity in education enrollment based on gender to the linkage of other factors that may be contributing to gendered disparity. It encouraged us to ask questions about the forces at play (economic, political, social) affecting women’s access to education and thus to a better life. We encourage you to investigate these revelations, which illuminate noteworthy discrepancies and their wider consequences for realizing gender parity in education.\n\n\nIntroduction\nIn the global pursuit of gender equality, education stands out as both a fundamental right and a pivotal area of progress and challenge [1]. The accessibility of education for women is closely associated with several economic metrics, such as GDP per capita, GNI per capita, and the fiscal priorities of the government [2].\nThroughout history, women have faced significant barriers to equitable access not only to education and the fundamental freedom to control their own lives. These challenges have both directly and indirectly fueled the ongoing global gaps in educational attainment. The Universal Declaration of Human Rights declares that everyone has the right to education is a bold and basic right [3]. This acknowledgement acts as a call to action to overcome the remaining obstacles that impede women from enjoying this right to the fullest extent possible, as well as a reminder of the progress that has been done. By examining these disparities and understanding their deep-rooted causes, we can better advocate for and implement strategies that ensure all women have the opportunity to learn, grow, and participate fully in society.\nOur exploration will delve into the current state of this divide, examining the myriad factors — from economic to socio-political — that perpetuate it. By understanding these underlying causes, we can more effectively devise strategies to overcome these barriers and ensure that women and girls everywhere have equal opportunities to learn and succeed. This analysis will not only shed light on the challenges but also highlight the areas where progress has been made, offering a nuanced perspective on the global journey towards educational equity.\nWe will address six critical questions:\n\nHow does female enrollment in education differ between different regions? How do these trends continue over time?\nWhat role does education play in economic growth and development?\nTo what extent is gender equality present in educational access and outcomes?\nHow do education trends reflect labor employment trends, especially regarding gender?\nHow much do governments prioritize resource allocation to education?\n\nThis analysis will not only shed light on the challenges but also highlight the areas where progress has been made, offering a nuanced perspective on the global journey towards educational equity.\n\n\nVisualizing Enrollment Disparities\nHaving a sense of global picture is important to understand the context of the data. We will start by looking at the Gross Enrollment Ratio per education level in the world. Gross enrollment ratio is calculated by dividing the number of students enrolled in a specific education category regardless of age by the population of the age group which officially corresponds to education category, and multiplying by 100. This will allow us not only to identify different areas where there are discrepancies or the gender gap in education is more pronounced, but also what data was available to the World Bank. Furthermore, we can also contrast globally different levels of education.\n\nPrimary Ed.Secondary Ed.Tertiary Ed.\n\n\n\n\n\n\n\nThese plots show the Gross Enrollment Ratio per education level around the world. Only the values collected by the World Bank are being displayed. Countries that did not share information do not display any values. Note that because of the way data is collected, small discrepancies in the reported age of children may occasionally cause net enrollment rates to exceed 100 percent by the World Bank. However, this allows us to visualize high vs low enrollment and contrast regions of interest.\n\n\n\n\n\n\n\n\n\n\nThese plots show the Gross Enrollment Ratio per education level around the world. Only the values collected by the World Bank are being displayed. Countries that did not share information do not display any values. Note that because of the way data is collected, small discrepancies in the reported age of children may occasionally cause net enrollment rates to exceed 100 percent by the World Bank. However, this allows us to visualize high vs low enrollment and contrast regions of interest.\n\n\n\n\n\n\n\n\n\n\nThese plots show the Gross Enrollment Ratio per education level around the world. Only the values collected by the World Bank are being displayed. Countries that did not share information do not display any values. Note that because of the way data is collected, small discrepancies in the reported age of children may occasionally cause net enrollment rates to exceed 100 percent by the World Bank. However, this allows us to visualize high vs low enrollment and contrast regions of interest.\n\n\n\n\n\n\nFrom these plots, we can see that globally, the greater education level, the lower the enrollment rate. This is expected as the higher the education level, the more specialized and innaccessible the education becomes and the less people are likely to enroll in it. Additionally, in most countries, the primary education level is mandatory by law. However, there seems to be a greater disparity in certain areas of the world. For the secondary level, while there is not too much data available in this region, Sub-Saharan Africa seems to have the lowest enrollment rates with rates as low as 28%. This is concerning as in many countries, secondary education is also mandatory. For the tertiary level, the same region seems to have extremely low enrollment rates, along with Latin America and the Caribbean, East Asia and the Pacific, and the Middle East and North Africa. However, there are outliers in these countries and they can be identified and studied more in depth in order to identify the reasons behind the high enrollment rates and try to mimic them in other nations. Additionally, Europe and Central Asia along with North America seem to have the highest female enrollment rates in the world, making them a good example to follow.\n\n\nGlobal Regions: A Tapestry of Economic and Educational Landscapes\nThe idea of education as a global source of hope is frequently associated with economic success. However, the way this connection appears varies greatly throughout the world, greatly impacted by local economic conditions. The World Bank’s seven-region model, which divides the world into Europe & Central Asia, Latin America & Caribbean, Middle East & North Africa, East Asia & Pacific, Sub-Saharan Africa, and North America, offers a structural lens through which to view these variations. The tale of educational access varies by region, depending on the economic context [4].\nWe can see this division here:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a map of the classification of regions provided by the World Bank. Each country is colored by the region they are in, the numbers are insicators for each region, and you can zoom in and out with the + and - bottons, as well as move around the map by clicking and dragging.\n\n\n\nThis map displays the categorization of countries into regions. Geographical proximity influences economic and social dynamics, e.g. climate conditions, natural resources, and trade patterns. These regions are also divided based on common developmental challenges, region often being a proxy for poverty, inequality, infrastructure needs, and access to basic services. These groupings are a helpful framework for comparative analysis by policymakers, researchers, and development practitioners to identify best practices, learn from each other’s experiences, and design more targeted interventions. Similarly, we used these region categories to help us understand the education data, and in the future to provide recommendations via regional classification for the formulation of regional policies and strategies.\nWhile exploring all of the education data, this image seemed necessary to us first as researchers, and next to our audience to quickly acclimate them for a deeper investigation.\n\n\nTeasing out Enrollment by Region\nTo begin our analysis, we will be looking at different regions usually studied by the World Bank and represented above. While this graph may not have a great impact in an analysis, we can see how higher level of education bubbles are smaller, specially the Tertiary level:\nTo begin our analysis, we will be looking at different regions usually studied by global organizations like the World Bank. We will be focusing on the following regions: South Asia, Europe & Central Asia, Latin America & Caribbean, Middle East & North Africa, East Asia & Pacific, Sub-Saharan Africa, and North America. The objective is to visualize the differences in women’s enrollment at different education levels across the world at a global scale and identify any trends or patterns that may arise. This will also allow us to consider if some regions are worth exploring in more detail than others in our analysis.\n\n\n\n\n\nThis chart shows the Female Gross Enrollment Ratios by Region for the year 2014. These region delimitations have been chosen from what the World Bank utilize for their studies. You can select and deselect regions to focus on different regions, each bubble inside the region represents an education level, and the values inside are the female gross enrollment ratio for 2014. The sizes of the bubbles correlate to the values. Note that because of the way data is collected, small discrepancies in the reported age of children may occasionally cause net enrollment rates to exceed 100 percent by the World Bank.\n\n\n\nFrom the chart above, not surprisingly, we can see that for all regions, the higher the education level, the lower the enrollment ratio. As highlighted, the regions with the largest disparity in higher education enrollment are significant. This is expected as the higher the education level, the more specialized and less accessible it becomes - we can see that these regions have higher enrollment ratios than others. Specifically, the regions with the lowest enrollment, South Asia and Sub-Saharan Africa, are followed by Middle East & North Africa, Latin America & Caribbean, and East Asia & Pacific. However, the scope of regions limits our ability to make a thoughtful comparison, as we do not know if data has not been collected by the World Bank in some countries.\nThis is important information to consider as we move forward with our analysis. One can think that it may be due to economic reasons, social and ideological reasons, or even political reasons. However, this problem has been present for decades and instead of taking assumptions to answer this question, we should identify the key reasons for this disparity and address it accordingly.\n\n\nLabor Force with Education Levels\nThe World Bank Data Catalog includes data about the proportion of males and females in the labor forces with advanced education and basic education. The proportion of high-income, upper middle income, lower middle income, and low-income countries is uneven in the bar plot below. High income countries reported the most labor statistics while low-income countries reported the least. In the high income category, over 40 countries reported these statistics. Although capturing data about female educational attainment is a priority for global development, only four low income countries reported labor statistics based on education level between 2010 and 2013. These four countries include Ethiopia, Madagascar, Mozambique, Uganda. The low income group represents the recent state of these four countries rather than the total of 26 low income countries classified by the World Bank.\n\n\n\nThe corresponding metric to female representation in education is understanding female labor force composition. One indicator of equality is the percentage of women in the work force, and their level of education. This plot is broken into high, medium, and low-income groups for countries. The World Bank´s classification of income countries means that most countries fall above low income, and for those nations it is also more challenging to collect data. Most countries, according to the World Bank´s classification are considered ´high income.´ It is important to understand this underlying distribution and classification of nations in order to view other color encodings throughout the remainder of this analysis.\n\n\nLabor force participation is an important statistic in assessing the progress for equal opporunity for women. For each income group, the proportion of males and females was captured for different levels of education, including basic education and advanced education. The proportion for each gender and education level was calculated with respect to the percentage of total labor forces participation for each gender. The countries were grouped by income group and the average labor percentage was calculated for each year.\nFrom 2010 to 2013, in high income countries, the proportion of male labor with advanced educaiton is above 80% while the proportion for female labor with the same education level is below 80%. The lower middle income countries showed the largest gap between female labor with advanced education and male labor, which was higher. For male and females labor with basic education, there are more males than females in the labor force. Interestingly, the gap between female labor and male labor is larger at the basic education level.\nSurprisingly, the “low income” group, which includes Ethiopia, Madagascar, Mozambique, Uganda, showed high proportions of female labor with basic and advanced education levels. According to the World Bank, “In many countries, enrollment in tertiary education slightly favors young women, however, better learning outcomes are not translating into better work and life outcomes for women. There is a large gender gap in labor force participation rates globally.” These findings suggest that reviewing overall labor force participation may begin the analysis of the recent progress in female education attainment. However, further research and more questions about different types of female labor can bring a better focus and attention to this developmental goal.\n\n\n\nComparing the percent of females in the labor force with basic and advanced education tells a more detailed story of what demographic (advanced or non-advanced education) is part of the work force composition.\n\n\n\n\nEconomic Influences on Female Access to Education\nThe World Bank assigns the world’s economies to four income groups, low income, lower-middle, upper-middle, and high income. The classifications are updated each year on July 1 and are based on the GNI per capita of the previous year (2021). GNI measures are expressed in United States dollars (USD), and are determined using conversion factors derived according to the Atlas method (The Atlas method smooths exchange rate fluctuations using a three-year moving average, price-adjusted conversion factor. The USD estimate of GNI per capita is derived by applying the Atlas conversion factor to estimates measured in local currency units (LCU)).\n\nimport { slider } from \"@jashkenas/inputs\"\nimport { vl } from \"@vega/vega-lite-api\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction parseCSV(csvString) {\n  return d3.csvParse(csvString);\n}\n\n\n\n\n\n\n\nd3 = require(\"d3\", \"d3-svg-legend\")\n\n// Define and load your datasets\ndat = await FileAttachment('data/parallel_coordinates_df.csv')\n .text()\n .then(parseCSV)\n .then(data =&gt; data.map(d =&gt; ({\n    'Country': d['Country'], \n    'Region': d['Region'], \n    'Female enrolment ratio': +d['Female enrolment ratio'],\n    'GDP per capita (USD)': +d['GDP per capita (USD)'],\n    'GNI per capita (USD)': +d['GNI per capita (USD)'],\n    'Income group': d['Income group']\n  })));\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof numTicks = slider({\n  min: 0,\n  max: 1,\n  value: 2,\n  step: 1,\n  title: \"Number of vertical ticks\"\n})\n\n\n\n\n\n\n\nchart_parallel = {\n  const height = 350;\n  const labelMap = {\n    gdp_per_capita: \"GDP per Capita\",\n    gni_per_capita: \"GNI per Capita\",\n    enrolment_ratio_female: \"Female Enrolment Ratio\",\n    income_group: \"Income Group\",\n  };\n\n  \n  const hover = vl\n    .selectSingle()\n    .on(\"mouseover\")\n    .nearest(true)\n    .empty(\"none\");\n\n  const base = vl.markRule({ color: \"#ccc\" }).encode(\n  vl.x().fieldN(\"key\").sort(['GDP per capita (USD)', 'Female enrolment ratio', 'GNI per capita (USD)']).title(d =&gt; labelMap[d] || d),  // Explicitly sort by desired order\n  vl.detail().count()\n);\n\nconst line = base.markLine().encode(\n  vl.color().fieldN(\"Income group\").scale({\n    domain: ['Low income', 'Lower middle income', 'Upper middle income', 'High income'],  // Adjust these categories based on your actual data\n    range: ['#ff4500', '#ffa205', '#d40637', '#5f0922']\n  }),\n\n\n  vl.detail().fieldN(\"index\"),\n  vl.opacity().if(hover, vl.value(1)).value(0.3),\n  vl.y().fieldQ(\"norm_val\").axis(null),\n  vl.tooltip([\n    'Country',\n    'Region',\n    'Female enrolment ratio', \n    'GDP per capita (USD)', \n    'GNI per capita (USD)', \n    'Income group'\n  ])\n);\n  const points = line.markCircle()\n    .select(hover)\n    .encode(vl.size().if(hover, vl.value(50)).value(5));\n\n  // Generates a spec to show tick values at an specific value of y0\n  const tick = y0 =&gt;\n    vl.layer(\n        base.markText({ style: \"label\" }).encode(vl.text().max(\"max\")),\n        base.markTick({ style: \"tick\", size: 8, color: \"#ccc\" })\n      )\n      .encode(vl.y().value(y0));\n\n  // Create an array with *numTicks* ticks\n  const ticks = Array.from({ length: numTicks })\n    .map((_, i) =&gt; tick((height / (numTicks - 1)) * i));\n\nreturn vl\n    .layer(base, line, points, ...ticks)\n    .data(dat)\n    .transform(\n      vl.filter(attribs.map(a =&gt; `datum[\"${a}\"] != null`).join(\" && \")),\n      vl.window(vl.count().as(\"index\")),\n      vl.fold(attribs),\n      vl.groupby(\"key\").joinaggregate(vl.min(\"value\").as(\"min\"), vl.max(\"value\").as(\"max\")),\n      vl.calculate(\"(datum.value - datum.min) / (datum.max - datum.min)\").as(\"norm_val\"),\n      vl.calculate(\"(datum.min + datum.max) / 2\").as(\"mid\")\n    )\n    .config({\n      axisX: { domain: false, labelAngle: 0, tickColor: \"#ccc\", title: null },\n      view: { stroke: null },\n      style: {\n        label: { baseline: \"middle\", align: \"right\", dx: -5 },\n        tick: { orient: \"horizontal\" }\n      }\n    })\n    .width(width - 100)\n    .height(height)\n    .title({\n      text: 'Economic and Educational Indicators by Income Group (2014)',\n      fontSize: 12,\n      fontWeight: 'bold',\n      anchor: 'middle',\n      color: 'black',\n      font: 'sans-serif',\n      offset: 40\n    })\n    .render();\n\n\n}\n\n\n\n\n\n\n\nattribs = Object.keys(dat[0]).filter(a =&gt; !isNaN(dat[0][a])) //Find the attributes that are numbers\n\n\n\n\n\n\nA clear indicator between a higher female enrollment rate is an increased GDP per capital and a higher GNI per capita. The socio-economic implications of this are clear: female education enrollment is linked to a higher female enrollment rate. This plot also visually helps to highlight standout data, who have an inverted relationship between enrollment and economic indicators: a higher GDP and GNI per capita and still have a lower female enrollment ratio. The inverted “V” of Switzerland and Norway at the top of the plot show one of the few flipped relationships between GDP & GNI - either due to the relative wealth of those nations, or the pedigree of their education systems (perhaps their students don´t require tertiary education). Income is included by color to show that high income countries do have a higher female enrollment ratio, although even for lower middle and low income countries, the GDP and GNI per capita ratios show that the female enrollment ratio can outstrip economic information.\n\n\nRegional Female Enrollment 1970 - 2013\nIt is also important to understand the historical changes in female enrollment of different regions. This would allow us to identify which regions are making an effort to support female education and which regions are not. Trends often emerge over longer periods in time. For this, we introduce the plot below - it shows the country count of each region (as it may be important to understand the data distribution and may be linked to agreements between them on certain aspects linked to education). It also shows the Gross Enrollment Ratio for females per education level overtime.\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n  \n  \n\n\nHovering over the bars will highlight the change overtime of Gross Enrollment Ration for women in all education levels\n\n\nOne of the primary reported metrics when education is covered in press is that the female enrollment rate for higher education (tertiary, in most countries) has surpassed the enrollment rate of men. In order to visualize the comparative rate of female enrollment over time for each education demographic, our interactive plot shows a breakdown of this (happily) increasing ratio of female enrollment.\nSome of the most interesting findings were that the increase in tertiary female enrollment over time supports the coverage that there are more women enrolling in higher education. The other, more surprising finding was that some regions relative enrollment were so low when seen on the global scale. For instance South Asia’s tertiary gross enrollment ratio is in the low twenties. The other humorous finding was that North America’s enrollment was consistently at 100% for primary school - a straight line - likely due to the mandatory enrollment rules for education.\n\n\nGovernment Spending (Dollar per capita, Purchasing Power Parity)\n\nviewof metric = Inputs.radio(new Map([[\"Absolute\", \"absolute\"], [\"Relative(%)\", \"relative\"]]), {value: \"absolute\", label: \"Change\"})\n\n\n\n\n\n\n\nviewof selectedDataset = Inputs.select(['Primary', 'Secondary', 'Tertiary'], {label: 'Dataset: ', value: 'Primary'})\n\n\n\n\n\n\n\nchart_gov_exp = async () =&gt; {\n  // Load primary data\n  const primaryData = await FileAttachment(\"data/primary_1.csv\").text().then(parseCSV);\n\n  // Load secondary data\n  const secondaryData = await FileAttachment(\"data/secondary_1.csv\").text().then(parseCSV);\n\n  // Load tertiary data\n  const tertiaryData = await FileAttachment(\"data/tertiary_1.csv\").text().then(parseCSV);\n\n  const selected = await selectedDataset;\n\n  let data;\n  if (selected === 'Primary') {\n    data = primaryData;\n  } else if (selected === 'Secondary') {\n    data = secondaryData;\n  } else if (selected === 'Tertiary') {\n    data = tertiaryData;\n  }\n  \n  data = data.map(d =&gt; ({\n    country_name: d.country_name,\n    value: metric === 'absolute' ? d.year2 - d.year1 : (d.year2 - d.year1) / d.year1\n  }));\n\n  const barHeight = 25;\n  const marginTop = 50; \n  const marginRight = 70;\n  const marginBottom = 50; \n  const marginLeft = 120;\n  const height = Math.ceil((data.length + 0.1) * barHeight) + marginTop + marginBottom;\n\n  const x = d3.scaleLinear()\n    .domain(d3.extent(data, d =&gt; d.value))\n    .rangeRound([marginLeft, width - marginRight]);\n\n  const y = d3.scaleBand()\n    .domain(data.map(d =&gt; d.country_name))\n    .rangeRound([marginTop, height - marginBottom])\n    .padding(0.1);\n\n  const format = d3.format(metric === 'absolute' ? '+,d' : '+.1%');\n  const tickFormat = metric === 'absolute' ? d3.formatPrefix('+.1', 1e1) : d3.format('+.0%');\n\n  const svg = d3.create('svg')\n    .attr('viewBox', [0, 0, width, height])\n    .attr('style', 'max-width: 100%; height: auto; font-family: sans-serif; font-size: 10px;');\n\n  // Chart Title\n  svg.append('text')\n    .attr('x', width / 2)\n    .attr('y', 13)\n    .attr('text-anchor', 'middle')\n    .style('font-size', '16px')\n    .style('font-weight', 'bold')\n    .text('Government Spending for Education (2014 - 2018)');\n\n  // X-axis Label\n  svg.append('text')\n    .attr('x', width / 2)\n    .attr('y', height - 15)\n    .attr('text-anchor', 'middle')\n    .style('font-size', '12px')\n    .text('Dollar / Per Capita (Purchasing Power Parity)');\n\n  // Y-axis Label\n  svg.append('text')\n    .attr('transform', `rotate(-90)`)\n    .attr('x', -(height / 2))\n    .attr('y', marginLeft - 40)\n    .attr('text-anchor', 'middle')\n    .style('font-size', '12px')\n    .text('Country');\n\n  svg.append('g')\n    .selectAll('rect')\n    .data(data)\n    .join('rect')\n    .attr('fill', d =&gt; d.value &gt; 0 ? '#3d6469' : '#d40637')\n    .attr('x', d =&gt; x(Math.min(d.value, 0)))\n    .attr('y', d =&gt; y(d.country_name))\n    .attr('width', d =&gt; Math.abs(x(d.value) - x(0)))\n    .attr('height', y.bandwidth());\n\n  svg.append('g')\n    .attr('font-family', 'sans-serif') \n    .attr('font-size', 10)\n    .selectAll('text')\n    .data(data)\n    .join('text')\n    .attr('text-anchor', d =&gt; d.value &lt; 0 ? 'end' : 'start')\n    .attr('x', d =&gt; x(d.value) + Math.sign(d.value) * 4)\n    .attr('y', d =&gt; y(d.country_name) + y.bandwidth() / 2)\n    .attr('dy', '0.35em')\n    .text(d =&gt; format(d.value));\n\n  svg.append('g')\n    .attr('transform', `translate(0,${marginTop})`)\n    .call(d3.axisTop(x).ticks(width / 80).tickFormat(tickFormat))\n    .call(g =&gt; g.selectAll('.tick line').clone()\n      .attr('y2', height - marginTop - marginBottom)\n      .attr('stroke-opacity', 0.1))\n    .call(g =&gt; g.select('.domain').remove());\n\n  svg.append('g')\n    .attr('transform', `translate(${x(0)},0)`)\n    .call(d3.axisLeft(y).tickSize(0).tickPadding(6))\n    .call(g =&gt; g.selectAll('.tick text').filter((d, i) =&gt; data[i].value &lt; 0)\n      .attr('text-anchor', 'start')\n      .attr('x', 6));\n\n  return svg.node();\n}\n\nchart_gov_exp();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThroughout this narrative, we looked at the data through the aggregates of regions and in terms of economic classes, however it is also important to understand this change on a granular scale and see if there any change within the Government spending per capita in Purchasing Power Parity between 2014-2018, as an extension to see if there has been any change prior to COVID-19´s emergence. This plot shows the link between national spending on education (per capita) and level of education (primary, secondary, tertiary). We see a lot of interesting patterns here how few countries such as Mexico has greater change in government spending in terms of Primary education than Secondary and Tertiary. On the other hand, Iceland has lowest relative change in government spending per capita for Primary than the others, but Chile has lowest change in Tertiary than others. In constrast, if we look at the OECD average overall, which measures the aggregate change for all its member nations, we see there is constant change across primary, secondary and tertiary. This highlights the importance of looking at countries on a granular level. In terms of the plot, the relative view in dollars per capita, it is quickly highlighted which nations spend per education demographic relative to their peers. The absolute view shows the actual spending per person, which can be a more useful metric for understanding the actual spending on education. All of these metrics are important for understanding the economic and social implications of education spending, and are standardized to be shown in USD.\n\n\nConclusion\nThe data presented in this analysis is a small part of the global education data available. The data is collected by the World Bank and other global organizations to help understand the state of education in the world. The data and the team´s subsequent analysis can be used by policymakers, researchers, and development practitioners to identify best practices, learn from each other’s experiences, and design more targeted interventions. The data is also used to monitor progress towards the Sustainable Development Goals (SDGs) and other global development goals.\n\n\nSources\n[1] Nussbaum, M. C. (2004). Women’s education: A global challenge. Signs: Journal of Women in Culture and Society, 29(2), 325-355.\n[2] Adams, D. K. (2002). Education and national development: Priorities, policies, and planning (Vol. 1). Manila Philippine: Asian Development Bank.\n[3] United Nations. (n.d.). Universal Declaration of Human Rights. Retrieved from [https://www.un.org/en/about-us/universal-declaration-of-human-rights#:~:text=Article%2026,on%20the%20basis%20of%20merit]\n[4] Our World in Data. (n.d.). Retrieved from [https://ourworldindata.org/grapher/world-regions-according-to-the-world-bank?tab=table]"
  },
  {
    "objectID": "Computer_vision.html",
    "href": "Computer_vision.html",
    "title": "HW-3: Pre-trained computer vision models",
    "section": "",
    "text": "HW-3: Pre-trained computer vision models\nAuthor: J. Hickman\nOverview:\n\nIn deep learning we don’t always have to train our neural networks from scratch\nOften “pre-trained models” exist which we can “pull of the shelf” and use as part of a data processing pipeline.\nIn this assignment, we will use the popular “You only look once” (YOLO) deep learning algorithm to do object detection.\n\nhttps://pytorch.org/hub/ultralytics_yolov5/\n\nAssume you are working for a company interested in traffic patterns and driver behavior\n\nFor example, a car-insurance company, a traffic data analytics firm, or a self driving car start-up\n\nThis company has “dash-camera” footage from various cars in their “fleet”\nAs an initial proof of concept they want you to write a code to track the locations of other vehicles from the recordings\nThey also want you to analyze the time-dependence of traffic patterns found in the videos.\n\nSubmission:\n\nYou need to upload ONE document to Canvas when you are done\n\n\nA PDF (or HTML) of the completed form of the “HW-3.ipynb” document\n\n\nThe final uploaded version should NOT have any code-errors present\nAll outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\nNote:\n\nThere are many ways to do this assignment and the methods below are guidelines not rules.\nIf you find more efficient ways to complete the objectives then feel free to do it however you want.\nIMPORTANT: START SMALL THEN SCALE UP\n\nGet everything working for a simple case with a few images, once everything seems to be working correctly then run it on the entire data set\n\n\nData source:\n\nDash cam footage: https://www.youtube.com/watch?v=9qy4lExIetk\nWebsite to download youtube videos: https://en.y2mate.is/67/\n\n\nImport\n\n# Libraries\nimport torch\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport shutil \nimport os\nimport cv2\n\nprint(torch.__version__)\n\n2.2.1\n\n\n\nPart-3.0\nOpen the file “video-res-2.mp4” and watch a few minutes to get familiar with the content\n\n\nPart-3.1: Pre-processing\n\nThe video is 2hr8min55s which is 26060+8*60+55=7735 seconds\nThe frame rate for the video is roughly 30 frames per second\nSo there are around 7735*30=232050 frames\nWrite code to read and convert the video “video-res-2.mp4” into an ORDERED set of images\nInclude a parameter to save frames every N seconds\nDon’t save every frame, at first save frames every 50 seconds, that will be around 155 images\nOnce everything is working, save frames every 10 seconds and re-run the code for the final product.\nSave the images to a folder “frames” with the convention\n\n“frames/00010.jpg” for the frame 10 seconds into the video\n“frames/00020.jpg” for the frame 20 seconds into the video and so on\n\n\n\n# For extra info\n\nvideo_path = 'video-res-2.mp4'\nvid = cv2.VideoCapture(video_path)\n\n# get video info\nfps = vid.get(cv2.CAP_PROP_FPS)\ntotal_frames = vid.get(cv2.CAP_PROP_FRAME_COUNT)\nduration = total_frames / fps\nprint(f'fps: {fps}, total_frames: {total_frames}, duration: {duration}')\n\nfps: 29.97, total_frames: 231828.0, duration: 7735.335335335336\n\n\n\n#INSERT CODE HERE \n# To obtain same output as the one in the question\nprint_intervals = [0, 50000, 100000, 150000, 200000]\nprint(vid.__class__)\n\nfor interval in print_intervals:\n    print(interval)\n\nvid.release()\n\n&lt;class 'cv2.VideoCapture'&gt;\n0\n50000\n100000\n150000\n200000\n\n\n\n\nPart-3.2: Explore the model\nBefore processing the video data, we want to make sure we can get YOLO working correctly for a few images\n\n#INSERT CODE TO LOAD THE YOLO MODEL USING PYTORCH\n\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n\nUsing cache found in /Users/jbm/.cache/torch/hub/ultralytics_yolov5_master\nYOLOv5 🚀 2024-3-16 Python-3.10.12 torch-2.2.1 CPU\n\nFusing layers... \nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\nAdding AutoShape... \n\n\n\n# Function -&gt; extract frames from a video\ndef get_frames(video_path, times, target_folder):\n\n    # Load the video\n    vid = cv2.VideoCapture(video_path)\n    \n    # Create frames list\n    frames = []\n\n    for t in times:\n        frame_number = t*1000\n\n        # Set the frame position based on the time (in seconds)\n        vid.set(cv2.CAP_PROP_POS_MSEC, frame_number)\n        \n        # Read the frame\n        success, image = vid.read()\n        \n        if success:\n            # Set each file's name\n            filename = f\"{str(int(t)).zfill(5)}.jpg\"\n            # Set the save path\n            save_path = os.path.join(target_folder, filename)\n\n            # Save frame as JPEG\n            cv2.imwrite(save_path, image)\n            # Save images in the right path\n            frames.append(save_path)\n    \n    # Release the video capture object\n    vid.release()\n    # return the frames (For checking the progress and that we are saving them all correctly)\n    return frames\n\n\ntimes = [50, 200]\ntarget_folder = 'frames'\n\nobtained_frames_paths = get_frames(video_path, times, target_folder)\nobtained_frames_paths\n\n['frames/00050.jpg', 'frames/00200.jpg']\n\n\n\n# AT FIRST USE THE FOLLOWING IMAGES AS A TEST CASE\nimages=['frames/00050.jpg','frames/00200.jpg']  \n\n\n# INSERT CODE TO EVALUATE THE YOLO MODEL ON THESE IMAGES\nresults = model(images)\nresults.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# WHEN WORKING IN PYTHON WITH AN OBJECT OF AN UN-FAMILIAR CLASS IT IS \n# VERY USEFUL TO FIGURE OUT THE CLASSES ATTRIBUTES AND METHOD\nprint(type(results))\n\n&lt;class 'models.common.Detections'&gt;\n\n\n\n# INSERT CODE TO FIGURE OUT WHAT THE OBJECTS ATTRIBUTES OF THE \"results\" OBJECT ARE\nprint(results.__dict__.keys())\n\ndict_keys(['ims', 'pred', 'names', 'files', 'times', 'xyxy', 'xywh', 'xyxyn', 'xywhn', 'n', 't', 's'])\n\n\n\n#  INSERT CODE TO FIGURE OUT WHAT THE OBJECTS METHODS OF THE \"results\" OBJECT ARE\nprint([method for method in dir(results) if callable(getattr(results, method))])\n\n['__class__', '__delattr__', '__dir__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_run', 'crop', 'pandas', 'print', 'render', 'save', 'show', 'tolist']\n\n\n\n# PRINT A SUMMARY OF THE RESULT\nresults.print()\n\nimage 1/2: 240x426 4 cars, 1 truck, 1 boat\nimage 2/2: 240x426 4 cars, 3 trucks\nSpeed: 4.3ms pre-process, 156.3ms inference, 1.3ms NMS per image at shape (2, 3, 384, 640)\n\n\n\n#THE INFORMATION ABOUT THE BOUNDING BOXES CAN BE EXTRACTED AS FOLLOWS\nprint(results.pandas().xyxy[1])\n\n         xmin        ymin        xmax        ymax  confidence  class   name\n0  288.718414  122.997223  329.926331  165.041534    0.869563      7  truck\n1  363.126007  147.772217  426.000000  182.583176    0.848963      2    car\n2  116.674019  131.229599  188.073257  182.016342    0.793729      7  truck\n3  239.904663  146.646088  282.098633  176.821854    0.714780      2    car\n4  204.365540  144.626816  220.182419  157.711853    0.386479      2    car\n5  229.952972  138.270065  263.365479  165.125015    0.349598      7  truck\n6  231.498993  142.332169  261.764008  167.849060    0.269167      2    car\n\n\n\n# INSERT CODE TO REMOVE THE FOLDER \"runs\" IF IT EXISTS THEN RUN \"results.save()\"\nresults.save()\n\nSaved 2 images to runs/detect/exp16\n\n\n\n# LOOK AT THE FIRST IMAGE\nImage(\"runs/detect/exp/00050.jpg\")\n\n\n\n\n\n\n\n\n\nImage(\"runs/detect/exp/00200.jpg\")\n\n\n\n\n\n\n\n\n\n\nPart-3.3: Explore the results\nThe following code will plot the image and add a vertical line roughly down the center of the car’s hood\n\n# Changed code slightly because it was not imgs but ims\n%matplotlib inline\nresults.render()\nplt.imshow(results.ims[1])\nplt.show()\n\n\n\n\n\n\n\n\n\n#PLOT IMAGE WITH REGIONAL SUB-DIVIDING LINES\ndef plot_lines(img):\n    YMAX=img.shape[0]\n    XMAX=img.shape[1]\n    XCENTER=0.57*XMAX\n\n    x1 = XCENTER; y1 = 0; x2 = XCENTER; y2 = YMAX \n    plt.plot([x1,x2], [y1,y2], marker = 'o',color=\"red\")\n    plt.imshow(img)\n    plt.show()\n\nresults.print()\nplot_lines(results.ims[1])\n\n\nimage 1/2: 240x426 4 cars, 1 truck, 1 boat\nimage 2/2: 240x426 4 cars, 3 trucks\nSpeed: 4.3ms pre-process, 156.3ms inference, 1.3ms NMS per image at shape (2, 3, 384, 640)\n\n\n\n\n\n\n\n\n\n\n# INSERT CODE TO MODIFY THE FUNCTION TO GENERATE THE FOLLOWING IMAGE TO ROUGHLY ISOLATE THE LANE\n# YOU CAN JUST USE GEOMETRY TO DO THIS\n\ndef plot_lane_lines(img):\n    YMAX = img.shape[0]\n    XMAX = img.shape[1]\n    XCENTER = 0.57 * XMAX\n\n    x1 = XCENTER; y1 = 0; x2 = XCENTER; y2 = YMAX\n    plt.plot([x1, x2], [y1, y2], marker = 'o', color=\"red\")\n    \n    # Diagonal Lines\n    left_x1 = XCENTER * 0.48  \n    left_y1 = YMAX\n    left_x2 = XCENTER\n    left_y2 = YMAX * 0.61 \n\n    right_x1 = XCENTER * 1.52\n    right_y1 = YMAX\n    right_x2 = XCENTER\n    right_y2 = YMAX * 0.61\n\n    # Add the lines to the plot\n    plt.plot([left_x1, left_x2], [left_y1, left_y2], color=\"red\")\n    plt.plot([right_x1, right_x2], [right_y1, right_y2], color=\"red\")\n    \n    # Plot\n    plt.imshow(img)\n    plt.show()\n\n# Call the plot function and display on the image\nplot_lane_lines(results.ims[1])\n\n\n\n\n\n\n\n\nInsert a function to output a modified version of the results.pandas().xyxy[i] data-frame in the following ways\n\nnormalize (scale) the xmin, xmax by the width of the image and ymin, ymax by the height –&gt; HxW=1x1\ncompute the area and center of each box using the normalized coordinates\nadd a column named “nearby” which is True if normalized_area&gt;0.05\nOptional: Use The geometric lane construction from the previous image and the location of the box center to compute which lane (left,center,right) nearby vehicles are in\n\n       xmin      ymin      xmax      ymax  confidence  ...   name nearby  \\\n0  0.677506  0.515625  0.771468  0.680202    0.859959  ...  truck  False   \n1  0.831234  0.617149  0.977826  0.743249    0.851794  ...    car  False   \n2  0.002621  0.457946  0.410202  0.926958    0.820941  ...  truck   True   \n3  0.547734  0.598833  0.656238  0.750011    0.581989  ...    car  False   \n4  0.473275  0.594087  0.518594  0.662508    0.503990  ...    car  False   \n\n       area  x_center  y_center  \n0  0.015464  0.724487  0.597913  \n1  0.018485  0.904530  0.680199  \n2  0.191160  0.206412  0.692452  \n3  0.016403  0.601986  0.674422  \n4  0.003101  0.495935  0.628297  \n\n#INSERT CODE TO MODIFY THE DATAFRAME \n#TO INCLUDE THE COORDINATES OF THE LINES\n\ndef modified_dataframe(results, image_index):\n    \n    # Get the df\n    df = results.pandas().xyxy[image_index]\n\n    # Width and height of image\n    width = results.ims[image_index].shape[1]\n    height = results.ims[image_index].shape[0]\n    \n    # Normalized box coordinates\n    df[['xmin', 'xmax']] /= width\n    df[['ymin', 'ymax']] /= height\n\n    # area of each box\n    df['area'] = (df['xmax'] - df['xmin']) * (df['ymax'] - df['ymin'])\n\n    # nearby\n    df['nearby'] = df['area'] &gt; 0.05\n\n    # center of each box\n    df['x_center'] = (df['xmin'] + df['xmax']) / 2\n    df['y_center'] = (df['ymin'] + df['ymax']) / 2\n\n    \n    \n    return df\n\nmodified_df = modified_dataframe(results, 1)\nmodified_df\n\n\n\n\n\n\n\n\nxmin\nymin\nxmax\nymax\nconfidence\n...\nname\narea\nnearby\nx_center\ny_center\n\n\n\n\n0\n0.677743\n0.512488\n0.774475\n0.687673\n0.869563\n...\ntruck\n0.016946\nFalse\n0.726109\n0.600081\n\n\n1\n0.852408\n0.615718\n1.000000\n0.760763\n0.848963\n...\ncar\n0.021408\nFalse\n0.926204\n0.688240\n\n\n2\n0.273883\n0.546790\n0.441487\n0.758401\n0.793729\n...\ntruck\n0.035467\nFalse\n0.357685\n0.652596\n\n\n3\n0.563156\n0.611025\n0.662203\n0.736758\n0.714780\n...\ncar\n0.012453\nFalse\n0.612680\n0.673892\n\n\n4\n0.479731\n0.602612\n0.516860\n0.657133\n0.386479\n...\ncar\n0.002024\nFalse\n0.498296\n0.629872\n\n\n5\n0.539796\n0.576125\n0.618229\n0.688021\n0.349598\n...\ntruck\n0.008776\nFalse\n0.579012\n0.632073\n\n\n6\n0.543425\n0.593051\n0.614470\n0.699371\n0.269167\n...\ncar\n0.007553\nFalse\n0.578947\n0.646211\n\n\n\n\n7 rows × 11 columns\n\n\n\n\n\n\nPart-3.4: Process all images\nNote: Those warning do not affect the outputs, they are called from sklearn. Also we have print the first 10 paths for checking purposes.\n\n# INSERT CODE TO FORM AN ORDERED (SORTED) LIST OF ALL IMAGES IN \"frames\" \ntimes = range(0, 7735, 10)\n\nobtained_frames_paths = get_frames(video_path, times, target_folder)\nobtained_frames_paths[:10]\n\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n[h264 @ 0x7f8f5d38b2c0] mmco: unref short failure\n\n\n['frames/00000.jpg',\n 'frames/00010.jpg',\n 'frames/00020.jpg',\n 'frames/00030.jpg',\n 'frames/00040.jpg',\n 'frames/00050.jpg',\n 'frames/00060.jpg',\n 'frames/00070.jpg',\n 'frames/00080.jpg',\n 'frames/00090.jpg']\n\n\n\n# INSERT CODE TO RUN YOLO ON THE COMPLETE LIST\n\nresults = model(obtained_frames_paths)\n\n\nresults.save()\n\nSaved 774 images to runs/detect/exp17\n\n\n\n\nPart-3.5: Time-series analysis\nLoop over the results and build arrays with time-series data for the following * The frames should be ordered so you can treat the index as a “time” variables * Total number num_cars and num_trucks in each frame * num_neighbor = Number of neighboring cars and trucks (use normalized box area &gt; .05 to signify a nearby car or truck)\n\n# 3.5.1: INSERT CODE TO LOOP OVER RESULTS AND BUILD ARRAYS\n\ndef overtime_frames(results):\n    # create empty lists\n    car = []\n    truck = []\n    nearby = []\n    \n    for i, df in enumerate(results.pandas().xyxy):\n        # Normalize box coordinates and get the new area\n        width = results.ims[i].shape[1]\n        height = results.ims[i].shape[0]\n        df['area'] = ((df['xmax'] - df['xmin']) / width) * ((df['ymax'] - df['ymin']) / height)\n        df['nearby'] = df['area'] &gt; 0.05\n        \n        # Total number of cars and trucks\n        car.append((df['name'] == 'car').sum())\n        truck.append((df['name'] == 'truck').sum())\n        \n        # Number of nearby vehicles\n        nearby.append(df['nearby'].sum())\n    \n    return car, truck, nearby\n\n# Obtaining the values\ncar, truck, nearby = overtime_frames(results)\n\n# print the first 30 values of each type\nprint(\"CAR \\n\", car[:30])\nprint(\"TRUCK \\n\", truck[:30])\nprint(\"NEARBY \\n\",nearby[:30])\n\nCAR \n [0, 3, 0, 2, 1, 4, 4, 3, 5, 3, 6, 11, 4, 5, 9, 5, 6, 0, 1, 6, 4, 9, 4, 0, 3, 7, 2, 4, 5, 5]\nTRUCK \n [0, 0, 0, 3, 0, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 3, 0, 6, 1, 3, 1, 5, 3, 1, 1]\nNEARBY \n [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 2]\n\n\n\n# INSERT CODE TO GENERATE THE FOLLOWING PLOT\nplt.plot(car, label='Cars')\nplt.plot(truck, label='Trucks')\nplt.title('Cars and Trucks Over Time')\nplt.xlabel('Time')\nplt.ylabel('Count')\nplt.legend()\n\n\n\n\n\n\n\n\n\n# INSERT CODE TO GENERATE THE FOLLOWING PLOT\nplt.plot(nearby, label='Nearby vehicles')\nplt.title('Nearby Vehicles Over Time')\nplt.xlabel('Time')\nplt.ylabel('Count')\n\nText(0, 0.5, 'Count')"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "What is the future of Public Transportation?",
    "section": "",
    "text": "Public transportation is any method of transportation available to the public that is sponsored by governmental entities. In some countries, it is the most used method of transportation while, in others, it is almost inexistent. What are the benefits of having public transportation? Should governmental entities invest on it? What are the preferred methods of transportation? These are just a few questions that every government asks themselves. The public transportation insdustry is one of the biggest lines of business worldwide, with trillions of everyday users. It is so important that cities plan their growth around it in order to provide equal access for everyone. Not only the methods of transport are taken into account, but all the infrastructure surrounding it such as freeways, roads, tunnels, etc. This infrastructure ensures social, economical, and environmental sustainability on the city (Murray et al. 1998)."
  },
  {
    "objectID": "Introduction.html#summary",
    "href": "Introduction.html#summary",
    "title": "What is the future of Public Transportation?",
    "section": "",
    "text": "Public transportation is any method of transportation available to the public that is sponsored by governmental entities. In some countries, it is the most used method of transportation while, in others, it is almost inexistent. What are the benefits of having public transportation? Should governmental entities invest on it? What are the preferred methods of transportation? These are just a few questions that every government asks themselves. The public transportation insdustry is one of the biggest lines of business worldwide, with trillions of everyday users. It is so important that cities plan their growth around it in order to provide equal access for everyone. Not only the methods of transport are taken into account, but all the infrastructure surrounding it such as freeways, roads, tunnels, etc. This infrastructure ensures social, economical, and environmental sustainability on the city (Murray et al. 1998)."
  },
  {
    "objectID": "Introduction.html#motivation",
    "href": "Introduction.html#motivation",
    "title": "What is the future of Public Transportation?",
    "section": "Motivation",
    "text": "Motivation\nI believe that the use of public transportation has an uncountable number of benefits. Growing up with many family members working for the public transportation sector in Madrid (Spain), I have developed a strong sense of value for this industry. While many variables and many different (sometimes opposite) interests need to be taken into account, public transport has become essential for our society. Millions of people depend on it to carry out essential activities such as shopping, going to work, or going to do other daily chores. For this reason, it is an industry that deserves severe research and development. Furthermore, due to COVID-19, the number of riders of public transportation have decreased, and there is an urgent need to bring them back. This will allow cities to be more socially sustainable and reduce the polution levels (UITP 2023).\nThe goal of this research is to make a case about the worth of having public transportation around the world and make recommendations for future steps that should be taken in this field."
  },
  {
    "objectID": "Introduction.html#polution-concerns",
    "href": "Introduction.html#polution-concerns",
    "title": "What is the future of Public Transportation?",
    "section": "Polution concerns",
    "text": "Polution concerns\nThere is a worldwide concern about pollution. Pollution has been increasing since the industrial revolution and has been a major concern for the past 50 years. The main reason for this is the constant increase in the use of fossil fuels and the lack of alternatives. The transportation sector is one of the biggest contributors to pollution. In the United States, the transportation sector is responsible for 29% of the total greenhouse gas emissions (Transportation and Quality 2023). Transportation is a great percentage and one of the main pollutors. For this reason, it is important to study the effects of public transportation on pollution and how it can help reduce it. It can be analyzed internally (how to make public transportation pollute less) or externally (how to make people use public transportation more and private transportation less)."
  },
  {
    "objectID": "Introduction.html#questions",
    "href": "Introduction.html#questions",
    "title": "What is the future of Public Transportation?",
    "section": "Questions",
    "text": "Questions\n\nHow does public transport affect pollution?\nWhat are the most pressing issues related to public transport?\nHow can we address public transportation at a large scale?\nHow does public transportation affect the economy and development of a city?\nTo what extent does public transportation reduce the usage of private transportation?\nWhich factors influence effectiveness of public transportation the most?\nWhat is the reffect of transportation in pollution?\nHas public transportation been improving overtime?\nDo different public transportation methods hold different values (important for investment)?\nWhat is most important to people regarding public transportation?"
  },
  {
    "objectID": "Auto_encoder.html",
    "href": "Auto_encoder.html",
    "title": "Lab 5.1: Dimensionality reduction with Auto-encoders",
    "section": "",
    "text": "Dimensionality reduction with Auto-encoders\n\n\n\nIn this lab we will be working with fashion_mnist, so that the code will run quickly on our laptops.\nHowever, the concepts can be easily generalized and applied to any data-set.\n\nSubmission: * You need to upload ONE document to Canvas when you are done * (1) A PDF (or HTML) of the completed form of this notebook * The final uploaded version should NOT have any code-errors present * All outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras import models\nfrom keras import layers\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.datasets import fashion_mnist\nfrom keras.callbacks import EarlyStopping\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n2024-05-01 12:28:03.706058: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\n\n\nLoad the fashion_mnist data\nRead the additional assignment tasks below and normalize and reshape the data as needed\nOptional: randomly visualize a couple of images\n\n\n\nCode\n# INSERT CODE HERE\n# fashion mnist dataset\n(train_im, train_labels), (test_im, test_labels) = fashion_mnist.load_data()\n\n# Print shapes\nprint(\"Shapes:\", \"\\n----------------------------------\")\nprint(\"Train images: \", train_im.shape, \"\\nShape of train labels: \", train_labels.shape, \"\\nShape of test images: \", test_im.shape, \"\\nShape of test labels: \", test_labels.shape)\n\n# normalize pixel values\ntrain_im = train_im.astype('float32') / 255\ntest_im = test_im.astype('float32') / 255\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n\n# Plot and image\nplt.imshow(train_im[5])\n\n\nShapes: \n----------------------------------\nTrain images:  (60000, 28, 28) \nShape of train labels:  (60000,) \nShape of test images:  (10000, 28, 28) \nShape of test labels:  (10000,)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite a function that;\n\nTakes a bottle-neck size as an input and returns training/validation errors for the auto-encoder\nYour training function MUST include an early stopping call-back to avoid over-fitting\nDo this with a standard dense feed forward auto-encoder with one hidden layer (i.e. not VAE)\n\nOptional: If you want you can use a convolutional auto-encoder instead of a dense feed forward ANN\n\nNOTE: You can do this with Keras or Pytorch, however, Keras is recommended since you can recycle large portions of the lecture-codes.\n\n\n\n\nCode\n# INSERT CODE HERE\n\n# dense autoencoder\ndef autoencoderinator(size_b_neck):\n\n    model = models.Sequential([\n        layers.Dense(size_b_neck, activation='linear', input_shape=(28 * 28,)),\n        layers.Dense(28 * 28, activation='linear')\n    ])\n    return model\n\n# training function\ndef training_autoencoder_mod(size_b_neck):\n    \n    global train_im\n\n    model = autoencoderinator(size_b_neck)\n    \n    # compile model\n    model.compile(optimizer='rmsprop', loss='mean_squared_error')\n\n    # early stopping\n    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n\n    # reshape training images for model\n    x_train = train_im.reshape((60000, 28 * 28))\n\n    # training model\n    history = model.fit(x_train, x_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stop])\n\n    # return training and validation loss per epoch\n    return history.history['loss'], history.history['val_loss']\n\n\n\n\n\n\nWrap a grid search loop (or random search) around your training function from part-5.1.2\nThis search should try different bottle-neck sizes (latent space dimensionalities)\nWhen the loop is done\n\nPlot validation_loss vs latent_space_dimensionality\nFrom the plot choose an “optimal” latent_space_dimension which has a good validation accuracy AND minimizes the size of the latent space\n\n\n\n\nCode\n# INSERT CODE HERE\n\n# grid search over bottleneck sizes fucntion\ndef grid_search(sizes_b_necks):\n\n    # store validation losses (initially zeros)\n    val_losses = np.zeros(len(sizes_b_necks))\n\n    for i, size_b_neck in enumerate(sizes_b_necks):\n        # train model for each bottleneck size\n        _, val_loss = training_autoencoder_mod(size_b_neck)\n\n        # save final validation loss\n        val_losses[i] = val_loss[-1]\n\n    return val_losses\n\n# bottleneck sizes (grid)\nsizes_b_necks = [4, 8, 16, 32, 64, 128, 256, 512]\n\n# grid search\nval_losses = grid_search(sizes_b_necks)\n\n\nEpoch 1/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0522 - val_loss: 0.0347\nEpoch 2/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0310 - val_loss: 0.0292\nEpoch 3/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0285 - val_loss: 0.0279\nEpoch 4/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0276 - val_loss: 0.0273\nEpoch 5/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0272 - val_loss: 0.0272\nEpoch 6/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0271 - val_loss: 0.0271\nEpoch 7/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0270 - val_loss: 0.0269\nEpoch 8/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 9/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 10/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 11/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0268\nEpoch 12/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 13/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 14/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0270\nEpoch 15/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0270\nEpoch 16/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 1/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0480 - val_loss: 0.0311\nEpoch 2/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0278 - val_loss: 0.0260\nEpoch 3/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0248 - val_loss: 0.0239\nEpoch 4/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0233 - val_loss: 0.0227\nEpoch 5/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0223 - val_loss: 0.0219\nEpoch 6/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0216 - val_loss: 0.0214\nEpoch 7/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0213 - val_loss: 0.0211\nEpoch 8/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0211 - val_loss: 0.0210\nEpoch 9/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0210 - val_loss: 0.0210\nEpoch 10/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0209 - val_loss: 0.0209\nEpoch 11/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0209 - val_loss: 0.0209\nEpoch 12/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.0208\nEpoch 13/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.0208\nEpoch 14/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.0208\nEpoch 15/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0208\nEpoch 16/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0209\nEpoch 17/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0209\nEpoch 18/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0207\nEpoch 19/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0207\nEpoch 20/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0207\nEpoch 21/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0207\nEpoch 22/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0207\nEpoch 23/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 24/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 25/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 26/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 27/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 28/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0207\nEpoch 29/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 30/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 31/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0207\nEpoch 32/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 1/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0445 - val_loss: 0.0291\nEpoch 2/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0264 - val_loss: 0.0246\nEpoch 3/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0231 - val_loss: 0.0220\nEpoch 4/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0211 - val_loss: 0.0202\nEpoch 5/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0196 - val_loss: 0.0191\nEpoch 6/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0186 - val_loss: 0.0183\nEpoch 7/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0179 - val_loss: 0.0176\nEpoch 8/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0173 - val_loss: 0.0171\nEpoch 9/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0168 - val_loss: 0.0167\nEpoch 10/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0165 - val_loss: 0.0165\nEpoch 11/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0162 - val_loss: 0.0161\nEpoch 12/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0160 - val_loss: 0.0159\nEpoch 13/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0159 - val_loss: 0.0158\nEpoch 14/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0158 - val_loss: 0.0157\nEpoch 15/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0157 - val_loss: 0.0157\nEpoch 16/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0157 - val_loss: 0.0157\nEpoch 17/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0156 - val_loss: 0.0156\nEpoch 18/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0156 - val_loss: 0.0156\nEpoch 19/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0156 - val_loss: 0.0156\nEpoch 20/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0155\nEpoch 21/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0155\nEpoch 22/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0155\nEpoch 23/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0155\nEpoch 24/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0155\nEpoch 25/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 26/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 27/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0155\nEpoch 28/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 29/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 30/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 31/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 32/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 33/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 34/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 35/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0153\nEpoch 36/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 37/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 38/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 39/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 40/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 41/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 42/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 43/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 44/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 45/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 46/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 47/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 48/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 49/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 50/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 51/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 52/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 53/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 54/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 55/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 56/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 57/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 1/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0423 - val_loss: 0.0282\nEpoch 2/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0250 - val_loss: 0.0226\nEpoch 3/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0211 - val_loss: 0.0199\nEpoch 4/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0189 - val_loss: 0.0182\nEpoch 5/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0173 - val_loss: 0.0167\nEpoch 6/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0161 - val_loss: 0.0156\nEpoch 7/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - val_loss: 0.0149\nEpoch 8/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0145 - val_loss: 0.0144\nEpoch 9/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0140 - val_loss: 0.0137\nEpoch 10/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0135 - val_loss: 0.0135\nEpoch 11/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0131 - val_loss: 0.0131\nEpoch 12/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0128 - val_loss: 0.0126\nEpoch 13/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0125 - val_loss: 0.0125\nEpoch 14/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0122 - val_loss: 0.0122\nEpoch 15/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0120 - val_loss: 0.0120\nEpoch 16/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0119 - val_loss: 0.0118\nEpoch 17/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0117 - val_loss: 0.0117\nEpoch 18/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0116 - val_loss: 0.0116\nEpoch 19/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0115 - val_loss: 0.0116\nEpoch 20/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0114 - val_loss: 0.0115\nEpoch 21/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0113 - val_loss: 0.0113\nEpoch 22/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0112 - val_loss: 0.0112\nEpoch 23/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0112 - val_loss: 0.0113\nEpoch 24/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0111 - val_loss: 0.0111\nEpoch 25/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0110 - val_loss: 0.0110\nEpoch 26/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0110 - val_loss: 0.0110\nEpoch 27/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0110 - val_loss: 0.0109\nEpoch 28/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.0109\nEpoch 29/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.0109\nEpoch 30/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.0108\nEpoch 31/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0108 - val_loss: 0.0109\nEpoch 32/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0108 - val_loss: 0.0109\nEpoch 33/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0108 - val_loss: 0.0108\nEpoch 34/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0108 - val_loss: 0.0108\nEpoch 35/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0108 - val_loss: 0.0108\nEpoch 36/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 37/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 38/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 39/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 40/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 41/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 42/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 43/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0106 - val_loss: 0.0107\nEpoch 44/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0106 - val_loss: 0.0107\nEpoch 45/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0106 - val_loss: 0.0107\nEpoch 46/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0106 - val_loss: 0.0107\nEpoch 1/100\n375/375 [==============================] - 2s 3ms/step - loss: 0.0413 - val_loss: 0.0290\nEpoch 2/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0237 - val_loss: 0.0205\nEpoch 3/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0194 - val_loss: 0.0177\nEpoch 4/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0168 - val_loss: 0.0158\nEpoch 5/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0150 - val_loss: 0.0144\nEpoch 6/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0137 - val_loss: 0.0136\nEpoch 7/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0128 - val_loss: 0.0123\nEpoch 8/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0120 - val_loss: 0.0117\nEpoch 9/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0115 - val_loss: 0.0113\nEpoch 10/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.0110\nEpoch 11/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0106 - val_loss: 0.0105\nEpoch 12/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0102 - val_loss: 0.0103\nEpoch 13/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0099 - val_loss: 0.0097\nEpoch 14/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0096 - val_loss: 0.0095\nEpoch 15/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0094 - val_loss: 0.0092\nEpoch 16/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0091 - val_loss: 0.0091\nEpoch 17/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0089 - val_loss: 0.0088\nEpoch 18/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0088 - val_loss: 0.0087\nEpoch 19/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0086 - val_loss: 0.0085\nEpoch 20/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0085 - val_loss: 0.0083\nEpoch 21/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0083 - val_loss: 0.0083\nEpoch 22/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0082 - val_loss: 0.0082\nEpoch 23/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0081 - val_loss: 0.0081\nEpoch 24/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0080 - val_loss: 0.0079\nEpoch 25/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0079 - val_loss: 0.0078\nEpoch 26/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0078 - val_loss: 0.0078\nEpoch 27/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0077 - val_loss: 0.0077\nEpoch 28/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0076 - val_loss: 0.0076\nEpoch 29/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0076 - val_loss: 0.0075\nEpoch 30/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0075 - val_loss: 0.0075\nEpoch 31/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0074 - val_loss: 0.0074\nEpoch 32/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0074 - val_loss: 0.0077\nEpoch 33/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0074 - val_loss: 0.0073\nEpoch 34/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0073 - val_loss: 0.0075\nEpoch 35/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0073 - val_loss: 0.0074\nEpoch 36/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0072 - val_loss: 0.0072\nEpoch 37/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0072 - val_loss: 0.0073\nEpoch 38/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0071 - val_loss: 0.0072\nEpoch 39/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0071 - val_loss: 0.0071\nEpoch 40/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0071 - val_loss: 0.0071\nEpoch 41/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0070 - val_loss: 0.0071\nEpoch 42/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0070 - val_loss: 0.0070\nEpoch 43/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0070 - val_loss: 0.0070\nEpoch 44/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0070 - val_loss: 0.0070\nEpoch 45/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0069 - val_loss: 0.0069\nEpoch 46/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0069 - val_loss: 0.0070\nEpoch 47/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0069 - val_loss: 0.0069\nEpoch 48/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0069 - val_loss: 0.0069\nEpoch 49/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0069\nEpoch 50/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 51/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 52/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 53/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 54/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 55/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 56/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0068\nEpoch 57/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0068\nEpoch 58/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 59/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 60/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 61/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 62/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 63/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 64/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0066 - val_loss: 0.0067\nEpoch 65/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0066 - val_loss: 0.0067\nEpoch 66/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0066 - val_loss: 0.0067\nEpoch 67/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0066 - val_loss: 0.0067\nEpoch 68/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 69/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 70/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 71/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 72/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0067\nEpoch 73/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 74/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 75/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 76/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 77/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 78/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 79/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 80/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 81/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 82/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 83/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 84/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 85/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 86/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 87/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 88/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 89/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 90/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 91/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 92/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 93/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 94/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 95/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 96/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 97/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 98/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 99/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 100/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 1/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0426 - val_loss: 0.0267\nEpoch 2/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0229 - val_loss: 0.0183\nEpoch 3/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0176 - val_loss: 0.0160\nEpoch 4/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0147 - val_loss: 0.0133\nEpoch 5/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0127 - val_loss: 0.0118\nEpoch 6/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0115 - val_loss: 0.0111\nEpoch 7/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0105 - val_loss: 0.0103\nEpoch 8/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0097 - val_loss: 0.0095\nEpoch 9/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0090 - val_loss: 0.0086\nEpoch 10/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0085 - val_loss: 0.0087\nEpoch 11/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0081 - val_loss: 0.0078\nEpoch 12/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0077 - val_loss: 0.0076\nEpoch 13/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0074 - val_loss: 0.0073\nEpoch 14/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0071 - val_loss: 0.0069\nEpoch 15/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0069 - val_loss: 0.0067\nEpoch 16/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0066 - val_loss: 0.0065\nEpoch 17/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0064 - val_loss: 0.0063\nEpoch 18/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0063 - val_loss: 0.0062\nEpoch 19/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0061 - val_loss: 0.0060\nEpoch 20/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0059 - val_loss: 0.0059\nEpoch 21/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0058 - val_loss: 0.0058\nEpoch 22/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0057 - val_loss: 0.0056\nEpoch 23/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0055 - val_loss: 0.0055\nEpoch 24/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0054 - val_loss: 0.0054\nEpoch 25/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0053 - val_loss: 0.0054\nEpoch 26/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0052 - val_loss: 0.0052\nEpoch 27/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0051 - val_loss: 0.0052\nEpoch 28/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0051 - val_loss: 0.0052\nEpoch 29/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0050 - val_loss: 0.0049\nEpoch 30/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0049 - val_loss: 0.0049\nEpoch 31/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0049 - val_loss: 0.0048\nEpoch 32/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0048 - val_loss: 0.0048\nEpoch 33/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0047 - val_loss: 0.0047\nEpoch 34/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0046 - val_loss: 0.0046\nEpoch 35/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0046 - val_loss: 0.0046\nEpoch 36/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0046 - val_loss: 0.0045\nEpoch 37/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0045 - val_loss: 0.0045\nEpoch 38/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0044 - val_loss: 0.0047\nEpoch 39/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0044 - val_loss: 0.0044\nEpoch 40/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0043 - val_loss: 0.0045\nEpoch 41/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0043 - val_loss: 0.0043\nEpoch 42/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0043 - val_loss: 0.0043\nEpoch 43/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0042 - val_loss: 0.0043\nEpoch 44/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0042 - val_loss: 0.0042\nEpoch 45/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0042 - val_loss: 0.0042\nEpoch 46/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0041 - val_loss: 0.0042\nEpoch 47/100\n375/375 [==============================] - 2s 6ms/step - loss: 0.0041 - val_loss: 0.0041\nEpoch 48/100\n375/375 [==============================] - 2s 6ms/step - loss: 0.0040 - val_loss: 0.0041\nEpoch 49/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0040 - val_loss: 0.0041\nEpoch 50/100\n375/375 [==============================] - 2s 6ms/step - loss: 0.0040 - val_loss: 0.0040\nEpoch 51/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0040 - val_loss: 0.0041\nEpoch 52/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0039 - val_loss: 0.0040\nEpoch 53/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0039 - val_loss: 0.0039\nEpoch 54/100\n375/375 [==============================] - 2s 6ms/step - loss: 0.0039 - val_loss: 0.0040\nEpoch 55/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0039 - val_loss: 0.0039\nEpoch 56/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0040\nEpoch 57/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0039\nEpoch 58/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0038\nEpoch 59/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0040\nEpoch 60/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0038\nEpoch 61/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0038\nEpoch 62/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0038\nEpoch 63/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0038\nEpoch 64/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0037\nEpoch 65/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0037\nEpoch 66/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0037\nEpoch 67/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0038\nEpoch 68/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0037\nEpoch 69/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0037\nEpoch 70/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0036\nEpoch 71/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0036\nEpoch 72/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0038\nEpoch 73/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0036\nEpoch 74/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0036\nEpoch 75/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0036\nEpoch 76/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0036\nEpoch 77/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0036\nEpoch 78/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 79/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 80/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0036\nEpoch 81/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 82/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 83/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 84/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 85/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 86/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 87/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 88/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 89/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 90/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 91/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 92/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 93/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 94/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 95/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 96/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 97/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 98/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 99/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0033 - val_loss: 0.0034\nEpoch 100/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0033 - val_loss: 0.0034\n\n\n\n\nCode\n# Plot validation loss vs bottleneck size\nplt.plot(sizes_b_necks, val_losses)\nplt.title('Validation Loss vs Bottle-neck Size')\nplt.xlabel('Bottle-neck Size')\nplt.ylabel('Validation Loss')\nplt.show()\n\n\n\n\n\n\n\n\n\nThis graph shows significant decrease in the validation loss as the latent space size increases. However, after 64, the decrease is not significant enough to increase the bottle neck size (as we do not want to overfit and it will also be more efficient). Therefore, we can choose 64 as the optimal latent space size.\n\n\n\n\nUsing your optimal latent space size;\n\nCompute the latent space vector for each data point in the validation set (i.e. perform the dimensionality reduction)\nIsolate the three latent space dimensions with the maximum variance\nPlot the data in this latent space with color encoding representing the MNIST class values (0,1,2 .. )\n\n\n\n\nCode\n# INSERT CODE HERE\n\n# 3D latent space of autoencoder's bottleneck feat.\ndef latent_space_plottinator(train_im, train_labels, bottleneck_size=8):\n\n    # fit autoencoder and get bottleneck feat.\n    model = autoencoderinator(bottleneck_size)\n    model.compile(optimizer='rmsprop', loss='mean_squared_error')\n\n    # reshape training images for model\n    x_train = train_im.reshape((60000, 28 * 28))\n\n    # train model\n    model.fit(x_train, x_train, epochs=10, batch_size=1000, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=5)], verbose=0)\n\n    # create model with output -- bottleneck feat.\n    bottleneck_model = models.Model(inputs=model.input, outputs=model.layers[0].output)\n\n    # predict feat.\n    x_bottleneck = bottleneck_model.predict(x_train)\n\n    # variances\n    vars = np.var(x_bottleneck, axis=0)\n\n    # top 3 indices (highest variances)\n    top3_indices = np.argsort(vars)[::-1][:3]\n\n    # subset data to top 3 indices\n    x_bottleneck_top3 = x_bottleneck[:, top3_indices]\n\n    # 3D scatter plot\n    fig = plt.figure(figsize=(10, 12))\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.scatter(\n        xs=x_bottleneck_top3[:, 0], \n        ys=x_bottleneck_top3[:, 1], \n        zs=x_bottleneck_top3[:, 2], \n        c=train_labels.argmax(axis=1))\n\n    # label axes and set title\n    ax.set_title('3D Latent Space Fashion MNIST')\n    ax.set_xlabel('Latent Dim. 1')\n    ax.set_ylabel('Latent Dim. 2')\n    ax.set_zlabel('Latent Dim. 3')\n    plt.show()\n\n# Visualize latent space with specific bottleneck size (128 overfits)\nlatent_space_plottinator(train_im, train_labels, bottleneck_size=64)\n\n\n1875/1875 [==============================] - 2s 988us/step"
  },
  {
    "objectID": "Auto_encoder.html#overview",
    "href": "Auto_encoder.html#overview",
    "title": "Lab 5.1: Dimensionality reduction with Auto-encoders",
    "section": "",
    "text": "In this lab we will be working with fashion_mnist, so that the code will run quickly on our laptops.\nHowever, the concepts can be easily generalized and applied to any data-set.\n\nSubmission: * You need to upload ONE document to Canvas when you are done * (1) A PDF (or HTML) of the completed form of this notebook * The final uploaded version should NOT have any code-errors present * All outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc"
  },
  {
    "objectID": "Auto_encoder.html#import",
    "href": "Auto_encoder.html#import",
    "title": "Lab 5.1: Dimensionality reduction with Auto-encoders",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras import models\nfrom keras import layers\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.datasets import fashion_mnist\nfrom keras.callbacks import EarlyStopping\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n2024-05-01 12:28:03.706058: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "Auto_encoder.html#part-1-load-data",
    "href": "Auto_encoder.html#part-1-load-data",
    "title": "Lab 5.1: Dimensionality reduction with Auto-encoders",
    "section": "",
    "text": "Load the fashion_mnist data\nRead the additional assignment tasks below and normalize and reshape the data as needed\nOptional: randomly visualize a couple of images\n\n\n\nCode\n# INSERT CODE HERE\n# fashion mnist dataset\n(train_im, train_labels), (test_im, test_labels) = fashion_mnist.load_data()\n\n# Print shapes\nprint(\"Shapes:\", \"\\n----------------------------------\")\nprint(\"Train images: \", train_im.shape, \"\\nShape of train labels: \", train_labels.shape, \"\\nShape of test images: \", test_im.shape, \"\\nShape of test labels: \", test_labels.shape)\n\n# normalize pixel values\ntrain_im = train_im.astype('float32') / 255\ntest_im = test_im.astype('float32') / 255\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n\n# Plot and image\nplt.imshow(train_im[5])\n\n\nShapes: \n----------------------------------\nTrain images:  (60000, 28, 28) \nShape of train labels:  (60000,) \nShape of test images:  (10000, 28, 28) \nShape of test labels:  (10000,)"
  },
  {
    "objectID": "Auto_encoder.html#part-2-auto-encoder-training",
    "href": "Auto_encoder.html#part-2-auto-encoder-training",
    "title": "Lab 5.1: Dimensionality reduction with Auto-encoders",
    "section": "",
    "text": "Write a function that;\n\nTakes a bottle-neck size as an input and returns training/validation errors for the auto-encoder\nYour training function MUST include an early stopping call-back to avoid over-fitting\nDo this with a standard dense feed forward auto-encoder with one hidden layer (i.e. not VAE)\n\nOptional: If you want you can use a convolutional auto-encoder instead of a dense feed forward ANN\n\nNOTE: You can do this with Keras or Pytorch, however, Keras is recommended since you can recycle large portions of the lecture-codes.\n\n\n\n\nCode\n# INSERT CODE HERE\n\n# dense autoencoder\ndef autoencoderinator(size_b_neck):\n\n    model = models.Sequential([\n        layers.Dense(size_b_neck, activation='linear', input_shape=(28 * 28,)),\n        layers.Dense(28 * 28, activation='linear')\n    ])\n    return model\n\n# training function\ndef training_autoencoder_mod(size_b_neck):\n    \n    global train_im\n\n    model = autoencoderinator(size_b_neck)\n    \n    # compile model\n    model.compile(optimizer='rmsprop', loss='mean_squared_error')\n\n    # early stopping\n    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n\n    # reshape training images for model\n    x_train = train_im.reshape((60000, 28 * 28))\n\n    # training model\n    history = model.fit(x_train, x_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stop])\n\n    # return training and validation loss per epoch\n    return history.history['loss'], history.history['val_loss']"
  },
  {
    "objectID": "Auto_encoder.html#part-3-find-optimal-latent-space-size",
    "href": "Auto_encoder.html#part-3-find-optimal-latent-space-size",
    "title": "Lab 5.1: Dimensionality reduction with Auto-encoders",
    "section": "",
    "text": "Wrap a grid search loop (or random search) around your training function from part-5.1.2\nThis search should try different bottle-neck sizes (latent space dimensionalities)\nWhen the loop is done\n\nPlot validation_loss vs latent_space_dimensionality\nFrom the plot choose an “optimal” latent_space_dimension which has a good validation accuracy AND minimizes the size of the latent space\n\n\n\n\nCode\n# INSERT CODE HERE\n\n# grid search over bottleneck sizes fucntion\ndef grid_search(sizes_b_necks):\n\n    # store validation losses (initially zeros)\n    val_losses = np.zeros(len(sizes_b_necks))\n\n    for i, size_b_neck in enumerate(sizes_b_necks):\n        # train model for each bottleneck size\n        _, val_loss = training_autoencoder_mod(size_b_neck)\n\n        # save final validation loss\n        val_losses[i] = val_loss[-1]\n\n    return val_losses\n\n# bottleneck sizes (grid)\nsizes_b_necks = [4, 8, 16, 32, 64, 128, 256, 512]\n\n# grid search\nval_losses = grid_search(sizes_b_necks)\n\n\nEpoch 1/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0522 - val_loss: 0.0347\nEpoch 2/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0310 - val_loss: 0.0292\nEpoch 3/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0285 - val_loss: 0.0279\nEpoch 4/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0276 - val_loss: 0.0273\nEpoch 5/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0272 - val_loss: 0.0272\nEpoch 6/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0271 - val_loss: 0.0271\nEpoch 7/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0270 - val_loss: 0.0269\nEpoch 8/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 9/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 10/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 11/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0268\nEpoch 12/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 13/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 14/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0270\nEpoch 15/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0270\nEpoch 16/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.0269\nEpoch 1/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0480 - val_loss: 0.0311\nEpoch 2/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0278 - val_loss: 0.0260\nEpoch 3/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0248 - val_loss: 0.0239\nEpoch 4/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0233 - val_loss: 0.0227\nEpoch 5/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0223 - val_loss: 0.0219\nEpoch 6/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0216 - val_loss: 0.0214\nEpoch 7/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0213 - val_loss: 0.0211\nEpoch 8/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0211 - val_loss: 0.0210\nEpoch 9/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0210 - val_loss: 0.0210\nEpoch 10/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0209 - val_loss: 0.0209\nEpoch 11/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0209 - val_loss: 0.0209\nEpoch 12/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.0208\nEpoch 13/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.0208\nEpoch 14/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.0208\nEpoch 15/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0208\nEpoch 16/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0209\nEpoch 17/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0209\nEpoch 18/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0207\nEpoch 19/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0207\nEpoch 20/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0207\nEpoch 21/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0207\nEpoch 22/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0207\nEpoch 23/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 24/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 25/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 26/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 27/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 28/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0207\nEpoch 29/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 30/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 31/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0207\nEpoch 32/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - val_loss: 0.0206\nEpoch 1/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0445 - val_loss: 0.0291\nEpoch 2/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0264 - val_loss: 0.0246\nEpoch 3/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0231 - val_loss: 0.0220\nEpoch 4/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0211 - val_loss: 0.0202\nEpoch 5/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0196 - val_loss: 0.0191\nEpoch 6/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0186 - val_loss: 0.0183\nEpoch 7/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0179 - val_loss: 0.0176\nEpoch 8/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0173 - val_loss: 0.0171\nEpoch 9/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0168 - val_loss: 0.0167\nEpoch 10/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0165 - val_loss: 0.0165\nEpoch 11/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0162 - val_loss: 0.0161\nEpoch 12/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0160 - val_loss: 0.0159\nEpoch 13/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0159 - val_loss: 0.0158\nEpoch 14/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0158 - val_loss: 0.0157\nEpoch 15/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0157 - val_loss: 0.0157\nEpoch 16/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0157 - val_loss: 0.0157\nEpoch 17/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0156 - val_loss: 0.0156\nEpoch 18/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0156 - val_loss: 0.0156\nEpoch 19/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0156 - val_loss: 0.0156\nEpoch 20/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0155\nEpoch 21/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0155\nEpoch 22/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0155\nEpoch 23/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0155\nEpoch 24/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0155\nEpoch 25/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 26/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 27/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0155\nEpoch 28/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 29/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 30/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 31/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 32/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 33/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 34/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0154\nEpoch 35/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.0153\nEpoch 36/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 37/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 38/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 39/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 40/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 41/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 42/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 43/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 44/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 45/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 46/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 47/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 48/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 49/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 50/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 51/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 52/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 53/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 54/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 55/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0154\nEpoch 56/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 57/100\n375/375 [==============================] - 1s 2ms/step - loss: 0.0153 - val_loss: 0.0153\nEpoch 1/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0423 - val_loss: 0.0282\nEpoch 2/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0250 - val_loss: 0.0226\nEpoch 3/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0211 - val_loss: 0.0199\nEpoch 4/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0189 - val_loss: 0.0182\nEpoch 5/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0173 - val_loss: 0.0167\nEpoch 6/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0161 - val_loss: 0.0156\nEpoch 7/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - val_loss: 0.0149\nEpoch 8/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0145 - val_loss: 0.0144\nEpoch 9/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0140 - val_loss: 0.0137\nEpoch 10/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0135 - val_loss: 0.0135\nEpoch 11/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0131 - val_loss: 0.0131\nEpoch 12/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0128 - val_loss: 0.0126\nEpoch 13/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0125 - val_loss: 0.0125\nEpoch 14/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0122 - val_loss: 0.0122\nEpoch 15/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0120 - val_loss: 0.0120\nEpoch 16/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0119 - val_loss: 0.0118\nEpoch 17/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0117 - val_loss: 0.0117\nEpoch 18/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0116 - val_loss: 0.0116\nEpoch 19/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0115 - val_loss: 0.0116\nEpoch 20/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0114 - val_loss: 0.0115\nEpoch 21/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0113 - val_loss: 0.0113\nEpoch 22/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0112 - val_loss: 0.0112\nEpoch 23/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0112 - val_loss: 0.0113\nEpoch 24/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0111 - val_loss: 0.0111\nEpoch 25/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0110 - val_loss: 0.0110\nEpoch 26/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0110 - val_loss: 0.0110\nEpoch 27/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0110 - val_loss: 0.0109\nEpoch 28/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.0109\nEpoch 29/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.0109\nEpoch 30/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.0108\nEpoch 31/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0108 - val_loss: 0.0109\nEpoch 32/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0108 - val_loss: 0.0109\nEpoch 33/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0108 - val_loss: 0.0108\nEpoch 34/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0108 - val_loss: 0.0108\nEpoch 35/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0108 - val_loss: 0.0108\nEpoch 36/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 37/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 38/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 39/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 40/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 41/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 42/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0107 - val_loss: 0.0107\nEpoch 43/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0106 - val_loss: 0.0107\nEpoch 44/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0106 - val_loss: 0.0107\nEpoch 45/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0106 - val_loss: 0.0107\nEpoch 46/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0106 - val_loss: 0.0107\nEpoch 1/100\n375/375 [==============================] - 2s 3ms/step - loss: 0.0413 - val_loss: 0.0290\nEpoch 2/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0237 - val_loss: 0.0205\nEpoch 3/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0194 - val_loss: 0.0177\nEpoch 4/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0168 - val_loss: 0.0158\nEpoch 5/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0150 - val_loss: 0.0144\nEpoch 6/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0137 - val_loss: 0.0136\nEpoch 7/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0128 - val_loss: 0.0123\nEpoch 8/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0120 - val_loss: 0.0117\nEpoch 9/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0115 - val_loss: 0.0113\nEpoch 10/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.0110\nEpoch 11/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0106 - val_loss: 0.0105\nEpoch 12/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0102 - val_loss: 0.0103\nEpoch 13/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0099 - val_loss: 0.0097\nEpoch 14/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0096 - val_loss: 0.0095\nEpoch 15/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0094 - val_loss: 0.0092\nEpoch 16/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0091 - val_loss: 0.0091\nEpoch 17/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0089 - val_loss: 0.0088\nEpoch 18/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0088 - val_loss: 0.0087\nEpoch 19/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0086 - val_loss: 0.0085\nEpoch 20/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0085 - val_loss: 0.0083\nEpoch 21/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0083 - val_loss: 0.0083\nEpoch 22/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0082 - val_loss: 0.0082\nEpoch 23/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0081 - val_loss: 0.0081\nEpoch 24/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0080 - val_loss: 0.0079\nEpoch 25/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0079 - val_loss: 0.0078\nEpoch 26/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0078 - val_loss: 0.0078\nEpoch 27/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0077 - val_loss: 0.0077\nEpoch 28/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0076 - val_loss: 0.0076\nEpoch 29/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0076 - val_loss: 0.0075\nEpoch 30/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0075 - val_loss: 0.0075\nEpoch 31/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0074 - val_loss: 0.0074\nEpoch 32/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0074 - val_loss: 0.0077\nEpoch 33/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0074 - val_loss: 0.0073\nEpoch 34/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0073 - val_loss: 0.0075\nEpoch 35/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0073 - val_loss: 0.0074\nEpoch 36/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0072 - val_loss: 0.0072\nEpoch 37/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0072 - val_loss: 0.0073\nEpoch 38/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0071 - val_loss: 0.0072\nEpoch 39/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0071 - val_loss: 0.0071\nEpoch 40/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0071 - val_loss: 0.0071\nEpoch 41/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0070 - val_loss: 0.0071\nEpoch 42/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0070 - val_loss: 0.0070\nEpoch 43/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0070 - val_loss: 0.0070\nEpoch 44/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0070 - val_loss: 0.0070\nEpoch 45/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0069 - val_loss: 0.0069\nEpoch 46/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0069 - val_loss: 0.0070\nEpoch 47/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0069 - val_loss: 0.0069\nEpoch 48/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0069 - val_loss: 0.0069\nEpoch 49/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0069\nEpoch 50/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 51/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 52/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 53/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 54/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 55/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0068 - val_loss: 0.0068\nEpoch 56/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0068\nEpoch 57/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0068\nEpoch 58/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 59/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 60/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 61/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 62/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 63/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.0067\nEpoch 64/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0066 - val_loss: 0.0067\nEpoch 65/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0066 - val_loss: 0.0067\nEpoch 66/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0066 - val_loss: 0.0067\nEpoch 67/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0066 - val_loss: 0.0067\nEpoch 68/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 69/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 70/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 71/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 72/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0067\nEpoch 73/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 74/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 75/100\n375/375 [==============================] - 2s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 76/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 77/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0066\nEpoch 78/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 79/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 80/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 81/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 82/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 83/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 84/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 85/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 86/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 87/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 88/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 89/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 90/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 91/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 92/100\n375/375 [==============================] - 1s 3ms/step - loss: 0.0065 - val_loss: 0.0066\nEpoch 93/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 94/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 95/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 96/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 97/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 98/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 99/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 100/100\n375/375 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0065\nEpoch 1/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0426 - val_loss: 0.0267\nEpoch 2/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0229 - val_loss: 0.0183\nEpoch 3/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0176 - val_loss: 0.0160\nEpoch 4/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0147 - val_loss: 0.0133\nEpoch 5/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0127 - val_loss: 0.0118\nEpoch 6/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0115 - val_loss: 0.0111\nEpoch 7/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0105 - val_loss: 0.0103\nEpoch 8/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0097 - val_loss: 0.0095\nEpoch 9/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0090 - val_loss: 0.0086\nEpoch 10/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0085 - val_loss: 0.0087\nEpoch 11/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0081 - val_loss: 0.0078\nEpoch 12/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0077 - val_loss: 0.0076\nEpoch 13/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0074 - val_loss: 0.0073\nEpoch 14/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0071 - val_loss: 0.0069\nEpoch 15/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0069 - val_loss: 0.0067\nEpoch 16/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0066 - val_loss: 0.0065\nEpoch 17/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0064 - val_loss: 0.0063\nEpoch 18/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0063 - val_loss: 0.0062\nEpoch 19/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0061 - val_loss: 0.0060\nEpoch 20/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0059 - val_loss: 0.0059\nEpoch 21/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0058 - val_loss: 0.0058\nEpoch 22/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0057 - val_loss: 0.0056\nEpoch 23/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0055 - val_loss: 0.0055\nEpoch 24/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0054 - val_loss: 0.0054\nEpoch 25/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0053 - val_loss: 0.0054\nEpoch 26/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0052 - val_loss: 0.0052\nEpoch 27/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0051 - val_loss: 0.0052\nEpoch 28/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0051 - val_loss: 0.0052\nEpoch 29/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0050 - val_loss: 0.0049\nEpoch 30/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0049 - val_loss: 0.0049\nEpoch 31/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0049 - val_loss: 0.0048\nEpoch 32/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0048 - val_loss: 0.0048\nEpoch 33/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0047 - val_loss: 0.0047\nEpoch 34/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0046 - val_loss: 0.0046\nEpoch 35/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0046 - val_loss: 0.0046\nEpoch 36/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0046 - val_loss: 0.0045\nEpoch 37/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0045 - val_loss: 0.0045\nEpoch 38/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0044 - val_loss: 0.0047\nEpoch 39/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0044 - val_loss: 0.0044\nEpoch 40/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0043 - val_loss: 0.0045\nEpoch 41/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0043 - val_loss: 0.0043\nEpoch 42/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0043 - val_loss: 0.0043\nEpoch 43/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0042 - val_loss: 0.0043\nEpoch 44/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0042 - val_loss: 0.0042\nEpoch 45/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0042 - val_loss: 0.0042\nEpoch 46/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0041 - val_loss: 0.0042\nEpoch 47/100\n375/375 [==============================] - 2s 6ms/step - loss: 0.0041 - val_loss: 0.0041\nEpoch 48/100\n375/375 [==============================] - 2s 6ms/step - loss: 0.0040 - val_loss: 0.0041\nEpoch 49/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0040 - val_loss: 0.0041\nEpoch 50/100\n375/375 [==============================] - 2s 6ms/step - loss: 0.0040 - val_loss: 0.0040\nEpoch 51/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0040 - val_loss: 0.0041\nEpoch 52/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0039 - val_loss: 0.0040\nEpoch 53/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0039 - val_loss: 0.0039\nEpoch 54/100\n375/375 [==============================] - 2s 6ms/step - loss: 0.0039 - val_loss: 0.0040\nEpoch 55/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0039 - val_loss: 0.0039\nEpoch 56/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0040\nEpoch 57/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0039\nEpoch 58/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0038\nEpoch 59/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0040\nEpoch 60/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0038\nEpoch 61/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0038\nEpoch 62/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0038\nEpoch 63/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0038\nEpoch 64/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0037\nEpoch 65/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0037\nEpoch 66/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0037\nEpoch 67/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0038\nEpoch 68/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0037\nEpoch 69/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0037\nEpoch 70/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0036\nEpoch 71/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0036\nEpoch 72/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0038\nEpoch 73/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0036\nEpoch 74/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - val_loss: 0.0036\nEpoch 75/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0036\nEpoch 76/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0036\nEpoch 77/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0036\nEpoch 78/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 79/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 80/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0036\nEpoch 81/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 82/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 83/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 84/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0035\nEpoch 85/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 86/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 87/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 88/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 89/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 90/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 91/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 92/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 93/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 94/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 95/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0035\nEpoch 96/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 97/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 98/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0034\nEpoch 99/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0033 - val_loss: 0.0034\nEpoch 100/100\n375/375 [==============================] - 2s 5ms/step - loss: 0.0033 - val_loss: 0.0034\n\n\n\n\nCode\n# Plot validation loss vs bottleneck size\nplt.plot(sizes_b_necks, val_losses)\nplt.title('Validation Loss vs Bottle-neck Size')\nplt.xlabel('Bottle-neck Size')\nplt.ylabel('Validation Loss')\nplt.show()\n\n\n\n\n\n\n\n\n\nThis graph shows significant decrease in the validation loss as the latent space size increases. However, after 64, the decrease is not significant enough to increase the bottle neck size (as we do not want to overfit and it will also be more efficient). Therefore, we can choose 64 as the optimal latent space size."
  },
  {
    "objectID": "Auto_encoder.html#part-4-latent-space-visualization",
    "href": "Auto_encoder.html#part-4-latent-space-visualization",
    "title": "Lab 5.1: Dimensionality reduction with Auto-encoders",
    "section": "",
    "text": "Using your optimal latent space size;\n\nCompute the latent space vector for each data point in the validation set (i.e. perform the dimensionality reduction)\nIsolate the three latent space dimensions with the maximum variance\nPlot the data in this latent space with color encoding representing the MNIST class values (0,1,2 .. )\n\n\n\n\nCode\n# INSERT CODE HERE\n\n# 3D latent space of autoencoder's bottleneck feat.\ndef latent_space_plottinator(train_im, train_labels, bottleneck_size=8):\n\n    # fit autoencoder and get bottleneck feat.\n    model = autoencoderinator(bottleneck_size)\n    model.compile(optimizer='rmsprop', loss='mean_squared_error')\n\n    # reshape training images for model\n    x_train = train_im.reshape((60000, 28 * 28))\n\n    # train model\n    model.fit(x_train, x_train, epochs=10, batch_size=1000, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=5)], verbose=0)\n\n    # create model with output -- bottleneck feat.\n    bottleneck_model = models.Model(inputs=model.input, outputs=model.layers[0].output)\n\n    # predict feat.\n    x_bottleneck = bottleneck_model.predict(x_train)\n\n    # variances\n    vars = np.var(x_bottleneck, axis=0)\n\n    # top 3 indices (highest variances)\n    top3_indices = np.argsort(vars)[::-1][:3]\n\n    # subset data to top 3 indices\n    x_bottleneck_top3 = x_bottleneck[:, top3_indices]\n\n    # 3D scatter plot\n    fig = plt.figure(figsize=(10, 12))\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.scatter(\n        xs=x_bottleneck_top3[:, 0], \n        ys=x_bottleneck_top3[:, 1], \n        zs=x_bottleneck_top3[:, 2], \n        c=train_labels.argmax(axis=1))\n\n    # label axes and set title\n    ax.set_title('3D Latent Space Fashion MNIST')\n    ax.set_xlabel('Latent Dim. 1')\n    ax.set_ylabel('Latent Dim. 2')\n    ax.set_zlabel('Latent Dim. 3')\n    plt.show()\n\n# Visualize latent space with specific bottleneck size (128 overfits)\nlatent_space_plottinator(train_im, train_labels, bottleneck_size=64)\n\n\n1875/1875 [==============================] - 2s 988us/step"
  },
  {
    "objectID": "Authors_data_viz.html",
    "href": "Authors_data_viz.html",
    "title": "Authors",
    "section": "",
    "text": "Jorge Bris Moreno\n\n\n\n\n\n\n\n\nEmail: jb3128@georgetown.edu\n\n\nLinkedIn: Jorge Bris Moreno\n\n\nGitHub: Jorge Bris Moreno\n\n\n\n\n\n\n\nSheeba Moghal\n\n\n\n\n\n\n\n\nEmail: sm3924@georgetown.edu\n\n\nLinkedIn: Sheeba Moghal\n\n\nGitHub: Sheeba Moghal\n\n\n\n\n\n\n\nTiana Le\n\n\n\n\n\n\n\n\nEmail: tvl8@georgetown.edu\n\n\nLinkedIn: Tiana Le\n\n\nGitHub: Tiana Le\n\n\n\n\n\n\n\nLiz Kovalchuk\n\n\n\n\n\n\n\n\nEmail: ek976@georgetown.edu\n\n\nLinkedIn: Liz Kovalchuk\n\n\nGitHub: Liz Kovalchuk"
  },
  {
    "objectID": "reportNomad_AI.html",
    "href": "reportNomad_AI.html",
    "title": "Nomad AI",
    "section": "",
    "text": "Nomad AI aims to revolutionize travel planning by creating a highly customizable and intelligent travel guide. Our project explores two complementary approaches to achieve this goal: a manual pipeline and an integrated LangChain-based solution.\nThe manual pipeline allows precise control by carefully designing and connecting individual components to handle user input, API calls, and responses. This approach leverages the flexibility of fine-tuned language models, enabling us to tailor each step to our specific requirements.\nOn the other hand, the LangChain-based solution simplifies implementation by centralizing the functionality into a single framework. While LangChain offers powerful out-of-the-box capabilities, its limited flexibility for customization presented challenges in adapting to our unique workflow.\nIn this report, we delve into both approaches’ design, implementation, and evaluation. The paper is structured as follows:\n\nManual Model – A detailed breakdown of its core components, their roles, and how they work together to deliver personalized travel planning.\nLangChain Model – An exploration of the LangChain-based solution, highlighting its strengths, limitations, and how it integrates with large language models.\nFine-Tuning and Data Preparation – A discussion of the methodologies used for data generation, model fine-tuning, and parameter optimization.\nComparison and Future Improvements – A comprehensive analysis of the two approaches, their trade-offs, and potential avenues for enhancing both systems.\n\nThrough this exploration, we aim to demonstrate how large language models can be harnessed to create a travel guide that is both intelligent and highly adaptable, providing users with seamless and personalized travel experiences."
  },
  {
    "objectID": "reportNomad_AI.html#structure",
    "href": "reportNomad_AI.html#structure",
    "title": "Nomad AI",
    "section": "Structure",
    "text": "Structure\n\n\n\nFigure 1: Manual Model Structure\n\n\n\nRecipient: Initial Interaction and Input Gathering\n\nThe Recipient is the user’s first point of contact, responsible for welcoming them and ensuring their input contains the necessary details for API requests. Using prompt engineering, the Recipient validates the input and, if needed, engages the user with follow-up questions to gather missing information, such as departure locations, destinations, or travel dates. This iterative interaction guarantees that all essential parameters are collected before proceeding to the next stage.\n\nParser: Natural Language to Structured Data\n\nThe Parser is a fine-tuned language model (Llama 3.2 1B) designed to convert natural language inputs into structured JSON format. This transformation enables seamless API integration. The Parser identifies the appropriate API to call based on the query and extracts required parameters. For instance, when a user requests, “Find me a four-star hotel in Paris with a swimming pool and Wi-Fi,” the Parser identifies “Paris” as the location and specifies the amenities under the corresponding JSON fields. This ensures accurate and actionable inputs for the subsequent Tool-Caller module.\n\nTool-Caller: API Integration\n\nThe Tool-Caller takes the structured JSON output from the Parser and executes the appropriate API requests. It retrieves relevant results, cleans and aggregates the responses, and passes the processed data to the Responder. Currently, the system supports APIs for flight searches, hotel searches, and activity recommendations.\n\nResponder: Summarizing Results\n\nThe Responder delivers a concise, user-friendly summary of the API results, ensuring the output aligns with the user’s original query. It synthesizes the retrieved information into a clear, actionable format, presenting options that directly address the user’s needs."
  },
  {
    "objectID": "reportNomad_AI.html#enabling-memory",
    "href": "reportNomad_AI.html#enabling-memory",
    "title": "Nomad AI",
    "section": "Enabling Memory",
    "text": "Enabling Memory\nTo enhance the manual model’s conversational capabilities, we introduced two additional components: Query Refiner and Chat History. These components enable the model to maintain context across interactions, transforming it into a more dynamic and intuitive chatbot:\n\nQuery Refiner: This component integrates prior user inputs and chat history into new queries, ensuring continuity. For example, if the user initially says, “I want a flight from Paris to London on 2025-01-01,” and later adds, “What about 2025-02-01?” the Query Refiner combines the inputs into a complete prompt: “I want a flight from Paris to London on 2025-02-01.”\nChat History: The chat history stores previous interactions, enabling the Responder to provide coherent replies that reflect the conversation’s ongoing context. This avoids repetitive greetings or disjointed responses.\n\nAdditionally, a reset option allows users to clear the chat history and start fresh if the model encounters unexpected errors or hallucinations."
  },
  {
    "objectID": "reportNomad_AI.html#data-gathering",
    "href": "reportNomad_AI.html#data-gathering",
    "title": "Nomad AI",
    "section": "Data Gathering",
    "text": "Data Gathering\nTo develop a fine-tuned travel guide model, we needed a dataset tailored to our specific requirements. Since no publicly available dataset met our needs, we generated our own dataset using prompt engineering with large language models. After testing several models, including OpenAI’s GPT models and Llama models, we found that Llama models performed the best for generating diverse and high-quality data. Thus, we generated our data with Llama 3.1 8B model.\n\nData Generation Process\nWe created prompts that instructed the language model to generate structured input-output pairs suitable for travel-related tasks. These outputs adhered to the requirements of our system, including APIs for flights, hotels, and activities. Below is an example prompt and its corresponding output:\n\nExample Prompt\nGenerate a JSON object where:\n1. \"input\" is a single string where a user requests information **on different locations** for a **flight, hotel, and activities**. \n   The request involves travel from one location to another, specifying a departure date, and optionally includes details \n   such as the number of passengers, flight class, non-stop preference, hotel ratings, amenities, or other preferences. \n   The input is provided in plain text, not JSON.\n\n2. \"output\" contains the corresponding Amadeus API call in JSON format:\n   - The \"action\" field is always [\"search_flights\", \"search_hotels\", \"activities\"].\n   - The \"action_input\" field includes:\n     - \"origin\": A 3-letter city code for the departure location or airport 3-letter code if specified.\n     - \"destination\": A 3-letter city code for the arrival location or airport 3-letter code if specified.\n     - \"departure_date\": A date in \"YYYY-MM-DD\" format.\n     - \"adults\": An integer representing the number of adults (default is 1).\n     - Optional fields are included only if explicitly mentioned in the input and should be excluded otherwise:\n       - \"return_date\": A date in \"YYYY-MM-DD\" format for the return flight.\n       - \"children\": An integer representing the number of children.\n       - \"infants\": An integer representing the number of infants.\n       - \"class\": One of \"ECONOMY\", \"PREMIUM_ECONOMY\", \"BUSINESS\", or \"FIRST\".\n       - \"nonStop\": A boolean indicating whether only non-stop flights are desired.\n       - \"maxPrice\": An integer specifying the maximum price.\n       - \"ratings\": An integer for hotel ratings (out of 5).\n       - \"amenities\": A list of amenities required by the user. Supported values: SWIMMING_POOL, SPA, FITNESS_CENTER, \n         AIR_CONDITIONING, PETS_ALLOWED, WIFI.\n     - \"max\": Always set to 5.\n\n3. Ensure the input always requests all three elements (flights, hotels, and activities). Avoid generating inputs that omit any of these.\nGenerate ONE EXAMPLE PER RESPONSE object. Use varied phrasing and combinations of optional fields to add diversity to the input while adhering to the guidelines.\n\n\nExample Outut\n{\n  \"input\": \"I’d like to travel from New York (JFK) to Paris (CDG) on March 15, 2024. It’s for two adults and one child. I need business class tickets and prefer non-stop flights. For hotels, I’m looking for something luxurious with at least 5 stars, a swimming pool, and a spa. Can you also suggest some activities there?\",\n  \"output\": {   \n    \"action\": [\"search_flights\", \"search_hotels\", \"activities\"],    \n    \"action_input\": {      \n      \"origin\": \"JFK\",      \n      \"destination\": \"CDG\",      \n      \"departure_date\": \"2024-03-15\",      \n      \"adults\": 2,      \n      \"children\": 1,      \n      \"class\": \"BUSINESS\",      \n      \"nonStop\": true,      \n      \"ratings\": 5,      \n      \"amenities\": [\"SWIMMING_POOL\", \"SPA\"],      \n      \"max\": 5    \n    }  \n  }\n}\n\n\n\nChallenges and Solutions\nOne major challenge we encountered was the tendency of the language model to overuse specific cities, resulting in a lack of diversity in the dataset. To address this, we manually cleaned the data to diversify city names and ensure comprehensive representation of different scenarios. This process involved randomizing inputs and varying optional fields to create a robust dataset."
  },
  {
    "objectID": "reportNomad_AI.html#fine-tuning",
    "href": "reportNomad_AI.html#fine-tuning",
    "title": "Nomad AI",
    "section": "Fine-Tuning",
    "text": "Fine-Tuning\nAfter generating the dataset consisting of 700 data points, 100 per possible scenario, we evaluated two fine-tuning packages, PEFT and Unsloth, both of which use the LoRA (Low-Rank Adaptation) approach. Despite their shared methodology, these packages differed in implementation, providing us with unique advantages for optimizing model performance.\n\nPEFT Package\nThe PEFT package fine-tunes models using single-string inputs, enabling straightforward adjustments to predict specific outputs based on given contexts. To align with our requirements, we introduced a divider (” -&gt; “) between user prompts and target outputs. This structure allowed the model to recognize when to generate JSON outputs.\n\n\n\nFigure 2: Peft\n\n\nHowever, PEFT presented notable inefficiencies:\n\nSlow Training: Fine-tuning 400 data points required approximately four hours.\nDecoding Overhead: Tokenizing and decoding significantly slowed the output generation process.\n\nThese drawbacks prompted us to explore alternative fine-tuning methods.\n\n\nUnsloth Package\nThe Unsloth package provided a faster and more structured fine-tuning process. Its key advantage lay in its ability to explicitly define a system prompt, user prompt, and model output during training. This structured approach minimized discrepancies between the generated and target outputs.\n\n\n\nFigure 3: Unsloth\n\n\nAdditional benefits of Unsloth included:\n\nRapid Training: Fine-tuning 700 data points took just two minutes.\nLocal Hosting: The fine-tuned model could be downloaded and hosted locally on Ollama, ensuring quick response times and seamless integration into our chatbot.\n\nThrough experimentation, we adjusted as little as 0.01% and up to 10% of the model’s parameters. Interestingly, loss values and output quality remained consistent across different levels of parameter adjustments. This demonstrated the robustness of the fine-tuning process."
  },
  {
    "objectID": "reportNomad_AI.html#structure-1",
    "href": "reportNomad_AI.html#structure-1",
    "title": "Nomad AI",
    "section": "Structure",
    "text": "Structure\n\nCentralized Orchestrator\n\nThe LangChain framework designates the LLM as the core orchestrator, handling all stages of user interaction, tool calling, and response generation. Tools are defined and registered within the framework using function docstrings that specify their parameters and functionality. Based on user input and tool descriptions, the LLM determines which tools to use and in what sequence.\n\nDynamic Tool Integration\n\nThe LangChain model employs a dynamic process for selecting and calling tools. When the user submits a query, the LLM analyzes the input, matches it with the registered tools, and executes the relevant API calls. This process is iterative, meaning the model may prompt the user for additional details if necessary, ensuring it has all the required information to perform the task.\n\nMemory Functionality\n\nTo enhance conversational capabilities, LangChain incorporates a memory module that maintains context across interactions. This allows the model to handle multi-turn queries coherently. For example, if a user asks for flights in one query and follows up with “What about hotels?” the memory enables the model to link the two requests seamlessly. This creates a more engaging and intuitive user experience.\n\nFinal Response Synthesis\n\nAfter gathering all the required data through API calls, the model synthesizes the outputs into a concise and user-friendly response. This simplifies the travel planning process, presenting users with a clear summary of options for flights, hotels, and activities."
  },
  {
    "objectID": "reportNomad_AI.html#strengths-and-limitations",
    "href": "reportNomad_AI.html#strengths-and-limitations",
    "title": "Nomad AI",
    "section": "Strengths and Limitations",
    "text": "Strengths and Limitations\n\nStrengths\n\nSimplified Implementation: The centralized design reduces the complexity of managing separate components, enabling faster development.\nDynamic Tool Selection: LangChain’s ability to match user queries with appropriate tools enhances automation and efficiency. This means the system can intelligently select and call specific APIs based on the user’s request without requiring manual intervention.\nBuilt-in Memory: Retaining context across queries ensures smoother and more coherent interactions.\nIterative Query Handling: The model can prompt users for additional information dynamically, improving the quality of responses.\n\n\n\nLimitations\n\nLimited Flexibility: LangChain lacks the customizability needed to integrate fine-tuned models like Llama seamlessly.\nTool Compatibility Issues: The framework handles tool calls differently across supported models (e.g., Ollama vs. OpenAI), leading to inconsistent behavior."
  },
  {
    "objectID": "reportNomad_AI.html#temporary-hosting-of-the-langchain-model",
    "href": "reportNomad_AI.html#temporary-hosting-of-the-langchain-model",
    "title": "Nomad AI",
    "section": "Temporary Hosting of the LangChain Model",
    "text": "Temporary Hosting of the LangChain Model\nTo enable hands-on testing and gather feedback, we have temporarily deployed the LangChain model. You can explore and interact with the hosted version of the model here. This deployment serves as a proof of concept, allowing users to evaluate its functionality, test its responses, and provide valuable insights for future iterations."
  },
  {
    "objectID": "reportNomad_AI.html#future-directions",
    "href": "reportNomad_AI.html#future-directions",
    "title": "Nomad AI",
    "section": "Future Directions",
    "text": "Future Directions\nTo further improve both approaches and enhance the capabilities of Nomad AI, we propose the following future directions:\n\nEnhancing Fine-Tuning Techniques\n\n\nExperimenting with more advanced fine-tuning methods, such as reinforcement learning with human feedback (RLHF), could improve model performance in parsing complex user inputs.\nExpanding the fine-tuning dataset to include more diverse scenarios, languages, and edge cases will make the system more robust and globally applicable.\n\n\nAddressing LangChain Limitations\n\n\nExploring ways to integrate fine-tuned models like Llama into the LangChain framework to enhance flexibility.\nDeveloping custom tool-calling logic to reduce compatibility issues between different LLMs, ensuring seamless tool integration regardless of the backend.\n\n\nImproving Memory and Context Handling\n\n\nInvestigating advanced memory mechanisms to improve long-term context retention across multiple queries and sessions.\nIntegrating embeddings or vector-based memory systems for scalable and efficient conversational history management.\n\n\nExpanding Tool Support\n\n\nAdding support for additional APIs, such as currency exchange rates and visa requirements, to provide a more comprehensive travel planning experience.\nEnabling dynamic tool registration, allowing the system to adapt to new tools or APIs without requiring extensive reconfiguration.\n\n\nReal-World Deployment and User Testing\n\n\nDeploying the system as a web application or chatbot to gather real-world user feedback.\nConducting A/B testing to evaluate the effectiveness of the manual model versus the LangChain model in real user scenarios.\n\nBy pursuing these directions, Nomad AI can evolve into a more powerful and adaptable travel assistant, meeting the needs of users across diverse contexts and scenarios. This project not only highlights the utility of LLMs in travel planning but also serves as a foundation for further innovations in AI-driven personal assistants."
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "Data",
    "section": "",
    "text": "The data sets can be found in the following link: Press here!\nThey have not been added to our Git Hub repository due to their size."
  },
  {
    "objectID": "Data.html#data",
    "href": "Data.html#data",
    "title": "Data",
    "section": "",
    "text": "The data sets can be found in the following link: Press here!\nThey have not been added to our Git Hub repository due to their size."
  },
  {
    "objectID": "Data.html#meanwhile-enjoy-these-pictures-i-took-that-have-different-methods-of-transportation",
    "href": "Data.html#meanwhile-enjoy-these-pictures-i-took-that-have-different-methods-of-transportation",
    "title": "Data",
    "section": "Meanwhile, enjoy these pictures I took that have different methods of transportation!",
    "text": "Meanwhile, enjoy these pictures I took that have different methods of transportation!"
  }
]